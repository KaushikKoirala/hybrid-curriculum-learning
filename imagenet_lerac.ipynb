{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaushikKoirala/hybrid-curriculum-learning/blob/main/imagenet_lerac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d79ca51",
      "metadata": {
        "id": "4d79ca51"
      },
      "outputs": [],
      "source": [
        "!pip install torchsummary torchvision tqdm wandb typing_extensions pytorch_metric_learning timm==0.9.16 einops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f3c0d88",
      "metadata": {
        "id": "7f3c0d88"
      },
      "outputs": [],
      "source": [
        "!pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e465c97b",
      "metadata": {
        "id": "e465c97b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import datasets, transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import gc\n",
        "# from tqdm import tqdm\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics as mt\n",
        "from scipy.optimize import brentq\n",
        "from scipy.interpolate import interp1d\n",
        "import glob\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_metric_learning import samplers\n",
        "import csv\n",
        "import logging\n",
        "from timm.data import create_loader, create_transform\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c88c2bb9",
      "metadata": {
        "id": "c88c2bb9"
      },
      "outputs": [],
      "source": [
        "# Configuration for ResNet-18 with LeRaC on ImageNet100\n",
        "config = {\n",
        "  \"OUTPUT_DIR\": \"OUTPUT/ImageNet100-ResNet18-LeRaC\",\n",
        "  \"WORKERS\": 8,\n",
        "  \"PRINT_FREQ\": 500,\n",
        "  \"AMP\": {\n",
        "    \"ENABLED\": True\n",
        "  },\n",
        "  \"MODEL\": {\n",
        "    \"NAME\": \"resnet18\",\n",
        "    \"SPEC\": {\n",
        "      \"NUM_CLASSES\": 100\n",
        "    }\n",
        "  },\n",
        "  \"AUG\": {\n",
        "    \"MIXUP_PROB\": 1.0,\n",
        "    \"MIXUP\": 0.8,\n",
        "    \"MIXCUT\": 1.0,\n",
        "    \"TIMM_AUG\": {\n",
        "      \"USE_LOADER\": False,\n",
        "      \"RE_COUNT\": 1,\n",
        "      \"RE_MODE\": \"pixel\",\n",
        "      \"RE_SPLIT\": False,\n",
        "      \"RE_PROB\": 0.25,\n",
        "      \"AUTO_AUGMENT\": \"rand-m9-mstd0.5-inc1\",\n",
        "      \"HFLIP\": 0.5,\n",
        "      \"VFLIP\": 0.0,\n",
        "      \"COLOR_JITTER\": 0.4,\n",
        "      \"INTERPOLATION\": \"bicubic\"\n",
        "    }\n",
        "  },\n",
        "  \"LOSS\": {\n",
        "    \"LABEL_SMOOTHING\": 0.1\n",
        "  },\n",
        "  \"CUDNN\": {\n",
        "    \"BENCHMARK\": True,\n",
        "    \"DETERMINISTIC\": False,\n",
        "    \"ENABLED\": True\n",
        "  },\n",
        "  \"DATASET\": {\n",
        "    \"DATASET\": \"imagenet\",\n",
        "    \"DATA_FORMAT\": \"jpg\",\n",
        "    \"ROOT\": \"./ImageNet100_224\",\n",
        "    \"TEST_SET\": \"val\",\n",
        "    \"TRAIN_SET\": \"train\"\n",
        "  },\n",
        "  \"TEST\": {\n",
        "    \"BATCH_SIZE_PER_GPU\": 64,\n",
        "    \"IMAGE_SIZE\": [224, 224],\n",
        "    \"MODEL_FILE\": \"\",\n",
        "    \"INTERPOLATION\": \"bicubic\"\n",
        "  },\n",
        "  \"TRAIN\": {\n",
        "    \"BATCH_SIZE_PER_GPU\": 128,\n",
        "    \"GRADIENT_ACCUMULATION_STEPS\": 4,\n",
        "    \"LR\": 0.05,  # ⭐ SAME AS CIFAR-10! (10x higher than before)\n",
        "    \"IMAGE_SIZE\": [224, 224],\n",
        "    \"BEGIN_EPOCH\": 0,\n",
        "    \"END_EPOCH\": 100,\n",
        "    \"LR_CURRICULUM\": {\n",
        "        \"MIN_LR\": 5e-5,  # ⭐ ADJUSTED: Higher min_lr to match scale\n",
        "        \"WARMUP_EPOCHS\": 5,\n",
        "        \"C\": 10.0\n",
        "    },\n",
        "    \"CLF_LR_MULTIPLIER\": 0.01,\n",
        "    \"LR_SCHEDULER\": {\n",
        "      \"METHOD\": \"lerac\",\n",
        "      \"ARGS\": {\n",
        "        \"sched\": \"cosine\",\n",
        "        \"warmup_epochs\": 5,\n",
        "        \"warmup_lr\": 1e-5,  # ⭐ ADJUSTED: Higher to match LR scale\n",
        "        \"min_lr\": 1e-6,     # ⭐ ADJUSTED: Floor for cosine decay\n",
        "        \"cooldown_epochs\": 0,\n",
        "        \"decay_rate\": 0.1\n",
        "      }\n",
        "    },\n",
        "    \"OPTIMIZER\": \"adamW\",\n",
        "    \"WD\": 0.05,\n",
        "    \"WITHOUT_WD_LIST\": [\"bn\", \"bias\", \"ln\"],\n",
        "    \"SHUFFLE\": True\n",
        "  },\n",
        "  \"DEBUG\": {\n",
        "    \"DEBUG\": False\n",
        "  }\n",
        "}\n",
        "\n",
        "# Create output directory\n",
        "import os\n",
        "os.makedirs(config['OUTPUT_DIR'], exist_ok=True)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"✓ Config loaded for ResNet-18 with LeRaC on ImageNet100\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Base LR: {config['TRAIN']['LR']} (SAME as CIFAR-10)\")\n",
        "print(f\"Batch size: {config['TRAIN']['BATCH_SIZE_PER_GPU']}\")\n",
        "print(f\"Gradient accumulation: {config['TRAIN']['GRADIENT_ACCUMULATION_STEPS']}\")\n",
        "print(f\"Effective batch size: {config['TRAIN']['BATCH_SIZE_PER_GPU'] * config['TRAIN']['GRADIENT_ACCUMULATION_STEPS']}\")\n",
        "print(f\"Min LR: {config['TRAIN']['LR_CURRICULUM']['MIN_LR']}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e29029b6",
      "metadata": {
        "id": "e29029b6"
      },
      "source": [
        "### Defining transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "facc7444",
      "metadata": {
        "id": "facc7444"
      },
      "outputs": [],
      "source": [
        "def build_transforms(config, is_train):\n",
        "    if is_train:\n",
        "        img_size = config['TRAIN']['IMAGE_SIZE'][0]\n",
        "        timm_cfg = config['AUG']['TIMM_AUG']\n",
        "        transforms = create_transform(\n",
        "            input_size = img_size,\n",
        "            is_training = True,\n",
        "            use_prefetcher=False,\n",
        "            no_aug=False,\n",
        "            re_prob=timm_cfg['RE_PROB'],\n",
        "            re_mode=timm_cfg['RE_MODE'],\n",
        "            re_count=timm_cfg['RE_COUNT'],\n",
        "            scale=(0.08, 1.0),\n",
        "            ratio=(3.0/4.0, 4.0/3.0),\n",
        "            hflip=timm_cfg['HFLIP'],\n",
        "            vflip=timm_cfg['VFLIP'],\n",
        "            color_jitter=timm_cfg['COLOR_JITTER'],\n",
        "            auto_augment=timm_cfg['AUTO_AUGMENT'],\n",
        "            interpolation=timm_cfg['INTERPOLATION'],\n",
        "            mean=(0.485, 0.456, 0.406),\n",
        "            std=(0.229, 0.224, 0.225),\n",
        "\n",
        "        )\n",
        "    else:\n",
        "        normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        img_size = config['TEST']['IMAGE_SIZE'][0]\n",
        "        transforms = T.Compose([\n",
        "            T.Resize(int(img_size/ 0.875), interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
        "            T.CenterCrop(img_size),\n",
        "            T.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "    return transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c46aa4e",
      "metadata": {
        "id": "6c46aa4e"
      },
      "source": [
        "### Building Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "234f1b25",
      "metadata": {
        "id": "234f1b25"
      },
      "outputs": [],
      "source": [
        "def _build_imagenet_dataset(config, is_train):\n",
        "    \"\"\"Build ImageNet dataset from directory structure\"\"\"\n",
        "    transforms = build_transforms(config, is_train)\n",
        "    dataset_name = config['DATASET']['TRAIN_SET'] if is_train else config['DATASET']['TEST_SET']\n",
        "    dataset_path = os.path.join(config['DATASET']['ROOT'], dataset_name)\n",
        "\n",
        "    print(f\"Loading dataset from: {dataset_path}\")\n",
        "    dataset = datasets.ImageFolder(dataset_path, transforms)\n",
        "\n",
        "    logging.info(f'Loaded {len(dataset)} samples, is_train: {is_train}')\n",
        "    print(f\"✓ Dataset loaded: {len(dataset)} images, {len(dataset.classes)} classes\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def build_dataset(config, is_train):\n",
        "    \"\"\"\n",
        "    Build dataset - calls ImageFolder for ImageNet structure\n",
        "    \"\"\"\n",
        "    dataset = _build_imagenet_dataset(config, is_train)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33df8988",
      "metadata": {
        "id": "33df8988"
      },
      "source": [
        "### Building Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ab5c15",
      "metadata": {
        "id": "c2ab5c15"
      },
      "outputs": [],
      "source": [
        "def build_dataloader(config, is_train):\n",
        "    \"\"\"Build data loader for ImageNet\"\"\"\n",
        "    if is_train:\n",
        "        batch_size_per_gpu = config['TRAIN']['BATCH_SIZE_PER_GPU']\n",
        "        shuffle = True\n",
        "    else:\n",
        "        batch_size_per_gpu = config['TEST']['BATCH_SIZE_PER_GPU']\n",
        "        shuffle = False\n",
        "\n",
        "    dataset = build_dataset(config, is_train)\n",
        "    sampler = None\n",
        "\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size_per_gpu,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=config['WORKERS'],\n",
        "        pin_memory=True,\n",
        "        sampler=sampler,\n",
        "        drop_last=is_train,  # Drop last incomplete batch in training\n",
        "    )\n",
        "\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "# Build dataloaders\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Building DataLoaders...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "train_loader = build_dataloader(config, is_train=True)\n",
        "val_loader = build_dataloader(config, is_train=False)\n",
        "\n",
        "print(f\"\\nDataLoader Info:\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")\n",
        "print(f\"  Train dataset size: {len(train_loader.dataset)}\")\n",
        "print(f\"  Val dataset size: {len(val_loader.dataset)}\")\n",
        "print(f\"  Number of classes: {len(train_loader.dataset.classes)}\")\n",
        "print(f\"  Train batch size: {config['TRAIN']['BATCH_SIZE_PER_GPU']}\")\n",
        "print(f\"  Val batch size: {config['TEST']['BATCH_SIZE_PER_GPU']}\")\n",
        "\n",
        "# Verify data loading\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Testing Data Loading...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "images, labels = next(iter(train_loader))\n",
        "print(f\"✓ Batch loaded successfully!\")\n",
        "print(f\"  Images shape: {images.shape}\")\n",
        "print(f\"  Labels shape: {labels.shape}\")\n",
        "print(f\"  Image dtype: {images.dtype}\")\n",
        "print(f\"  Label range: [{labels.min()}, {labels.max()}]\")\n",
        "\n",
        "# Verify directory structure\n",
        "train_path = os.path.join(config['DATASET']['ROOT'], config['DATASET']['TRAIN_SET'])\n",
        "val_path = os.path.join(config['DATASET']['ROOT'], config['DATASET']['TEST_SET'])\n",
        "print(f\"\\nDataset Directories:\")\n",
        "print(f\"  Train: {train_path} - Exists: {os.path.exists(train_path)}\")\n",
        "print(f\"  Val: {val_path} - Exists: {os.path.exists(val_path)}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2afd8966",
      "metadata": {
        "id": "2afd8966"
      },
      "outputs": [],
      "source": [
        "# ResNet-18 Model Definition for ImageNet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # Initial convolution for ImageNet (7x7 kernel, stride 2)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # ResNet layers\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.maxpool(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(num_classes=100):\n",
        "    \"\"\"ResNet-18 for ImageNet\"\"\"\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "\n",
        "print(\"ResNet-18 model defined for ImageNet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "915ccc8a",
      "metadata": {
        "id": "915ccc8a"
      },
      "outputs": [],
      "source": [
        "# Create ResNet-18 model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Creating ResNet-18 Model...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "num_classes = config['MODEL']['SPEC']['NUM_CLASSES']\n",
        "model = ResNet18(num_classes=num_classes)\n",
        "\n",
        "print(f\"✓ ResNet-18 created with {num_classes} output classes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56b18871",
      "metadata": {
        "id": "56b18871"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"Count model parameters\"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Model Parameter Count\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return total_params, trainable_params\n",
        "\n",
        "# Count parameters\n",
        "total_params, trainable_params = count_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4d41901",
      "metadata": {
        "id": "f4d41901"
      },
      "source": [
        "## Setup Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e23eef8",
      "metadata": {
        "id": "5e23eef8"
      },
      "outputs": [],
      "source": [
        "def _is_depthwise(m):\n",
        "    return (\n",
        "        isinstance(m, nn.Conv2d)\n",
        "        and m.groups == m.in_channels\n",
        "        and m.groups == m.out_channels\n",
        "    )\n",
        "\n",
        "def set_wd(cfg, model):\n",
        "    \"\"\"Separate parameters by weight decay\"\"\"\n",
        "    without_decay_list = cfg[\"TRAIN\"][\"WITHOUT_WD_LIST\"]\n",
        "    without_decay_depthwise = []\n",
        "    without_decay_norm = []\n",
        "\n",
        "    for m in model.modules():\n",
        "        if _is_depthwise(m) and 'dw' in without_decay_list:\n",
        "            without_decay_depthwise.append(m.weight)\n",
        "        elif isinstance(m, nn.BatchNorm2d) and 'bn' in without_decay_list:\n",
        "            without_decay_norm.append(m.weight)\n",
        "            without_decay_norm.append(m.bias)\n",
        "        elif isinstance(m, nn.GroupNorm) and 'gn' in without_decay_list:\n",
        "            without_decay_norm.append(m.weight)\n",
        "            without_decay_norm.append(m.bias)\n",
        "        elif isinstance(m, nn.LayerNorm) and 'ln' in without_decay_list:\n",
        "            without_decay_norm.append(m.weight)\n",
        "            without_decay_norm.append(m.bias)\n",
        "\n",
        "    with_decay = []\n",
        "    without_decay = []\n",
        "\n",
        "    skip = {}\n",
        "    if hasattr(model, 'no_weight_decay'):\n",
        "        skip = model.no_weight_decay()\n",
        "\n",
        "    skip_keys = {}\n",
        "    if hasattr(model, 'no_weight_decay_keywords'):\n",
        "        skip_keys = model.no_weight_decay_keywords()\n",
        "\n",
        "    for n, p in model.named_parameters():\n",
        "        ever_set = False\n",
        "\n",
        "        if p.requires_grad is False:\n",
        "            continue\n",
        "\n",
        "        skip_flag = False\n",
        "        if n in skip:\n",
        "            print('=> set {} wd to 0'.format(n))\n",
        "            without_decay.append(p)\n",
        "            skip_flag = True\n",
        "        else:\n",
        "            for i in skip:\n",
        "                if i in n:\n",
        "                    print('=> set {} wd to 0'.format(n))\n",
        "                    without_decay.append(p)\n",
        "                    skip_flag = True\n",
        "\n",
        "        if skip_flag:\n",
        "            continue\n",
        "\n",
        "        for i in skip_keys:\n",
        "            if i in n:\n",
        "                print('=> set {} wd to 0'.format(n))\n",
        "\n",
        "        if skip_flag:\n",
        "            continue\n",
        "\n",
        "        for pp in without_decay_depthwise:\n",
        "            if p is pp:\n",
        "                if cfg['DEBUG']['DEBUG']:\n",
        "                    print('=> set depthwise({}) wd to 0'.format(n))\n",
        "                without_decay.append(p)\n",
        "                ever_set = True\n",
        "                break\n",
        "\n",
        "        for pp in without_decay_norm:\n",
        "            if p is pp:\n",
        "                if cfg['DEBUG']['DEBUG']:\n",
        "                    print('=> set norm({}) wd to 0'.format(n))\n",
        "                without_decay.append(p)\n",
        "                ever_set = True\n",
        "                break\n",
        "\n",
        "        if (\n",
        "            (not ever_set)\n",
        "            and 'bias' in without_decay_list\n",
        "            and n.endswith('.bias')\n",
        "        ):\n",
        "            if cfg['DEBUG']['DEBUG']:\n",
        "                print('=> set bias({}) wd to 0'.format(n))\n",
        "            without_decay.append(p)\n",
        "        elif not ever_set:\n",
        "            with_decay.append(p)\n",
        "\n",
        "    params = [\n",
        "        {'params': with_decay},\n",
        "        {'params': without_decay, 'weight_decay': 0.}\n",
        "    ]\n",
        "    return params\n",
        "\n",
        "\n",
        "def build_optimizer_resnet18_lerac(model, config):\n",
        "    \"\"\"\n",
        "    Build optimizer with LeRaC (Learning Rate Curriculum) for ResNet-18 on ImageNet\n",
        "    \"\"\"\n",
        "    base_lr = config['TRAIN']['LR']\n",
        "    min_lr = config['TRAIN']['LR_CURRICULUM']['MIN_LR']\n",
        "    weight_decay = config['TRAIN']['WD']\n",
        "\n",
        "    # ResNet-18 layer structure: conv1, bn1, maxpool, layer1, layer2, layer3, layer4, linear\n",
        "    # Each layer has 2 blocks (indexed 0, 1)\n",
        "    layers = ['layer1', 'layer2', 'layer3', 'layer4']\n",
        "    blocks = [str(i) for i in range(2)]  # ResNet-18 has 2 blocks per layer\n",
        "\n",
        "    # Build learning rate dictionary with curriculum\n",
        "    dictionary_lr = {}\n",
        "    start_lr = base_lr * 0.5  # Start at 50% of base for earlier layers\n",
        "\n",
        "    for layer in layers:\n",
        "        for block in blocks:\n",
        "            key = f\"{layer}.{block}\"\n",
        "            dictionary_lr[key] = max(start_lr, min_lr)\n",
        "            if start_lr >= min_lr:\n",
        "                start_lr *= 0.5  # Decay by 50% each step\n",
        "\n",
        "    # Get parameter groups with weight decay handling\n",
        "    params_with_wd_info = set_wd(config, model)\n",
        "\n",
        "    # Build parameter groups with layer-wise learning rates\n",
        "    param_to_name = {id(p): n for n, p in model.named_parameters()}\n",
        "    new_param_groups = []\n",
        "\n",
        "    # Process both with_decay and without_decay groups\n",
        "    for group in params_with_wd_info:\n",
        "        group_params = group['params'] if isinstance(group['params'], list) else [group['params']]\n",
        "        wd = group.get('weight_decay', weight_decay)\n",
        "\n",
        "        for param in group_params:\n",
        "            name = param_to_name.get(id(param), \"unknown\")\n",
        "            assigned_lr = base_lr\n",
        "\n",
        "            # Check if parameter belongs to a specific layer block\n",
        "            if name.startswith('conv1') or name.startswith('bn1') or name.startswith('maxpool'):\n",
        "                assigned_lr = base_lr\n",
        "            elif 'linear' in name or 'fc' in name:\n",
        "                assigned_lr = base_lr * config['TRAIN'].get('CLF_LR_MULTIPLIER', 0.01)\n",
        "            else:\n",
        "                # Check layer blocks\n",
        "                for key in dictionary_lr:\n",
        "                    if name.startswith(key):\n",
        "                        assigned_lr = dictionary_lr[key]\n",
        "                        break\n",
        "\n",
        "            new_param_groups.append({\n",
        "                'params': param,\n",
        "                'lr': assigned_lr,\n",
        "                'weight_decay': wd\n",
        "            })\n",
        "\n",
        "    optimizer = torch.optim.AdamW(new_param_groups)\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Optimizer: AdamW with LeRaC for ResNet-18 (ImageNet)\")\n",
        "    print(f\"Total parameter groups: {len(new_param_groups)}\")\n",
        "    print(f\"Base LR: {base_lr:.10f}\")\n",
        "    print(f\"Min LR: {min_lr:.10f}\")\n",
        "    print(f\"Weight Decay: {weight_decay}\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Layer-wise Learning Rates:\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    print(f\"conv1/bn1:       {base_lr:.6f}\")\n",
        "    for layer in layers:\n",
        "        for block in blocks:\n",
        "            key = f\"{layer}.{block}\"\n",
        "            if key in dictionary_lr:\n",
        "                print(f\"{key:15s} {dictionary_lr[key]:.6f}\")\n",
        "    print(f\"classifier:      {base_lr * config['TRAIN'].get('CLF_LR_MULTIPLIER', 0.01):.6f}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "# Build optimizer\n",
        "optimizer = build_optimizer_resnet18_lerac(model, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1084832f",
      "metadata": {
        "id": "1084832f"
      },
      "source": [
        "## Setup Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2018a522",
      "metadata": {
        "id": "2018a522"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import _LRScheduler, SequentialLR, CosineAnnealingLR\n",
        "\n",
        "class LeRaCScheduler(_LRScheduler):\n",
        "    \"\"\"\n",
        "    LeRaC Warmup Scheduler: Exponential growth during warmup\n",
        "    Based on Eq.(9): lr_t = init_lr * (c^t) for t in {1..warmup_epochs}\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, base_lr, warmup_epochs, c=10.0, last_epoch=-1):\n",
        "        self.base_lr = float(base_lr)\n",
        "        self.warmup_epochs = max(1, int(warmup_epochs))\n",
        "        self.c = float(c)\n",
        "        super().__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        # After warmup: all groups use base_lr\n",
        "        if self.last_epoch >= self.warmup_epochs:\n",
        "            return [self.base_lr for _ in self.optimizer.param_groups]\n",
        "\n",
        "        # Warmup epoch counter t ∈ {1..warmup_epochs}\n",
        "        t = self.last_epoch + 1\n",
        "\n",
        "        lrs = []\n",
        "        for g in self.optimizer.param_groups:\n",
        "            init = float(g.get('_init_lr', g['lr']))\n",
        "            # Exponential growth per Eq.(9), clamped to base_lr\n",
        "            lr_t = init * (self.c ** t)\n",
        "            lrs.append(min(lr_t, self.base_lr))  # Don't exceed base_lr\n",
        "        return lrs\n",
        "\n",
        "\n",
        "def get_scheduler(config, optimizer):\n",
        "    \"\"\"\n",
        "    Build LeRaC scheduler: Exponential warmup → Cosine annealing\n",
        "    \"\"\"\n",
        "    num_epochs = int(config['TRAIN']['END_EPOCH'])\n",
        "    warmup_epochs = int(config['TRAIN']['LR_CURRICULUM'].get('WARMUP_EPOCHS', 5))\n",
        "    base_lr = float(config['TRAIN']['LR'])\n",
        "    min_lr = float(config['TRAIN']['LR_SCHEDULER']['ARGS'].get('min_lr', 1e-7))\n",
        "    c = float(config['TRAIN']['LR_CURRICULUM'].get('C', 10.0))  # Growth factor\n",
        "\n",
        "    # Store initial LRs for LeRaC warmup\n",
        "    for g in optimizer.param_groups:\n",
        "        if '_init_lr' not in g:\n",
        "            g['_init_lr'] = g['lr']\n",
        "\n",
        "    # Phase 1: LeRaC exponential warmup\n",
        "    lerac_scheduler = LeRaCScheduler(\n",
        "        optimizer,\n",
        "        base_lr=base_lr,\n",
        "        warmup_epochs=warmup_epochs,\n",
        "        c=c,\n",
        "        last_epoch=-1\n",
        "    )\n",
        "\n",
        "    # Phase 2: Cosine annealing after warmup\n",
        "    cosine_epochs = max(1, num_epochs - warmup_epochs)\n",
        "    cosine_scheduler = CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=cosine_epochs,\n",
        "        eta_min=min_lr\n",
        "    )\n",
        "\n",
        "    # Combine: LeRaC warmup → Cosine decay\n",
        "    scheduler = SequentialLR(\n",
        "        optimizer,\n",
        "        schedulers=[lerac_scheduler, cosine_scheduler],\n",
        "        milestones=[warmup_epochs]\n",
        "    )\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"LeRaC Scheduler Configuration (ImageNet100)\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Phase 1 - LeRaC Warmup:\")\n",
        "    print(f\"  Duration: {warmup_epochs} epochs\")\n",
        "    print(f\"  Growth factor (c): {c}\")\n",
        "    print(f\"  Target LR: {base_lr}\")\n",
        "    print(f\"\\nPhase 2 - Cosine Annealing:\")\n",
        "    print(f\"  Duration: {cosine_epochs} epochs\")\n",
        "    print(f\"  Start LR: {base_lr}\")\n",
        "    print(f\"  Min LR: {min_lr}\")\n",
        "    print(f\"\\nTotal epochs: {num_epochs}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return scheduler\n",
        "\n",
        "\n",
        "# Build scheduler for ImageNet100\n",
        "scheduler = get_scheduler(config, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e06b86f6",
      "metadata": {
        "id": "e06b86f6"
      },
      "source": [
        "## Set Criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83a10b7d",
      "metadata": {
        "id": "83a10b7d"
      },
      "outputs": [],
      "source": [
        "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
        "\n",
        "def build_criterion(config):\n",
        "    \"\"\"\n",
        "    Build loss criterion with label smoothing\n",
        "    \"\"\"\n",
        "    if config['AUG']['MIXUP'] > 0.:\n",
        "        # Use soft target cross entropy for mixup/cutmix\n",
        "        criterion = SoftTargetCrossEntropy()\n",
        "    elif config['LOSS']['LABEL_SMOOTHING'] > 0.:\n",
        "        # Use label smoothing\n",
        "        criterion = LabelSmoothingCrossEntropy(smoothing=config['LOSS']['LABEL_SMOOTHING'])\n",
        "    else:\n",
        "        # Standard cross entropy\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    return criterion\n",
        "\n",
        "# Build criterion for training (with mixup support)\n",
        "criterion = build_criterion(config)\n",
        "\n",
        "# Build criterion for validation (no mixup/label smoothing)\n",
        "criterion_eval = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"\\nCriterion Info:\")\n",
        "print(f\"Training: Label Smoothing = {config['LOSS']['LABEL_SMOOTHING']}\")\n",
        "print(f\"Training: Mixup/Cutmix enabled = {config['AUG']['MIXUP'] > 0.}\")\n",
        "print(f\"Validation: Standard CrossEntropyLoss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "366b4a18",
      "metadata": {
        "id": "366b4a18"
      },
      "outputs": [],
      "source": [
        "from timm.data.mixup import Mixup\n",
        "\n",
        "def build_mixup(config):\n",
        "    \"\"\"\n",
        "    Build mixup/cutmix augmentation\n",
        "    \"\"\"\n",
        "    mixup_fn = None\n",
        "    mixup_active = config['AUG']['MIXUP'] > 0 or config['AUG']['MIXCUT'] > 0.\n",
        "\n",
        "    if mixup_active:\n",
        "        mixup_fn = Mixup(\n",
        "            mixup_alpha=config['AUG']['MIXUP'],\n",
        "            cutmix_alpha=config['AUG']['MIXCUT'],\n",
        "            prob=config['AUG']['MIXUP_PROB'],\n",
        "            switch_prob=0.5,\n",
        "            mode='batch',\n",
        "            label_smoothing=config['LOSS']['LABEL_SMOOTHING'],\n",
        "            num_classes=config['MODEL']['SPEC']['NUM_CLASSES']\n",
        "        )\n",
        "\n",
        "    return mixup_fn\n",
        "\n",
        "# Build mixup function\n",
        "mixup_fn = build_mixup(config)\n",
        "print(f\"\\nMixup Info:\")\n",
        "print(f\"Mixup alpha: {config['AUG']['MIXUP']}\")\n",
        "print(f\"Cutmix alpha: {config['AUG']['MIXCUT']}\")\n",
        "print(f\"Mixup probability: {config['AUG']['MIXUP_PROB']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1443a04c",
      "metadata": {
        "id": "1443a04c"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1193ec5c",
      "metadata": {
        "id": "1193ec5c"
      },
      "outputs": [],
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    maxk = min(max(topk), output.size()[1])\n",
        "    batch_size = target.size(0)\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
        "    return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d7c3963",
      "metadata": {
        "id": "4d7c3963"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch, model, train_loader, criterion, optimizer, scheduler,\n",
        "                    config, scaler, mixup_fn):\n",
        "    \"\"\"\n",
        "    Train for one epoch\n",
        "    Following: https://github.com/microsoft/CvT/blob/main/lib/core/function.py\n",
        "    Note: scheduler is passed but NOT stepped here - stepping happens in main loop\n",
        "    \"\"\"\n",
        "    losses = AverageMeter()\n",
        "    acc_m = AverageMeter()\n",
        "    model.train()\n",
        "\n",
        "    accumulation_steps = config['TRAIN'].get('GRADIENT_ACCUMULATION_STEPS', 1)\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n",
        "\n",
        "    for idx, (images, targets) in enumerate(train_loader):\n",
        "        images = images.to(DEVICE, non_blocking=True)\n",
        "        targets = targets.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        # Apply mixup\n",
        "        images, targets = mixup_fn(images, targets)\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with torch.cuda.amp.autocast(enabled=True):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss = loss / accumulation_steps\n",
        "\n",
        "        # Backward pass\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Gradient accumulation step\n",
        "        if (idx + 1) % accumulation_steps == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "\n",
        "            # Optional: Print gradient norms for debugging\n",
        "            if idx % 100 == 0 and epoch >= 6:\n",
        "                total_norm = 0\n",
        "                for p in model.parameters():\n",
        "                    if p.grad is not None:\n",
        "                        total_norm += p.grad.norm().item() ** 2\n",
        "                total_norm = total_norm ** 0.5\n",
        "                print(f\"Epoch {epoch}, Batch {idx}, Grad norm: {total_norm:.4f}\")\n",
        "\n",
        "            # Gradient clipping (manual + torch utility)\n",
        "            for param in model.parameters():\n",
        "                if param.grad is not None:\n",
        "                    param.grad.data.clamp_(-0.5, 0.5)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "\n",
        "            # Optimizer step\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        targets_for_acc = torch.argmax(targets, dim=1)\n",
        "        acc = accuracy(outputs, targets_for_acc)\n",
        "\n",
        "        # Update metrics\n",
        "        losses.update(loss.item() * accumulation_steps, images.size(0))\n",
        "        acc_m.update(acc[0].item(), images.size(0))\n",
        "\n",
        "        # Update progress bar\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.02f}% ({:.02f}%)\".format(acc[0].item(), acc_m.avg),\n",
        "            loss=\"{:.04f} ({:.04f})\".format(loss.item() * accumulation_steps, losses.avg),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr']))\n",
        "        )\n",
        "        batch_bar.update()\n",
        "\n",
        "    # Handle final accumulated gradients if any\n",
        "    if (idx + 1) % accumulation_steps != 0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        for param in model.parameters():\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.clamp_(-0.5, 0.5)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return losses.avg, acc_m.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d111e7d1",
      "metadata": {
        "id": "d111e7d1"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate(model, val_loader, criterion, config):\n",
        "    losses = AverageMeter()\n",
        "    acc_m = AverageMeter()\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val.', ncols=5)\n",
        "\n",
        "    for idx, (images, targets) in enumerate(val_loader):\n",
        "        images = images.to(DEVICE, non_blocking=True)\n",
        "        targets = targets.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        acc = accuracy(outputs, targets)\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        acc_m.update(acc[0].item(), images.size(0))\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.02f}% ({:.02f}%)\".format(acc[0].item(), acc_m.avg),\n",
        "            loss=\"{:.04f} ({:.04f})\".format(loss.item(), losses.avg))\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "    print(f' * Acc {acc_m.avg:.3f}')\n",
        "\n",
        "    return losses.avg, acc_m.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ae0f47e",
      "metadata": {
        "id": "3ae0f47e"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         'epoch'                    : epoch},\n",
        "         path)\n",
        "\n",
        "\n",
        "def load_model(model, optimizer=None, scheduler=None, path='./checkpoint.pth'):\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    else:\n",
        "        optimizer = None\n",
        "    if scheduler is not None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    else:\n",
        "        scheduler = None\n",
        "    epoch = checkpoint['epoch']\n",
        "    return model, optimizer, scheduler, epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da521cae",
      "metadata": {
        "id": "da521cae"
      },
      "outputs": [],
      "source": [
        "os.makedirs(config['OUTPUT_DIR'], exist_ok=True)\n",
        "model = model.to(DEVICE)\n",
        "scaler = scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea26dc10",
      "metadata": {
        "id": "ea26dc10"
      },
      "outputs": [],
      "source": [
        "wandb.login(key=os.environ.get('WANDB_API_KEY')) # API Key is in your wandb account, under settings (wandb.ai/settings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1871908",
      "metadata": {
        "id": "c1871908"
      },
      "outputs": [],
      "source": [
        "# For a NEW run (ResNet-18 with LeRaC)\n",
        "run = wandb.init(\n",
        "    name=\"imagenet100-resnet18-lerac-v1\",  # NEW name for your ResNet run\n",
        "    project=\"idl-project\",\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38aa432a",
      "metadata": {
        "id": "38aa432a"
      },
      "outputs": [],
      "source": [
        "gc.collect() # These commands help you when you face CUDA OOM error\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79070f24",
      "metadata": {
        "id": "79070f24"
      },
      "outputs": [],
      "source": [
        "# Training loop with checkpoint saving every 10 epochs\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "best_loss = -1\n",
        "start_epoch = config['TRAIN']['BEGIN_EPOCH']\n",
        "end_epoch = config['TRAIN']['END_EPOCH']\n",
        "\n",
        "print(\"Starting Training\")\n",
        "print(f\"Epochs: {start_epoch} -> {end_epoch}\")\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Checkpoint save frequency: Every 10 epochs\")\n",
        "\n",
        "for epoch in range(start_epoch, end_epoch):\n",
        "    # Training\n",
        "    print(f\"\\nEpoch {epoch+1}/{end_epoch}\")\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        epoch, model, train_loader, criterion, optimizer, scheduler,\n",
        "        config, scaler, mixup_fn\n",
        "    )\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion_eval, config)\n",
        "\n",
        "    scheduler.step()\n",
        "    # Check if best model\n",
        "    is_best = (best_loss == -1) or (val_loss < best_loss)\n",
        "    if is_best:\n",
        "        best_loss = val_loss\n",
        "\n",
        "    # Save checkpoint every 10 epochs OR if it's the best model\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        checkpoint_path = os.path.join(config['OUTPUT_DIR'], f'epoch_{epoch+1}.pth')\n",
        "        save_model(model, optimizer, scheduler, epoch, checkpoint_path)\n",
        "        print(f\"✓ Saved checkpoint: epoch_{epoch+1}.pth\")\n",
        "\n",
        "    # Always save best model\n",
        "    if is_best:\n",
        "        best_path = os.path.join(config['OUTPUT_DIR'], 'best.pth')\n",
        "        save_model(model, optimizer, scheduler, epoch, best_path)\n",
        "        print(f\"✓ Saved best model: best.pth (Val Loss: {val_loss:.4f})\")\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"Epoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"  Train Acc: {train_acc:.3f}, Val Acc: {val_acc:.3f}\")\n",
        "    print(f\"  Best Val Loss: {best_loss:.4f}\")\n",
        "\n",
        "    # Log to wandb\n",
        "    metrics = {\n",
        "        'train_loss': train_loss,\n",
        "        'val_loss': val_loss,\n",
        "        'train_acc': train_acc,\n",
        "        'val_acc': val_acc,\n",
        "        'epoch': epoch,\n",
        "        'best_val_loss': best_loss\n",
        "    }\n",
        "    if run is not None:\n",
        "        run.log(metrics)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Training Complete!\")\n",
        "print(f\"Best Validation Loss: {best_loss:.4f}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c86f9ec",
      "metadata": {
        "id": "9c86f9ec"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# ========================================\n",
        "# STEP 1: LOAD CHECKPOINT FROM Best Epoch\n",
        "# ========================================\n",
        "resume_path = os.path.join(config['OUTPUT_DIR'], 'best.pth')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"Loading checkpoint: {resume_path}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "model, optimizer, scheduler, loaded_epoch = load_model(\n",
        "    model, optimizer, scheduler,\n",
        "    path=resume_path\n",
        ")\n",
        "\n",
        "# Set start epoch for training loop\n",
        "start_epoch = loaded_epoch + 1\n",
        "end_epoch = config['TRAIN']['END_EPOCH']\n",
        "\n",
        "print(f\"✓ Checkpoint loaded successfully!\")\n",
        "print(f\"   Completed epoch: {loaded_epoch}\")\n",
        "print(f\"   Resuming from: {start_epoch}\")\n",
        "print(f\"   Training until: {end_epoch}\")\n",
        "print(f\"   Remaining epochs: {end_epoch - start_epoch}\")\n",
        "print(\"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "867bef61",
      "metadata": {
        "id": "867bef61"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test(model, test_loader, config):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for images, targets in tqdm(test_loader, desc='Testing'):\n",
        "        images = images.to(DEVICE, non_blocking=True)\n",
        "        targets = targets.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=True):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "        _, preds = outputs.topk(1, 1, True, True)\n",
        "\n",
        "        all_predictions.extend(preds.cpu().numpy().flatten())\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "        all_probabilities.extend(probs.cpu().numpy())\n",
        "\n",
        "        acc1 = accuracy(outputs, targets)\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        top1.update(acc1[0].item(), images.size(0))\n",
        "\n",
        "    return {\n",
        "        'predictions': np.array(all_predictions),\n",
        "        'targets': np.array(all_targets),\n",
        "        'probabilities': np.array(all_probabilities),\n",
        "        'loss': losses.avg,\n",
        "        'top1_acc': top1.avg\n",
        "    }\n",
        "\n",
        "test_results = test(model, val_loader, config)\n",
        "\n",
        "print(\"Final Test Results\")\n",
        "print(f\"Test Loss: {test_results['loss']:.4f}\")\n",
        "print(f\"Test Acc@1: {test_results['top1_acc']:.3f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}