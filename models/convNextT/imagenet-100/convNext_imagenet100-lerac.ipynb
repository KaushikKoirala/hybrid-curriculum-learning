{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MVq8X-kvyx1"
   },
   "source": [
    "# ConvNeXt-Tiny Lerac Training on ImageNet-100\n",
    "(Will fix the intro when got time)\n",
    "This notebook implements a baseline ConvNeXt-Tiny model trained on ImageNet-100, focusing on modernizing the architecture to align with ViT principles while retaining the convolutional nature.\n",
    "\n",
    "## Dataset: ImageNet-100\n",
    "- 100 classes subset of ImageNet\n",
    "\n",
    "- 224x224 input images\n",
    "\n",
    "- Standard data augmentation\n",
    "\n",
    "## Model: ConvNeXt-Tiny\n",
    "- Architecture based on modernizing a ResNet structure towards Vision Transformer design\n",
    "\n",
    "- ~28.6M parameters (specific to ConvNeXt-Tiny)\n",
    "\n",
    "- Features include: large kernel sizes, use of Layer Normalization, and GELU activation\n",
    "\n",
    "- Conventional training (standard image classification setup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlbT_PETvyx2",
    "outputId": "fc8c1ebe-1bde-42e5-bc75-29cee93bb2da"
   },
   "outputs": [],
   "source": [
    "%pip install torchsummary torchvision tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6l-Qiu0lvyx3",
    "outputId": "37ca1b12-b9f6-4def-d0c9-1baab794707a"
   },
   "outputs": [],
   "source": [
    "import math, os, time, copy, random\n",
    "import numpy as np\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED);  torch.manual_seed(SEED);  torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGCxfNXWvyx3",
    "outputId": "a48088d0-81ad-410e-86a5-b81d677cb283"
   },
   "outputs": [],
   "source": [
    "CFG = dict(\n",
    "    num_epochs       = 100,          # keep it or change as you wish\n",
    "    batch_size       = 128,          # tune to your GPU memory\n",
    "    lr               = 4e-3,         # a good starting point for ImageNet-size data\n",
    "    weight_decay     = 0.05,\n",
    "    warmup_epochs    = 5,\n",
    "    num_workers      = 8,            # ImageNet is stored on disk â†’ use more workers\n",
    "    image_size       = 224,\n",
    "    amp              = True,\n",
    "    pretrained       = False,\n",
    "    ckpt_dir         = \"./imagenet_lerac_checkpoint\",\n",
    "\n",
    "    dataset_path     = \"ImageNet100_224\",\n",
    "    num_classes      = 100,\n",
    "\n",
    "    # --- LeRaC Hyperparameters (YOU MUST TUNE THESE) ---\n",
    "    lerac_end_lr     = 1e-8,     # This is eta_n^0\n",
    ")\n",
    "Path(CFG[\"ckpt_dir\"]).mkdir(exist_ok=True)\n",
    "\n",
    "CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWGjiFesvyx4",
    "outputId": "32a89cea-d02e-4032-df91-d5da12be533e"
   },
   "outputs": [],
   "source": [
    "def get_imagenet100_loaders(data_path, batch_size=128, num_workers=5):\n",
    "    \"\"\"Load ImageNet-100 dataset with standard augmentation\"\"\"\n",
    "\n",
    "    # ImageNet normalization\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                   std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # Training transforms with augmentation\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    # Validation transforms (no augmentation)\n",
    "    transform_val = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    # Load datasets\n",
    "    train_dir = Path(data_path) / 'train'\n",
    "    val_dir = Path(data_path) / 'val'\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "    val_dataset = datasets.ImageFolder(val_dir, transform=transform_val)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, train_dataset, val_dataset\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading ImageNet-100 dataset...\")\n",
    "train_loader, val_loader, train_dataset, val_dataset = get_imagenet100_loaders(\n",
    "    data_path=CFG['dataset_path'],\n",
    "    batch_size=CFG['batch_size'],\n",
    "    num_workers=CFG['num_workers']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
    "print(f\"Classes: {train_dataset.classes[:10]}...\" if len(train_dataset.classes) > 10 else f\"Classes: {train_dataset.classes}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3ak7jzrvyx4",
    "outputId": "57ad7db5-789d-4f96-f009-1648d358ddbe"
   },
   "outputs": [],
   "source": [
    "if CFG[\"pretrained\"]:\n",
    "    weights = models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1\n",
    "else:\n",
    "    weights = models.ConvNeXt_Tiny_Weights.DEFAULT\n",
    "\n",
    "model = models.convnext_tiny(weights=weights)\n",
    "\n",
    "# Replace classification head (last linear) to output 10 classes\n",
    "in_features = model.classifier[-1].in_features\n",
    "model.classifier[-1] = nn.Linear(in_features, CFG[\"num_classes\"])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTSVE0ciwalc"
   },
   "outputs": [],
   "source": [
    "def get_convnext_tiny_lerac_groups(model, base_lr, lerac_end_lr):\n",
    "    \"\"\"\n",
    "    Creates parameter groups for ConvNextTiny with LeRaC initial LRs.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The ConvNextTiny model.\n",
    "        base_lr (float): The base learning rate (eta^0), for the first layer[cite: 210, 313].\n",
    "        lerac_end_lr (float): The final learning rate (eta_n^0), for the last layer[cite: 210].\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the logical \"layers\" of ConvNextTiny from input to output\n",
    "    # This is a manual process based on the model architecture.\n",
    "    layers = []\n",
    "\n",
    "    # 1. Features (Backbone)\n",
    "    if hasattr(model, 'features'):\n",
    "        layers.extend([\n",
    "            model.features[0],  # Stem\n",
    "            model.features[1],  # Stage 1\n",
    "            model.features[2],  # Downsample\n",
    "            model.features[3],  # Stage 2\n",
    "            model.features[4],  # Downsample\n",
    "            model.features[5],  # Stage 3\n",
    "            model.features[6],  # Downsample\n",
    "            model.features[7],  # Stage 4\n",
    "        ])\n",
    "    else:\n",
    "        print(\"Warning: 'model.features' not found. Check model architecture.\")\n",
    "\n",
    "    # 2. Classifier Head\n",
    "    if hasattr(model, 'classifier'):\n",
    "        # Add the LayerNorm before the head\n",
    "        if isinstance(model.classifier[0], nn.LayerNorm):\n",
    "             layers.append(model.classifier[0])\n",
    "\n",
    "        # Add the final Linear layer (user-replaced)\n",
    "        if isinstance(model.classifier[-1], nn.Linear):\n",
    "            layers.append(model.classifier[-1])\n",
    "        else:\n",
    "            print(f\"Warning: Expected nn.Linear at model.classifier[-1], but found {type(model.classifier[-1])}.\")\n",
    "    else:\n",
    "         print(\"Warning: 'model.classifier' not found. Check model architecture.\")\n",
    "\n",
    "    num_layers = len(layers)\n",
    "    if num_layers == 0:\n",
    "        print(\"Error: No layers were found. Returning default parameter group.\")\n",
    "        return model.parameters()\n",
    "\n",
    "    print(f\"LeRaC: Found {num_layers} logical layers for parameter groups.\")\n",
    "\n",
    "    # Generate the initial learning rates (linear interpolation in log-space)\n",
    "    # [eta_1^0, ..., eta_n^0]\n",
    "    initial_lerac_lrs = np.logspace(\n",
    "        np.log10(base_lr),\n",
    "        np.log10(lerac_end_lr),\n",
    "        num_layers\n",
    "    )\n",
    "\n",
    "    param_groups = []\n",
    "    for layer, lr in zip(layers, initial_lerac_lrs):\n",
    "        param_groups.append({\n",
    "            'params': layer.parameters(),\n",
    "            'lr': lr\n",
    "        })\n",
    "\n",
    "    return param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMeBfxBGwfLI"
   },
   "outputs": [],
   "source": [
    "class LeRaCScheduler(_LRScheduler):\n",
    "    \"\"\"\n",
    "    Implements the Learning Rate Curriculum (LeRaC) scheduler.\n",
    "\n",
    "    This scheduler increases the learning rate of each parameter group\n",
    "    from its initial value (eta_j^0) to a target value (eta^0) over\n",
    "    a specified number of iterations (k)[cite: 15, 194].\n",
    "\n",
    "    This scheduler should be stepped *every iteration*.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): The optimizer with LeRaC parameter groups.\n",
    "        target_lr (float): The target learning rate (eta^0) that all groups\n",
    "                           will reach at iteration k[cite: 190].\n",
    "        num_iterations (int): The number of iterations (k) for the curriculum[cite: 196].\n",
    "        c (float): The base for the exponential scheduler[cite: 203].\n",
    "                   The paper fixes this at 10[cite: 203, 313].\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, target_lr, num_iterations, c=10.0, last_epoch=-1):\n",
    "        self.target_lr = target_lr\n",
    "        self.num_iterations = num_iterations\n",
    "        self.c = c\n",
    "        self.k = num_iterations\n",
    "\n",
    "        # self.base_lrs stores the initial LRs (eta_j^0) for each group\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        # self.last_epoch is the current *iteration* number (t)\n",
    "        t = self.last_epoch\n",
    "\n",
    "        # If curriculum is over, all LRs are the target_lr\n",
    "        if t > self.k:\n",
    "            return [self.target_lr for _ in self.base_lrs]\n",
    "\n",
    "        new_lrs = []\n",
    "        for eta_0_j in self.base_lrs: # eta_0_j is the initial LR for group j\n",
    "            eta_k = self.target_lr\n",
    "\n",
    "            # Avoid division by zero if eta_0_j is 0\n",
    "            if eta_0_j == 0:\n",
    "                new_lrs.append(0.0)\n",
    "                continue\n",
    "\n",
    "            # This implements Eq. 9: eta_j(t) = eta_j(0) * c^((t/k) * log_c(eta_k / eta_j(0)))\n",
    "            #\n",
    "            log_ratio = np.log(eta_k / eta_0_j) / np.log(self.c)\n",
    "            exponent = (t / (self.k - 1.0)) * log_ratio\n",
    "            new_lr = eta_0_j * (self.c ** exponent)\n",
    "            new_lrs.append(new_lr)\n",
    "\n",
    "        return new_lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjuPgQPAwhnm"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "# Create LeRaC Parameter Groups\n",
    "print(\"Setting up LeRaC parameter groups...\")\n",
    "param_groups = get_convnext_tiny_lerac_groups(\n",
    "    model,\n",
    "    CFG[\"lr\"],\n",
    "    CFG[\"lerac_end_lr\"]\n",
    ")\n",
    "\n",
    "# Create Optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    param_groups,\n",
    "    lr=CFG[\"lr\"], # This default lr is ignored\n",
    "    weight_decay=CFG[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "\n",
    "def build_warmup_cosine_scheduler(optimizer, steps_per_epoch, num_epochs,\n",
    "                                  warmup_epochs=1, eta_min=1e-5, accum_steps=1):\n",
    "    steps_per_epoch = math.ceil(steps_per_epoch / max(1, accum_steps))\n",
    "    total_steps = num_epochs * steps_per_epoch\n",
    "    warmup_steps = warmup_epochs * steps_per_epoch\n",
    "    cosine_steps = max(1, total_steps - warmup_steps)\n",
    "\n",
    "    scheds = []\n",
    "    milestones = []\n",
    "\n",
    "    if warmup_steps > 0:\n",
    "        s1 = LeRaCScheduler(\n",
    "            optimizer,\n",
    "            target_lr=CFG[\"lr\"],\n",
    "            num_iterations=warmup_steps\n",
    "        )\n",
    "\n",
    "        scheds.append(s1)\n",
    "        milestones.append(warmup_steps)\n",
    "\n",
    "    s2 = CosineAnnealingLR(optimizer, T_max=cosine_steps, eta_min=eta_min)\n",
    "    scheds.append(s2)\n",
    "\n",
    "    scheduler = SequentialLR(optimizer, schedulers=scheds, milestones=milestones or [0])\n",
    "    return scheduler\n",
    "\n",
    "scheduler = build_warmup_cosine_scheduler(\n",
    "    optimizer,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    num_epochs=CFG[\"num_epochs\"],\n",
    "    warmup_epochs=CFG[\"warmup_epochs\"]\n",
    ")\n",
    "\n",
    "scaler     = torch.cuda.amp.GradScaler(enabled=CFG[\"amp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J02jkqcCvyx5"
   },
   "outputs": [],
   "source": [
    "def accuracy(preds, targets, topk=(1,)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        _, pred = preds.topk(maxk, dim=1, largest=True, sorted=True)\n",
    "        pred   = pred.t()\n",
    "        correct= pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "        return [correct[:k].reshape(-1).float().mean().item()*100. for k in topk]\n",
    "\n",
    "\n",
    "def run_epoch(loader, model, optimizer=None, epoch:int=0, phase:str=\"train\"):\n",
    "    \"\"\"\n",
    "    If `optimizer` is given â†’ training mode, otherwise evaluation mode.\n",
    "    Memory-safe: no graph is kept when we don't need gradients.\n",
    "    \"\"\"\n",
    "    train = optimizer is not None\n",
    "    model.train(train)\n",
    "\n",
    "    running_loss, running_acc = 0.0, 0.0\n",
    "    steps = len(loader)\n",
    "\n",
    "    bar = tqdm(loader, desc=f\"{phase.title():>5} | Epoch {epoch:02}\", leave=False)\n",
    "\n",
    "    # Choose the right context managers\n",
    "    grad_ctx = nullcontext() if train else torch.no_grad()\n",
    "    amp_ctx  = torch.amp.autocast(device_type=\"cuda\",\n",
    "                                  dtype=torch.float16,\n",
    "                                  enabled=CFG[\"amp\"] and torch.cuda.is_available())\n",
    "\n",
    "    with grad_ctx:\n",
    "        for images, labels in bar:\n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device)\n",
    "\n",
    "            with amp_ctx:\n",
    "                outputs = model(images)\n",
    "                loss    = criterion(outputs, labels)\n",
    "\n",
    "            if train:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer); scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_acc  += accuracy(outputs, labels)[0]\n",
    "            bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    torch.cuda.empty_cache()     # free any leftover cached blocks\n",
    "    return running_loss/steps, running_acc/steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "714EhUlgvyx5"
   },
   "outputs": [],
   "source": [
    "gc.collect() # These commands help you when you face CUDA OOM error\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDbGA1TVvyx5",
    "outputId": "bef70d7a-72d6-414c-f9b7-1b5e33598748"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_dir = CFG['ckpt_dir']\n",
    "\n",
    "print(checkpoint_dir)\n",
    "print(os.path.exists(checkpoint_dir))\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58Kc2Moxvyx6"
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, metrics, epoch, path):\n",
    "    torch.save(\n",
    "        {'model_state_dict'         : model.state_dict(),\n",
    "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
    "         'scheduler_state_dict'     : scheduler.state_dict() if scheduler is not None else '',\n",
    "         'metric'                   : metrics,\n",
    "         'epoch'                    : epoch},\n",
    "         path)\n",
    "\n",
    "\n",
    "def load_model(model, optimizer=None, scheduler=None, path=f\"{CFG['ckpt_dir']}/current_epoch.pth\"):\n",
    "    checkpoint = torch.load(path, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    else:\n",
    "        optimizer = None\n",
    "    if scheduler is not None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    else:\n",
    "        scheduler = None\n",
    "    epoch = checkpoint['epoch']\n",
    "    metrics = checkpoint['metric']\n",
    "    return model, optimizer, scheduler, epoch, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IaYz9WRwvyx6",
    "outputId": "9724917d-cccc-4897-ca2e-38c0958dfd37"
   },
   "outputs": [],
   "source": [
    "best_val_acc = 0.0\n",
    "patience = 16\n",
    "epoches_no_improve = 0\n",
    "\n",
    "history = {\"train_loss\": [], \"train_acc\": [],\n",
    "           \"val_loss\": [],   \"val_acc\": []}\n",
    "\n",
    "for epoch in range(1, CFG[\"num_epochs\"]+1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, model, optimizer, epoch, \"train\")\n",
    "    val_loss, val_acc= run_epoch(val_loader,   model, None,     epoch, \"val\")\n",
    "\n",
    "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_loss\"].append(val_loss);   history[\"val_acc\"].append(val_acc)\n",
    "    \n",
    "    metrics = {\n",
    "        \"train_loss\": tr_loss,\n",
    "        \"train_acc\": tr_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_loss,\n",
    "    }\n",
    "\n",
    "    if val_acc >= best_val_acc:\n",
    "        epoches_no_improve = 0\n",
    "        best_val_acc = val_acc\n",
    "\n",
    "#         save_model(model, optimizer, scheduler, metrics, epoch, f\"{CFG['ckpt_dir']}/best_convnext_tiny.pth\")\n",
    "#         print(\"Saved best val acc model\")\n",
    "\n",
    "    else:\n",
    "        epoches_no_improve += 1\n",
    "\n",
    "    try:\n",
    "        os.replace(f\"{CFG['ckpt_dir']}/current_epoch.pth\", f\"{CFG['ckpt_dir']}/last_epoch.pth\")\n",
    "        print(\"Saved last epoch model\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred when creating last.pth: {e}\")\n",
    "\n",
    "    save_model(model, optimizer, scheduler, metrics, epoch, f\"{CFG['ckpt_dir']}/current_epoch.pth\")\n",
    "    print(f\"Saved epoch {epoch} model\")\n",
    "\n",
    "    print(f\"Epoch {epoch:02}/{CFG['num_epochs']} \"\n",
    "          f\"| train loss {tr_loss:.4f} acc {tr_acc:.2f}% \"\n",
    "          f\"| val loss {val_loss:.4f} acc {val_acc:.2f}% \"\n",
    "          f\"| lr {scheduler.get_last_lr()[0]:.2e} \"\n",
    "          f\"| time {(time.time()-t0):.1f}s\")\n",
    "\n",
    "    if epoches_no_improve >= patience:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ug5m3ofvyx6"
   },
   "source": [
    "# Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dD9MMSwWvyx7"
   },
   "outputs": [],
   "source": [
    "model, optimizer, scheduler, start_epoch, metrics = load_model(model, optimizer, scheduler, f\"{CFG['ckpt_dir']}/best_convnext_tiny.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SatGWqKAvyx7",
    "outputId": "e8b4b364-a037-40cf-f9b8-420248be9c35"
   },
   "outputs": [],
   "source": [
    "optimizer, scheduler, start_epoch, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bw-cq9P0vyx7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
