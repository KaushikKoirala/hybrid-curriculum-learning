{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f944d09",
   "metadata": {},
   "source": [
    "Code for Cvt-13 largely based off of: https://github.com/microsoft/CvT/blob/main/lib/dataset/transformas/build.py\n",
    "and https://arxiv.org/pdf/2103.15808\n",
    "with some modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchsummary torchvision tqdm wandb typing_extensions pytorch_metric_learning timm==0.9.16 einops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3c0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import datasets, transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import gc\n",
    "# from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics as mt\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import glob\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_metric_learning import samplers\n",
    "import csv\n",
    "import logging\n",
    "from timm.data import create_loader, create_transform\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params largely copied from https://github.com/leoxiaobin/CvT/blob/main/experiments/imagenet/cvt/cvt-13-224x224.yaml\n",
    "config = {\n",
    "  \"OUTPUT_DIR\": \"OUTPUT/CIFAR-10-LeRaC-Attempt-13\",\n",
    "  \"WORKERS\": 8,\n",
    "  \"PRINT_FREQ\": 500,\n",
    "  \"AMP\": {\n",
    "    \"ENABLED\": True\n",
    "  },\n",
    "  \"MODEL\": {\n",
    "    \"NAME\": \"cls_cvt\",\n",
    "    \"SPEC\": {\n",
    "      \"INIT\": \"trunc_norm\",\n",
    "      \"NUM_STAGES\": 3,\n",
    "      \"PATCH_SIZE\": [7, 3, 3],\n",
    "      \"PATCH_STRIDE\": [4, 2, 2],\n",
    "      \"PATCH_PADDING\": [2, 1, 1],\n",
    "      \"DIM_EMBED\": [64, 192, 384],\n",
    "      \"NUM_HEADS\": [1, 3, 6],\n",
    "      \"DEPTH\": [1, 2, 10],\n",
    "      \"MLP_RATIO\": [4.0, 4.0, 4.0],\n",
    "      \"ATTN_DROP_RATE\": [0.0, 0.0, 0.0],\n",
    "      \"DROP_RATE\": [0.0, 0.0, 0.0],\n",
    "      \"DROP_PATH_RATE\": [0.0, 0.0, 0.1],\n",
    "      \"QKV_BIAS\": [True, True, True],\n",
    "      \"CLS_TOKEN\": [False, False, True],\n",
    "      \"POS_EMBED\": [False, False, False],\n",
    "      \"QKV_PROJ_METHOD\": [\"dw_bn\", \"dw_bn\", \"dw_bn\"],\n",
    "      \"KERNEL_QKV\": [3, 3, 3],\n",
    "      \"PADDING_KV\": [1, 1, 1],\n",
    "      \"STRIDE_KV\": [2, 2, 2],\n",
    "      \"PADDING_Q\": [1, 1, 1],\n",
    "      \"STRIDE_Q\": [1, 1, 1]\n",
    "    }\n",
    "  },\n",
    "  \"AUG\": {\n",
    "    \"MIXUP_PROB\": 1.0,\n",
    "    \"MIXUP\": 0.8,\n",
    "    \"MIXCUT\": 1.0,\n",
    "    \"TIMM_AUG\": {\n",
    "      \"USE_LOADER\": False,\n",
    "      \"RE_COUNT\": 1,\n",
    "      \"RE_MODE\": \"pixel\",\n",
    "      \"RE_SPLIT\": False,\n",
    "      \"RE_PROB\": 0.25,\n",
    "      \"AUTO_AUGMENT\": \"rand-m9-mstd0.5-inc1\",\n",
    "      \"HFLIP\": 0.5,\n",
    "      \"VFLIP\": 0.0,\n",
    "      \"COLOR_JITTER\": 0.4,\n",
    "      \"INTERPOLATION\": \"bicubic\"\n",
    "    }\n",
    "  },\n",
    "  \"LOSS\": {\n",
    "    \"LABEL_SMOOTHING\": 0.1\n",
    "  },\n",
    "  \"CUDNN\": {\n",
    "    \"BENCHMARK\": True,\n",
    "    \"DETERMINISTIC\": False,\n",
    "    \"ENABLED\": True\n",
    "  },\n",
    "  \"DATASET\": {\n",
    "    \"DATASET\": \"cifar-10\",\n",
    "    \"DATA_FORMAT\": \"jpg\",\n",
    "    \"ROOT\": \"./cifar-10\",\n",
    "    \"TEST_SET\": \"val\",\n",
    "    \"TRAIN_SET\": \"train\"\n",
    "  },\n",
    "  \"TEST\": {\n",
    "    \"BATCH_SIZE_PER_GPU\": 256,\n",
    "    \"IMAGE_SIZE\": [32, 32],\n",
    "    \"MODEL_FILE\": \"\",\n",
    "    \"INTERPOLATION\": \"bicubic\"\n",
    "  },\n",
    "  \"TRAIN\": {\n",
    "    \"BATCH_SIZE_PER_GPU\": 512,\n",
    "    \"GRADIENT_ACCUMULATION_STEPS\": 4,       \n",
    "    \"LR\": .00125,\n",
    "    \"IMAGE_SIZE\": [32, 32],\n",
    "    \"BEGIN_EPOCH\": 0,\n",
    "    \"END_EPOCH\": 100,\n",
    "    \"LR_CURRICULUM\": {\n",
    "        \"MIN_LR\": 2e-6 #https://github.com/CroitoruAlin/LeRaC/blob/main/experiments/cvt_experiments.py#L78\n",
    "    },\n",
    "    \"LR_SCHEDULER\": {\n",
    "      \"METHOD\": \"timm\",\n",
    "      \"ARGS\": {\n",
    "        \"sched\": \"cosine\",\n",
    "        \"warmup_epochs\": 5,\n",
    "        \"warmup_lr\": 0.000001,\n",
    "        \"min_lr\": 0.00001,\n",
    "        \"cooldown_epochs\": 10,\n",
    "        \"decay_rate\": 0.1\n",
    "      }\n",
    "    },\n",
    "    \"OPTIMIZER\": \"adamW\",\n",
    "    \"WD\": 0.05,\n",
    "    \"WITHOUT_WD_LIST\": [\"bn\", \"bias\", \"ln\"],\n",
    "    \"SHUFFLE\": True\n",
    "  },\n",
    "  \"DEBUG\": {\n",
    "    \"DEBUG\": False\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29029b6",
   "metadata": {},
   "source": [
    "### Defining transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facc7444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transforms(config, is_train):\n",
    "    if is_train:\n",
    "        img_size = config['TRAIN']['IMAGE_SIZE'][0]\n",
    "        timm_cfg = config['AUG']['TIMM_AUG']\n",
    "        # hardcoded values are from defaults e.g., https://github.com/microsoft/CvT/blob/main/lib/config/default.py#L68\n",
    "        transforms = create_transform(\n",
    "            input_size = img_size,\n",
    "            is_training = True,\n",
    "            use_prefetcher=False,\n",
    "            no_aug=False,\n",
    "            re_prob=timm_cfg['RE_PROB'],\n",
    "            re_mode=timm_cfg['RE_MODE'],\n",
    "            re_count=timm_cfg['RE_COUNT'],\n",
    "            scale=(0.8, 1.0),\n",
    "            ratio=(3.0/4.0, 4.0/3.0),\n",
    "            hflip=timm_cfg['HFLIP'],\n",
    "            vflip=timm_cfg['VFLIP'],\n",
    "            color_jitter=timm_cfg['COLOR_JITTER'],\n",
    "            auto_augment=timm_cfg['AUTO_AUGMENT'],\n",
    "            interpolation=timm_cfg['INTERPOLATION'],\n",
    "            mean=(0.491, 0.482, 0.446), # https://stackoverflow.com/a/69699979\n",
    "            std=(0.247, 0.243, 0.261),            \n",
    "            \n",
    "        )\n",
    "    else:\n",
    "        normalize = T.Normalize(mean=[0.491, 0.482, 0.446], std=[0.247, 0.243, 0.261])\n",
    "        img_size = config['TEST']['IMAGE_SIZE'][0]\n",
    "        transforms = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c46aa4e",
   "metadata": {},
   "source": [
    "### Building Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f1b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def build_dataset(config, is_train):\n",
    "    '''\n",
    "    In the CIFAR file it will call the appropriate method\n",
    "    '''\n",
    "    dataset = None\n",
    "    transforms = build_transforms(config, is_train)\n",
    "    dataset = datasets.CIFAR10(root=config['DATASET']['ROOT'], train=is_train, download=True, transform=transforms)\n",
    "    logging.info(f'load samples: {len(dataset)}, is_train: {is_train}')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df8988",
   "metadata": {},
   "source": [
    "### Building Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ab5c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(config, is_train):\n",
    "    if is_train:\n",
    "        batch_size_per_gpu = config['TRAIN']['BATCH_SIZE_PER_GPU']\n",
    "        shuffle = True\n",
    "    else:\n",
    "        batch_size_per_gpu = config['TEST']['BATCH_SIZE_PER_GPU']\n",
    "        shuffle = False\n",
    "    dataset = build_dataset(config, is_train)\n",
    "    sampler = None # the cvt code has code to use multi-gpu. here we assume single A100 gpu\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size_per_gpu,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=config['WORKERS'],\n",
    "        pin_memory=True,\n",
    "        sampler=sampler,\n",
    "        drop_last=is_train,\n",
    "    )     \n",
    "    return data_loader\n",
    "train_loader = build_dataloader(config, is_train=True)\n",
    "val_loader = build_dataloader(config, is_train=False)\n",
    "\n",
    "print(f\"\\nDataLoader Info:\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_loader.dataset)}\")\n",
    "print(f\"Number of classes: {len(train_loader.dataset.classes)}\")\n",
    "\n",
    "# Test loading a batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"Images: {images.shape}\")\n",
    "print(f\"Labels: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd8966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVT code from: https://github.com/microsoft/CvT/blob/main/lib/models/cls_cvt.py\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "import collections.abc as container_abcs\n",
    "from collections import OrderedDict\n",
    "from einops.layers.torch import Rearrange\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "\n",
    "# From PyTorch internals\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, container_abcs.Iterable):\n",
    "            return x\n",
    "        return tuple(repeat(x, n))\n",
    "\n",
    "    return parse\n",
    "\n",
    "\n",
    "to_1tuple = _ntuple(1)\n",
    "to_2tuple = _ntuple(2)\n",
    "to_3tuple = _ntuple(3)\n",
    "to_4tuple = _ntuple(4)\n",
    "to_ntuple = _ntuple\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features=None,\n",
    "                 out_features=None,\n",
    "                 act_layer=QuickGELU,\n",
    "                 drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim_in,\n",
    "                 dim_out,\n",
    "                 num_heads,\n",
    "                 qkv_bias=False,\n",
    "                 attn_drop=0.,\n",
    "                 proj_drop=0.,\n",
    "                 method='dw_bn',\n",
    "                 kernel_size=3,\n",
    "                 stride_kv=1,\n",
    "                 stride_q=1,\n",
    "                 padding_kv=1,\n",
    "                 padding_q=1,\n",
    "                 with_cls_token=True,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.stride_kv = stride_kv\n",
    "        self.stride_q = stride_q\n",
    "        self.dim = dim_out\n",
    "        self.num_heads = num_heads\n",
    "        # head_dim = self.qkv_dim // num_heads\n",
    "        self.scale = dim_out ** -0.5\n",
    "        self.with_cls_token = with_cls_token\n",
    "\n",
    "        self.conv_proj_q = self._build_projection(\n",
    "            dim_in, dim_out, kernel_size, padding_q,\n",
    "            stride_q, 'linear' if method == 'avg' else method\n",
    "        )\n",
    "        self.conv_proj_k = self._build_projection(\n",
    "            dim_in, dim_out, kernel_size, padding_kv,\n",
    "            stride_kv, method\n",
    "        )\n",
    "        self.conv_proj_v = self._build_projection(\n",
    "            dim_in, dim_out, kernel_size, padding_kv,\n",
    "            stride_kv, method\n",
    "        )\n",
    "\n",
    "        self.proj_q = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "        self.proj_k = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "        self.proj_v = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim_out, dim_out)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def _build_projection(self,\n",
    "                          dim_in,\n",
    "                          dim_out,\n",
    "                          kernel_size,\n",
    "                          padding,\n",
    "                          stride,\n",
    "                          method):\n",
    "        if method == 'dw_bn':\n",
    "            proj = nn.Sequential(OrderedDict([\n",
    "                ('conv', nn.Conv2d(\n",
    "                    dim_in,\n",
    "                    dim_in,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                    groups=dim_in\n",
    "                )),\n",
    "                ('bn', nn.BatchNorm2d(dim_in)),\n",
    "                ('rearrage', Rearrange('b c h w -> b (h w) c')),\n",
    "            ]))\n",
    "        elif method == 'avg':\n",
    "            proj = nn.Sequential(OrderedDict([\n",
    "                ('avg', nn.AvgPool2d(\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    stride=stride,\n",
    "                    ceil_mode=True\n",
    "                )),\n",
    "                ('rearrage', Rearrange('b c h w -> b (h w) c')),\n",
    "            ]))\n",
    "        elif method == 'linear':\n",
    "            proj = None\n",
    "        else:\n",
    "            raise ValueError('Unknown method ({})'.format(method))\n",
    "\n",
    "        return proj\n",
    "\n",
    "    def forward_conv(self, x, h, w):\n",
    "        if self.with_cls_token:\n",
    "            cls_token, x = torch.split(x, [1, h*w], 1)\n",
    "\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "        if self.conv_proj_q is not None:\n",
    "            q = self.conv_proj_q(x)\n",
    "        else:\n",
    "            q = rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "        if self.conv_proj_k is not None:\n",
    "            k = self.conv_proj_k(x)\n",
    "        else:\n",
    "            k = rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "        if self.conv_proj_v is not None:\n",
    "            v = self.conv_proj_v(x)\n",
    "        else:\n",
    "            v = rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "        if self.with_cls_token:\n",
    "            q = torch.cat((cls_token, q), dim=1)\n",
    "            k = torch.cat((cls_token, k), dim=1)\n",
    "            v = torch.cat((cls_token, v), dim=1)\n",
    "\n",
    "        return q, k, v\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        if (\n",
    "            self.conv_proj_q is not None\n",
    "            or self.conv_proj_k is not None\n",
    "            or self.conv_proj_v is not None\n",
    "        ):\n",
    "            q, k, v = self.forward_conv(x, h, w)\n",
    "\n",
    "        q = rearrange(self.proj_q(q), 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        k = rearrange(self.proj_k(k), 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        v = rearrange(self.proj_v(v), 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "\n",
    "        attn_score = torch.einsum('bhlk,bhtk->bhlt', [q, k]) * self.scale\n",
    "        attn = F.softmax(attn_score, dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = torch.einsum('bhlt,bhtv->bhlv', [attn, v])\n",
    "        x = rearrange(x, 'b h t d -> b t (h d)')\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_macs(module, input, output):\n",
    "        # T: num_token\n",
    "        # S: num_token\n",
    "        input = input[0]\n",
    "        flops = 0\n",
    "\n",
    "        _, T, C = input.shape\n",
    "        H = W = int(np.sqrt(T-1)) if module.with_cls_token else int(np.sqrt(T))\n",
    "\n",
    "        H_Q = H / module.stride_q\n",
    "        W_Q = H / module.stride_q\n",
    "        T_Q = H_Q * W_Q + 1 if module.with_cls_token else H_Q * W_Q\n",
    "\n",
    "        H_KV = H / module.stride_kv\n",
    "        W_KV = W / module.stride_kv\n",
    "        T_KV = H_KV * W_KV + 1 if module.with_cls_token else H_KV * W_KV\n",
    "\n",
    "        # C = module.dim\n",
    "        # S = T\n",
    "        # Scaled-dot-product macs\n",
    "        # [B x T x C] x [B x C x T] --> [B x T x S]\n",
    "        # multiplication-addition is counted as 1 because operations can be fused\n",
    "        flops += T_Q * T_KV * module.dim\n",
    "        # [B x T x S] x [B x S x C] --> [B x T x C]\n",
    "        flops += T_Q * module.dim * T_KV\n",
    "\n",
    "        if (\n",
    "            hasattr(module, 'conv_proj_q')\n",
    "            and hasattr(module.conv_proj_q, 'conv')\n",
    "        ):\n",
    "            params = sum(\n",
    "                [\n",
    "                    p.numel()\n",
    "                    for p in module.conv_proj_q.conv.parameters()\n",
    "                ]\n",
    "            )\n",
    "            flops += params * H_Q * W_Q\n",
    "\n",
    "        if (\n",
    "            hasattr(module, 'conv_proj_k')\n",
    "            and hasattr(module.conv_proj_k, 'conv')\n",
    "        ):\n",
    "            params = sum(\n",
    "                [\n",
    "                    p.numel()\n",
    "                    for p in module.conv_proj_k.conv.parameters()\n",
    "                ]\n",
    "            )\n",
    "            flops += params * H_KV * W_KV\n",
    "\n",
    "        if (\n",
    "            hasattr(module, 'conv_proj_v')\n",
    "            and hasattr(module.conv_proj_v, 'conv')\n",
    "        ):\n",
    "            params = sum(\n",
    "                [\n",
    "                    p.numel()\n",
    "                    for p in module.conv_proj_v.conv.parameters()\n",
    "                ]\n",
    "            )\n",
    "            flops += params * H_KV * W_KV\n",
    "\n",
    "        params = sum([p.numel() for p in module.proj_q.parameters()])\n",
    "        flops += params * T_Q\n",
    "        params = sum([p.numel() for p in module.proj_k.parameters()])\n",
    "        flops += params * T_KV\n",
    "        params = sum([p.numel() for p in module.proj_v.parameters()])\n",
    "        flops += params * T_KV\n",
    "        params = sum([p.numel() for p in module.proj.parameters()])\n",
    "        flops += params * T\n",
    "\n",
    "        module.__flops__ += flops\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim_in,\n",
    "                 dim_out,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.with_cls_token = kwargs['with_cls_token']\n",
    "\n",
    "        self.norm1 = norm_layer(dim_in)\n",
    "        self.attn = Attention(\n",
    "            dim_in, dim_out, num_heads, qkv_bias, attn_drop, drop,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) \\\n",
    "            if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim_out)\n",
    "\n",
    "        dim_mlp_hidden = int(dim_out * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim_out,\n",
    "            hidden_features=dim_mlp_hidden,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        res = x\n",
    "\n",
    "        x = self.norm1(x)\n",
    "        attn = self.attn(x, h, w)\n",
    "        x = res + self.drop_path(attn)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvEmbed(nn.Module):\n",
    "    \"\"\" Image to Conv Embedding\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 patch_size=7,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=64,\n",
    "                 stride=4,\n",
    "                 padding=2,\n",
    "                 norm_layer=None):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=stride,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 patch_size=16,\n",
    "                 patch_stride=16,\n",
    "                 patch_padding=0,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=768,\n",
    "                 depth=12,\n",
    "                 num_heads=12,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 init='trunc_norm',\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "\n",
    "        self.rearrage = None\n",
    "\n",
    "        self.patch_embed = ConvEmbed(\n",
    "            # img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            stride=patch_stride,\n",
    "            padding=patch_padding,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer\n",
    "        )\n",
    "\n",
    "        with_cls_token = kwargs['with_cls_token']\n",
    "        if with_cls_token:\n",
    "            self.cls_token = nn.Parameter(\n",
    "                torch.zeros(1, 1, embed_dim)\n",
    "            )\n",
    "        else:\n",
    "            self.cls_token = None\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "\n",
    "        blocks = []\n",
    "        for j in range(depth):\n",
    "            blocks.append(\n",
    "                Block(\n",
    "                    dim_in=embed_dim,\n",
    "                    dim_out=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    drop=drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                    drop_path=dpr[j],\n",
    "                    act_layer=act_layer,\n",
    "                    norm_layer=norm_layer,\n",
    "                    **kwargs\n",
    "                )\n",
    "            )\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "        if self.cls_token is not None:\n",
    "            trunc_normal_(self.cls_token, std=.02)\n",
    "\n",
    "        if init == 'xavier':\n",
    "            self.apply(self._init_weights_xavier)\n",
    "        else:\n",
    "            self.apply(self._init_weights_trunc_normal)\n",
    "\n",
    "    def _init_weights_trunc_normal(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            logging.info('=> init weight of Linear from trunc norm')\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                logging.info('=> init bias of Linear to zeros')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def _init_weights_xavier(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            logging.info('=> init weight of Linear from xavier uniform')\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                logging.info('=> init bias of Linear to zeros')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "        cls_tokens = None\n",
    "        if self.cls_token is not None:\n",
    "            # stole cls_tokens impl from Phil Wang, thanks\n",
    "            cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x, H, W)\n",
    "\n",
    "        if self.cls_token is not None:\n",
    "            cls_tokens, x = torch.split(x, [1, H*W], 1)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)\n",
    "\n",
    "        return x, cls_tokens\n",
    "\n",
    "\n",
    "class ConvolutionalVisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_chans=3,\n",
    "                 num_classes=1000,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 init='trunc_norm',\n",
    "                 spec=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.num_stages = spec['NUM_STAGES']\n",
    "        for i in range(self.num_stages):\n",
    "            kwargs = {\n",
    "                'patch_size': spec['PATCH_SIZE'][i],\n",
    "                'patch_stride': spec['PATCH_STRIDE'][i],\n",
    "                'patch_padding': spec['PATCH_PADDING'][i],\n",
    "                'embed_dim': spec['DIM_EMBED'][i],\n",
    "                'depth': spec['DEPTH'][i],\n",
    "                'num_heads': spec['NUM_HEADS'][i],\n",
    "                'mlp_ratio': spec['MLP_RATIO'][i],\n",
    "                'qkv_bias': spec['QKV_BIAS'][i],\n",
    "                'drop_rate': spec['DROP_RATE'][i],\n",
    "                'attn_drop_rate': spec['ATTN_DROP_RATE'][i],\n",
    "                'drop_path_rate': spec['DROP_PATH_RATE'][i],\n",
    "                'with_cls_token': spec['CLS_TOKEN'][i],\n",
    "                'method': spec['QKV_PROJ_METHOD'][i],\n",
    "                'kernel_size': spec['KERNEL_QKV'][i],\n",
    "                'padding_q': spec['PADDING_Q'][i],\n",
    "                'padding_kv': spec['PADDING_KV'][i],\n",
    "                'stride_kv': spec['STRIDE_KV'][i],\n",
    "                'stride_q': spec['STRIDE_Q'][i],\n",
    "            }\n",
    "\n",
    "            stage = VisionTransformer(\n",
    "                in_chans=in_chans,\n",
    "                init=init,\n",
    "                act_layer=act_layer,\n",
    "                norm_layer=norm_layer,\n",
    "                **kwargs\n",
    "            )\n",
    "            setattr(self, f'stage{i}', stage)\n",
    "\n",
    "            in_chans = spec['DIM_EMBED'][i]\n",
    "\n",
    "        dim_embed = spec['DIM_EMBED'][-1]\n",
    "        self.norm = norm_layer(dim_embed)\n",
    "        self.cls_token = spec['CLS_TOKEN'][-1]\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(dim_embed, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        trunc_normal_(self.head.weight, std=0.02)\n",
    "\n",
    "    def init_weights(self, pretrained='', pretrained_layers=[], verbose=True):\n",
    "        if os.path.isfile(pretrained):\n",
    "            pretrained_dict = torch.load(pretrained, map_location='cpu')\n",
    "            logging.info(f'=> loading pretrained model {pretrained}')\n",
    "            model_dict = self.state_dict()\n",
    "            pretrained_dict = {\n",
    "                k: v for k, v in pretrained_dict.items()\n",
    "                if k in model_dict.keys()\n",
    "            }\n",
    "            need_init_state_dict = {}\n",
    "            for k, v in pretrained_dict.items():\n",
    "                need_init = (\n",
    "                        k.split('.')[0] in pretrained_layers\n",
    "                        or pretrained_layers[0] is '*'\n",
    "                )\n",
    "                if need_init:\n",
    "                    if verbose:\n",
    "                        logging.info(f'=> init {k} from {pretrained}')\n",
    "                    if 'pos_embed' in k and v.size() != model_dict[k].size():\n",
    "                        size_pretrained = v.size()\n",
    "                        size_new = model_dict[k].size()\n",
    "                        logging.info(\n",
    "                            '=> load_pretrained: resized variant: {} to {}'\n",
    "                            .format(size_pretrained, size_new)\n",
    "                        )\n",
    "\n",
    "                        ntok_new = size_new[1]\n",
    "                        ntok_new -= 1\n",
    "\n",
    "                        posemb_tok, posemb_grid = v[:, :1], v[0, 1:]\n",
    "\n",
    "                        gs_old = int(np.sqrt(len(posemb_grid)))\n",
    "                        gs_new = int(np.sqrt(ntok_new))\n",
    "\n",
    "                        logging.info(\n",
    "                            '=> load_pretrained: grid-size from {} to {}'\n",
    "                            .format(gs_old, gs_new)\n",
    "                        )\n",
    "\n",
    "                        posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
    "                        zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
    "                        posemb_grid = scipy.ndimage.zoom(\n",
    "                            posemb_grid, zoom, order=1\n",
    "                        )\n",
    "                        posemb_grid = posemb_grid.reshape(1, gs_new ** 2, -1)\n",
    "                        v = torch.tensor(\n",
    "                            np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
    "                        )\n",
    "\n",
    "                    need_init_state_dict[k] = v\n",
    "            self.load_state_dict(need_init_state_dict, strict=False)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        layers = set()\n",
    "        for i in range(self.num_stages):\n",
    "            layers.add(f'stage{i}.pos_embed')\n",
    "            layers.add(f'stage{i}.cls_token')\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for i in range(self.num_stages):\n",
    "            x, cls_tokens = getattr(self, f'stage{i}')(x)\n",
    "\n",
    "        if self.cls_token:\n",
    "            x = self.norm(cls_tokens)\n",
    "            x = torch.squeeze(x)\n",
    "        else:\n",
    "            x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "            x = self.norm(x)\n",
    "            x = torch.mean(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ccc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cvt_13(config, num_classes=100):\n",
    "    spec = config['MODEL']['SPEC']\n",
    "    model = ConvolutionalVisionTransformer(\n",
    "        in_chans=3,\n",
    "        num_classes=num_classes,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=partial(LayerNorm, eps=1e-5),\n",
    "        init=config['MODEL']['SPEC']['INIT'],\n",
    "        spec=spec\n",
    "    )\n",
    "    return model\n",
    "model = create_cvt_13(config, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b18871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count model parameters\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Model Parameter Count\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return total_params, trainable_params\n",
    "\n",
    "# Count parameters\n",
    "total_params, trainable_params = count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d41901",
   "metadata": {},
   "source": [
    "## Setup Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e23eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "CVT_BLOCKS = [1, 2, 10]\n",
    "\n",
    "depth_prefixes = []\n",
    "for s, nb in enumerate(CVT_BLOCKS):\n",
    "    depth_prefixes.append(f\"stages.{s}.patch_embed\")\n",
    "    for b in range(nb):\n",
    "        depth_prefixes.append(f\"stages.{s}.blocks.{b}\")\n",
    "\n",
    "param_names = [n for n, _ in model.named_parameters()]\n",
    "for extra in [\"cls_token\", \"pos_embed\", \"norm\", \"head\", \"pre_logits\", \"fc\"]:\n",
    "    if any(n.startswith(extra) for n in param_names):\n",
    "        depth_prefixes.append(extra)\n",
    "for s in range(len(CVT_BLOCKS)):  \n",
    "    maybe = f\"stages.{s}.downsample\"\n",
    "    if any(n.startswith(maybe) for n in param_names):\n",
    "        depth_prefixes.append(maybe)\n",
    "\n",
    "rules = set(config['TRAIN']['WITHOUT_WD_LIST'])  \n",
    "no_decay_names = set()\n",
    "\n",
    "def is_depthwise(m: nn.Module):\n",
    "    return isinstance(m, nn.Conv2d) and m.groups == m.in_channels == m.out_channels\n",
    "\n",
    "if {'bn','gn','ln'} & rules:\n",
    "    for mod_name, mod in model.named_modules():\n",
    "        if isinstance(mod, (nn.BatchNorm2d, nn.GroupNorm, nn.LayerNorm)):\n",
    "            for p_name, _ in mod.named_parameters(recurse=False):\n",
    "                full = f\"{mod_name}.{p_name}\" if mod_name else p_name\n",
    "                no_decay_names.add(full)\n",
    "\n",
    "if 'bias' in rules:\n",
    "    for n, p in model.named_parameters():\n",
    "        if n.endswith(\".bias\"):\n",
    "            no_decay_names.add(n)\n",
    "\n",
    "if 'dw' in rules:\n",
    "    for mod_name, mod in model.named_modules():\n",
    "        if is_depthwise(mod) and getattr(mod, \"weight\", None) is not None:\n",
    "            no_decay_names.add(f\"{mod_name}.weight\")\n",
    "\n",
    "base_lr = float(config['TRAIN']['LR'])\n",
    "min_lr  = float(config['TRAIN']['LR_CURRICULUM']['MIN_LR'])\n",
    "gamma   = float(config['TRAIN']['LR_CURRICULUM'].get('GAMMA', 0.002))\n",
    "\n",
    "init_lrs = []\n",
    "cur = base_lr\n",
    "for _ in depth_prefixes:\n",
    "    init_lrs.append(max(min_lr, cur))\n",
    "    cur *= gamma\n",
    "\n",
    "buckets_decay    = OrderedDict((p, []) for p in depth_prefixes)\n",
    "buckets_nodecay  = OrderedDict((p, []) for p in depth_prefixes)\n",
    "misc_decay, misc_nodecay = [], []\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "    if not p.requires_grad:\n",
    "        continue\n",
    "    hit = False\n",
    "    for pref in depth_prefixes:\n",
    "        if n.startswith(pref):\n",
    "            (buckets_nodecay[pref] if n in no_decay_names else buckets_decay[pref]).append(p)\n",
    "            hit = True\n",
    "            break\n",
    "    if not hit:\n",
    "        (misc_nodecay if n in no_decay_names else misc_decay).append(p)\n",
    "\n",
    "wd = float(config['TRAIN']['WD'])\n",
    "param_groups = []\n",
    "for pref, init_lr in zip(depth_prefixes, init_lrs):\n",
    "    if buckets_decay[pref]:\n",
    "        param_groups.append({'params': buckets_decay[pref],   'lr': init_lr, 'weight_decay': wd,  '_init_lr': init_lr})\n",
    "    if buckets_nodecay[pref]:\n",
    "        param_groups.append({'params': buckets_nodecay[pref], 'lr': init_lr, 'weight_decay': 0.0, '_init_lr': init_lr})\n",
    "\n",
    "if misc_decay:\n",
    "    param_groups.append({'params': misc_decay,   'lr': base_lr, 'weight_decay': wd,  '_init_lr': base_lr})\n",
    "if misc_nodecay:\n",
    "    param_groups.append({'params': misc_nodecay, 'lr': base_lr, 'weight_decay': 0., '_init_lr': base_lr})\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups, betas=(0.9, 0.999), eps=1e-8)\n",
    "\n",
    "print(f\"[LeRaC] groups={len(param_groups)}  distinct_init_LRs={sorted({g['lr'] for g in param_groups}, reverse=True)[:6]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1084832f",
   "metadata": {},
   "source": [
    "## Setup Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2018a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler, SequentialLR, CosineAnnealingLR\n",
    "\n",
    "class LeRaCScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, base_lr, warmup_epochs, c=10.0, last_epoch = -1):\n",
    "        self.base_lr = float(base_lr)\n",
    "        self.warmup_epochs = max(1, int(warmup_epochs))\n",
    "        self.c = float(c)\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        # After warmup: all groups share base_lr\n",
    "        if self.last_epoch >= self.warmup_epochs:\n",
    "            return [self.base_lr for _ in self.optimizer.param_groups]\n",
    "\n",
    "        # Warmup epoch counter t \\in {1..warmup_epochs}\n",
    "        t = self.last_epoch + 1\n",
    "\n",
    "        lrs = []\n",
    "        for g in self.optimizer.param_groups:\n",
    "            init = float(g.get('_init_lr', g['lr']))\n",
    "            # growth per Eq.(9), clamped to base\n",
    "            lr_t = init * (self.c ** t)\n",
    "            lrs.append(self.base_lr if lr_t >= self.base_lr else lr_t)\n",
    "        return lrs\n",
    "\n",
    "def get_scheduler(config, optimizer):\n",
    "    num_epochs = int(config['TRAIN']['END_EPOCH'])\n",
    "    warmup_epochs = int(config['TRAIN']['LR_CURRICULUM'].get('WARMUP_EPOCHS', 5))\n",
    "    base_lr = float(config['TRAIN']['LR'])\n",
    "    min_lr = float(config['TRAIN']['LR_SCHEDULER']['ARGS'].get('min_lr', 1e-5))\n",
    "\n",
    "    for g in optimizer.param_groups:\n",
    "        if '_init_lr' not in g:\n",
    "            g['_init_lr'] = g['lr']  \n",
    "\n",
    "    lerac_scheduler = LeRaCScheduler(\n",
    "        optimizer, base_lr=base_lr, warmup_epochs=warmup_epochs, last_epoch=-1\n",
    "    )\n",
    "\n",
    "    cosine_epochs = max(1, num_epochs - warmup_epochs)\n",
    "    cosine_scheduler = CosineAnnealingLR(\n",
    "        optimizer, T_max=cosine_epochs, eta_min=min_lr\n",
    "    )\n",
    "\n",
    "    scheduler = SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[lerac_scheduler, cosine_scheduler],\n",
    "        milestones=[warmup_epochs]  \n",
    "    )\n",
    "\n",
    "    print(f\"Scheduler: LeRaC warm-up ({warmup_epochs} ep) -> Cosine ({cosine_epochs} ep)\")\n",
    "    print(f\"  Base LR: {base_lr} | Min LR: {min_lr} | Total epochs: {num_epochs}\")\n",
    "    return scheduler\n",
    "scheduler = get_scheduler(config, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b86f6",
   "metadata": {},
   "source": [
    "## Set Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a10b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data.mixup import Mixup\n",
    "class SoftTargetCrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoftTargetCrossEntropy, self).__init__()\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        loss = torch.sum(-target * F.log_softmax(x, dim=-1), dim=-1)\n",
    "        return loss.mean()\n",
    "\n",
    "def build_criterion(is_train=True):\n",
    "    if is_train:\n",
    "        return SoftTargetCrossEntropy()\n",
    "    else:\n",
    "        return nn.CrossEntropyLoss()\n",
    "aug = config['AUG']\n",
    "mixup_fn = Mixup(\n",
    "        mixup_alpha=aug['MIXUP'], cutmix_alpha=aug['MIXCUT'],\n",
    "        cutmix_minmax=None,\n",
    "        prob=aug['MIXUP_PROB'],\n",
    "        label_smoothing=0.0,\n",
    "        num_classes=10\n",
    "    )\n",
    "criterion = build_criterion()\n",
    "criterion.cuda()\n",
    "criterion_eval = build_criterion(is_train=False)\n",
    "criterion_eval.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1443a04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1193ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    maxk = min(max(topk), output.size()[1])\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "    return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c3963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, model, train_loader, criterion, optimizer, scheduler, \n",
    "                    config, scaler, mixup_fn):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    Following: https://github.com/microsoft/CvT/blob/main/lib/core/function.py\n",
    "    \"\"\"\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    acc_m = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "    accumulation_steps = config['TRAIN'].get('GRADIENT_ACCUMULATION_STEPS', 1)\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n",
    "\n",
    "    for idx, (images, targets) in enumerate(train_loader):\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        targets = targets.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        images, targets = mixup_fn(images, targets)\n",
    "\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            outputs = model(images)\n",
    "   \n",
    "            loss = criterion(outputs, targets)\n",
    "       \n",
    "            loss = loss / accumulation_steps\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        if (idx + 1) % accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            if idx % 100 == 0 and epoch >= 6:\n",
    "                total_norm = 0\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        total_norm += p.grad.norm().item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "                print(f\"Epoch {epoch}, Batch {idx}, Grad norm: {total_norm:.4f}\")            \n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad.data.clamp_(-0.5, 0.5)                    \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()        \n",
    "\n",
    "        targets_for_acc = torch.argmax(targets, dim=1)\n",
    "\n",
    "        acc = accuracy(outputs, targets_for_acc)\n",
    "\n",
    "        losses.update(loss.item()*accumulation_steps, images.size(0))\n",
    "        acc_m.update(acc[0].item(), images.size(0))\n",
    "        batch_bar.set_postfix(\n",
    "            acc=\"{:.02f}% ({:.02f}%)\".format(acc[0].item(), acc_m.avg),\n",
    "            loss=\"{:.04f} ({:.04f})\".format(loss.item()*accumulation_steps, losses.avg),\n",
    "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "        batch_bar.update() # Update tqdm bar        \n",
    "    \n",
    "    if (idx + 1) % accumulation_steps != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-0.5, 0.5)                    \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)                    \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    batch_bar.close()\n",
    "\n",
    "    scheduler.step()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return losses.avg, acc_m.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111e7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, val_loader, criterion, config):\n",
    "    losses = AverageMeter()\n",
    "    acc_m = AverageMeter()\n",
    "    \n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val.', ncols=5)\n",
    "\n",
    "    for idx, (images, targets) in enumerate(val_loader):\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        targets = targets.to(DEVICE, non_blocking=True)\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        acc = accuracy(outputs, targets)\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        acc_m.update(acc[0].item(), images.size(0))\n",
    "        batch_bar.set_postfix(\n",
    "            acc=\"{:.02f}% ({:.02f}%)\".format(acc[0].item(), acc_m.avg),\n",
    "            loss=\"{:.04f} ({:.04f})\".format(loss.item(), losses.avg))\n",
    "\n",
    "        batch_bar.update()\n",
    "\n",
    "    batch_bar.close()\n",
    "    print(f' * Acc {acc_m.avg:.3f}')\n",
    "    \n",
    "    return losses.avg, acc_m.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae0f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, path):\n",
    "    torch.save(\n",
    "        {'model_state_dict'         : model.state_dict(),\n",
    "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
    "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
    "         'epoch'                    : epoch},\n",
    "         path)\n",
    "\n",
    "\n",
    "def load_model(model, optimizer=None, scheduler=None, path='./checkpoint.pth'):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    else:\n",
    "        optimizer = None\n",
    "    if scheduler is not None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    else:\n",
    "        scheduler = None\n",
    "    epoch = checkpoint['epoch']\n",
    "    return model, optimizer, scheduler, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da521cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config['OUTPUT_DIR'], exist_ok=True)\n",
    "model = model.to(DEVICE)\n",
    "scaler = scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea26dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=os.environ.get('WANDB_API_KEY')) # API Key is in your wandb account, under settings (wandb.ai/settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1871908",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    name = \"idl-project-cvt-13-baseline-no-curriculum-learning-cifar-10-lerac-13\", ## Wandb creates random run names if you skip this field\n",
    "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    #id = \"lqcgq8ng\", #Insert specific run id here if you want to resume a previous run\n",
    "    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"idl-project\", ### Project should be created in your wandb account\n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect() # These commands help you when you face CUDA OOM error\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79070f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "best_loss = -1\n",
    "start_epoch = config['TRAIN']['BEGIN_EPOCH']\n",
    "end_epoch = config['TRAIN']['END_EPOCH']\n",
    "\n",
    "print(\"Starting Training\")\n",
    "print(f\"Epochs: {start_epoch} -> {end_epoch}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "for epoch in range(start_epoch, end_epoch):\n",
    "    # epoch\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, end_epoch))\n",
    "    train_loss, train_acc = train_one_epoch(epoch, model, train_loader, criterion, optimizer, scheduler, \n",
    "                    config, scaler, mixup_fn)\n",
    "    \n",
    "    val_loss, val_acc = validate(model, val_loader, criterion_eval, config)\n",
    "    is_best = (best_loss) == -1 or val_loss < best_loss\n",
    "    best_loss = min(val_loss, best_loss)\n",
    "#     save_model(model, optimizer, scheduler, epoch,os.path.join(config['OUTPUT_DIR'], f'{epoch}.pth'))\n",
    "    if is_best:\n",
    "        save_model(model, optimizer, scheduler, epoch, str(os.path.join(config['OUTPUT_DIR'], 'best.pth')) )\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Train Acc: {train_acc:.3f}, Val Acc: {val_acc:.3f}\")\n",
    "    metrics = {'train_loss': train_loss, 'val_loss': val_loss, 'train_acc': train_acc, 'val_acc': val_acc, 'epoch': epoch}\n",
    "    if run is not None:\n",
    "        run.log(metrics)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c81f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler, epoch = load_model(model, optimizer, scheduler, path=\"./OUTPUT/CIFAR-10-LeRaC-Attempt-13/best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, test_loader, config):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for images, targets in tqdm(test_loader, desc='Testing'):\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        targets = targets.to(DEVICE, non_blocking=True)\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        \n",
    "        _, preds = outputs.topk(1, 1, True, True)\n",
    "        \n",
    "        all_predictions.extend(preds.cpu().numpy().flatten())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        all_probabilities.extend(probs.cpu().numpy())\n",
    "        \n",
    "        acc1 = accuracy(outputs, targets)\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0].item(), images.size(0))\n",
    "    \n",
    "    return {\n",
    "        'predictions': np.array(all_predictions),\n",
    "        'targets': np.array(all_targets),\n",
    "        'probabilities': np.array(all_probabilities),\n",
    "        'loss': losses.avg,\n",
    "        'top1_acc': top1.avg\n",
    "    }\n",
    "\n",
    "test_results = test(model, val_loader, config)\n",
    "\n",
    "print(\"Final Test Results\")\n",
    "print(f\"Test Loss: {test_results['loss']:.4f}\")\n",
    "print(f\"Test Acc@1: {test_results['top1_acc']:.3f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8cef4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
