{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f944d09",
   "metadata": {},
   "source": [
    "Code for Cvt-13 largely based off of: https://github.com/microsoft/CvT/blob/main/lib/dataset/transformas/build.py\n",
    "and https://arxiv.org/pdf/2103.15808\n",
    "with some modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchsummary torchvision tqdm wandb typing_extensions pytorch_metric_learning timm==0.9.16 einops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3c0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import datasets, transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import gc\n",
    "# from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics as mt\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import glob\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_metric_learning import samplers\n",
    "import csv\n",
    "import logging\n",
    "from timm.data import create_loader, create_transform\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params largely copied from https://github.com/leoxiaobin/CvT/blob/main/experiments/imagenet/cvt/cvt-13-224x224.yaml\n",
    "config = {\n",
    "  \"OUTPUT_DIR\": \"OUTPUT/imagenet-blur-1\",\n",
    "  \"WORKERS\": 8,\n",
    "  \"PRINT_FREQ\": 500,\n",
    "  \"AMP\": {\n",
    "    \"ENABLED\": True\n",
    "  },\n",
    "  \"MODEL\": {\n",
    "    \"NAME\": \"cls_cvt\",\n",
    "    \"SPEC\": {\n",
    "      \"INIT\": \"trunc_norm\",\n",
    "      \"NUM_STAGES\": 3,\n",
    "      \"PATCH_SIZE\": [7, 3, 3],\n",
    "      \"PATCH_STRIDE\": [4, 2, 2],\n",
    "      \"PATCH_PADDING\": [2, 1, 1],\n",
    "      \"DIM_EMBED\": [64, 192, 384],\n",
    "      \"NUM_HEADS\": [1, 3, 6],\n",
    "      \"DEPTH\": [1, 2, 10],\n",
    "      \"MLP_RATIO\": [4.0, 4.0, 4.0],\n",
    "      \"ATTN_DROP_RATE\": [0.0, 0.0, 0.0],\n",
    "      \"DROP_RATE\": [0.0, 0.0, 0.0],\n",
    "      \"DROP_PATH_RATE\": [0.0, 0.0, 0.1],\n",
    "      \"QKV_BIAS\": [True, True, True],\n",
    "      \"CLS_TOKEN\": [False, False, True],\n",
    "      \"POS_EMBED\": [False, False, False],\n",
    "      \"QKV_PROJ_METHOD\": [\"dw_bn\", \"dw_bn\", \"dw_bn\"],\n",
    "      \"KERNEL_QKV\": [3, 3, 3],\n",
    "      \"PADDING_KV\": [1, 1, 1],\n",
    "      \"STRIDE_KV\": [2, 2, 2],\n",
    "      \"PADDING_Q\": [1, 1, 1],\n",
    "      \"STRIDE_Q\": [1, 1, 1]\n",
    "    }\n",
    "  },\n",
    "  \"AUG\": {\n",
    "    \"MIXUP_PROB\": 1.0,\n",
    "    \"MIXUP\": 0.8,\n",
    "    \"MIXCUT\": 1.0,\n",
    "    \"TIMM_AUG\": {\n",
    "      \"USE_LOADER\": True,\n",
    "      \"RE_COUNT\": 1,\n",
    "      \"RE_MODE\": \"pixel\",\n",
    "      \"RE_SPLIT\": False,\n",
    "      \"RE_PROB\": 0.25,\n",
    "      \"AUTO_AUGMENT\": \"rand-m9-mstd0.5-inc1\",\n",
    "      \"HFLIP\": 0.5,\n",
    "      \"VFLIP\": 0.0,\n",
    "      \"COLOR_JITTER\": 0.4,\n",
    "      \"INTERPOLATION\": \"bicubic\"\n",
    "    }\n",
    "  },\n",
    "  \"LOSS\": {\n",
    "    \"LABEL_SMOOTHING\": 0.1\n",
    "  },\n",
    "  \"CUDNN\": {\n",
    "    \"BENCHMARK\": True,\n",
    "    \"DETERMINISTIC\": False,\n",
    "    \"ENABLED\": True\n",
    "  },\n",
    "  \"DATASET\": {\n",
    "    \"DATASET\": \"imagenet\",\n",
    "    \"DATA_FORMAT\": \"jpg\",\n",
    "    \"ROOT\": \"./ImageNet100_224\",\n",
    "    \"TEST_SET\": \"val\",\n",
    "    \"TRAIN_SET\": \"train\"\n",
    "  },\n",
    "  \"TEST\": {\n",
    "    \"BATCH_SIZE_PER_GPU\": 32,\n",
    "    \"IMAGE_SIZE\": [224, 224],\n",
    "    \"MODEL_FILE\": \"\",\n",
    "    \"INTERPOLATION\": \"bicubic\"\n",
    "  },\n",
    "  \"TRAIN\": {\n",
    "    \"BATCH_SIZE_PER_GPU\": 256,\n",
    "    \"GRADIENT_ACCUMULATION_STEPS\": 8,       \n",
    "    \"LR\": .00125,\n",
    "    \"IMAGE_SIZE\": [224, 224],\n",
    "    \"BEGIN_EPOCH\": 0,\n",
    "    \"END_EPOCH\": 100,\n",
    "    \"BLUR\": {\n",
    "        \"KERNEL_SIZE\": 7,\n",
    "        \"SIGMA\": 1,\n",
    "        \"EPOCHS\": 20\n",
    "    },      \n",
    "    \"LR_SCHEDULER\": {\n",
    "      \"METHOD\": \"timm\",\n",
    "      \"ARGS\": {\n",
    "        \"sched\": \"cosine\",\n",
    "        \"warmup_epochs\": 5,\n",
    "        \"warmup_lr\": 0.000001,\n",
    "        \"min_lr\": 0.00001,\n",
    "        \"cooldown_epochs\": 10,\n",
    "        \"decay_rate\": 0.1\n",
    "      }\n",
    "    },\n",
    "    \"OPTIMIZER\": \"adamW\",\n",
    "    \"WD\": 0.05,\n",
    "    \"WITHOUT_WD_LIST\": [\"bn\", \"bias\", \"ln\"],\n",
    "    \"SHUFFLE\": True\n",
    "  },\n",
    "  \"DEBUG\": {\n",
    "    \"DEBUG\": False\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29029b6",
   "metadata": {},
   "source": [
    "### Defining transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facc7444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transforms(config, is_train):\n",
    "    if is_train:\n",
    "        img_size = config['TRAIN']['IMAGE_SIZE'][0]\n",
    "        timm_cfg = config['AUG']['TIMM_AUG']\n",
    "        # hardcoded values are from defaults e.g., https://github.com/microsoft/CvT/blob/main/lib/config/default.py#L68\n",
    "        transforms = create_transform(\n",
    "            input_size = img_size,\n",
    "            is_training = True,\n",
    "            use_prefetcher=False,\n",
    "            no_aug=False,\n",
    "            re_prob=timm_cfg['RE_PROB'],\n",
    "            re_mode=timm_cfg['RE_MODE'],\n",
    "            re_count=timm_cfg['RE_COUNT'],\n",
    "            scale=(0.08, 1.0),\n",
    "            ratio=(3.0/4.0, 4.0/3.0),\n",
    "            hflip=timm_cfg['HFLIP'],\n",
    "            vflip=timm_cfg['VFLIP'],\n",
    "            color_jitter=timm_cfg['COLOR_JITTER'],\n",
    "            auto_augment=timm_cfg['AUTO_AUGMENT'],\n",
    "            interpolation=timm_cfg['INTERPOLATION'],\n",
    "            mean=(0.485, 0.456, 0.406),\n",
    "            std=(0.229, 0.224, 0.225),            \n",
    "            \n",
    "        )\n",
    "    else:\n",
    "        normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        img_size = config['TEST']['IMAGE_SIZE'][0]\n",
    "        transforms = T.Compose([\n",
    "            T.Resize(int(img_size/ 0.875), interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "            T.CenterCrop(img_size),\n",
    "            T.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c46aa4e",
   "metadata": {},
   "source": [
    "### Building Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f1b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_imagenet_dataset(config, is_train):\n",
    "    transforms = build_transforms(config, is_train)\n",
    "    dataset_name = config['DATASET']['TRAIN_SET'] if is_train else config['DATASET']['TEST_SET']\n",
    "    dataset = datasets.ImageFolder(os.path.join(config['DATASET']['ROOT'], dataset_name), transforms)\n",
    "    logging.info(\n",
    "        'load samples: {}, is_train: {}'\n",
    "        .format(len(dataset), is_train)\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "def build_dataset(config, is_train):\n",
    "    '''\n",
    "    In this file this will call the build_imagenet_dataset method.\n",
    "    In the CIFAR file it will call the appropriate method\n",
    "    '''\n",
    "    dataset = None\n",
    "    dataset = _build_imagenet_dataset(config, is_train)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df8988",
   "metadata": {},
   "source": [
    "### Building Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ab5c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(config, is_train):\n",
    "    if is_train:\n",
    "        batch_size_per_gpu = config['TRAIN']['BATCH_SIZE_PER_GPU']\n",
    "        shuffle = True\n",
    "    else:\n",
    "        batch_size_per_gpu = config['TEST']['BATCH_SIZE_PER_GPU']\n",
    "        shuffle = False\n",
    "    dataset = build_dataset(config, is_train)\n",
    "    sampler = None # the cvt code has code to use multi-gpu. here we assume single A100 gpu\n",
    "    # set to true in the config so we are going to use TIMM loader for training\n",
    "    if is_train:\n",
    "        logging.info('use timm loader for training')\n",
    "        timm_cfg = config['AUG']['TIMM_AUG']\n",
    "        data_loader = create_loader(\n",
    "            dataset,\n",
    "            input_size=tuple(config['TRAIN']['IMAGE_SIZE']),\n",
    "            batch_size=config['TRAIN']['BATCH_SIZE_PER_GPU'],\n",
    "            is_training=True,\n",
    "            use_prefetcher=False,\n",
    "            no_aug=False,\n",
    "            re_prob=timm_cfg['RE_PROB'],\n",
    "            re_mode=timm_cfg['RE_MODE'],\n",
    "            re_count=timm_cfg['RE_COUNT'],\n",
    "            re_split=timm_cfg['RE_SPLIT'],\n",
    "            scale=(0.08, 1.0),\n",
    "            ratio=(3.0/4.0, 4.0/3.0),\n",
    "            hflip=timm_cfg['HFLIP'],\n",
    "            vflip=timm_cfg['VFLIP'],\n",
    "            color_jitter=timm_cfg['COLOR_JITTER'],\n",
    "            auto_augment=timm_cfg['AUTO_AUGMENT'],\n",
    "            interpolation=timm_cfg['INTERPOLATION'],\n",
    "            num_aug_splits=0,\n",
    "            mean=(0.485, 0.456, 0.406),\n",
    "            std=(0.229, 0.224, 0.225),\n",
    "            num_workers=config['WORKERS'],\n",
    "            distributed=False,\n",
    "            collate_fn=None,\n",
    "            pin_memory=True,\n",
    "            use_multi_epochs_loader=True\n",
    "        )        \n",
    "    else:\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size_per_gpu,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=config['WORKERS'],\n",
    "            pin_memory=True,\n",
    "            sampler=sampler,\n",
    "            drop_last=True if is_train else False,\n",
    "        )     \n",
    "    return data_loader\n",
    "train_loader = build_dataloader(config, is_train=True)\n",
    "val_loader = build_dataloader(config, is_train=False)\n",
    "\n",
    "print(f\"\\nDataLoader Info:\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_loader.dataset)}\")\n",
    "print(f\"Number of classes: {len(train_loader.dataset.classes)}\")\n",
    "\n",
    "# Test loading a batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"Images: {images.shape}\")\n",
    "print(f\"Labels: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354570ad",
   "metadata": {},
   "source": [
    "## Setup Gaussian DataLoader\n",
    "The following section sets up a Gaussian DataLoader that will be used in early epochs of training. Approach closely follows the method described here, except scaled down to a limited number of epochs: https://dl.acm.org/doi/pdf/10.1145/3606043.3606086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ebe885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gaussian_transforms(config):\n",
    "    normalize = T.Normalize(mean=[0.485,  0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    transforms = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.GaussianBlur(kernel_size=config['TRAIN']['BLUR']['KERNEL_SIZE'], sigma=config['TRAIN']['BLUR']['SIGMA']),\n",
    "        normalize\n",
    "    ])\n",
    "    return transforms\n",
    "\n",
    "def build_gaussian_dataset(config, is_train=True):\n",
    "    transforms = build_gaussian_transforms(config)\n",
    "    dataset_name = config['DATASET']['TRAIN_SET'] # will only use blur dataset for training\n",
    "    dataset = datasets.ImageFolder(os.path.join(config['DATASET']['ROOT'], dataset_name), transforms)\n",
    "    logging.info(f'load samples: {len(dataset)}, is_train: {is_train}')\n",
    "    return dataset    \n",
    "\n",
    "def build_gaussian_dataloader(config):\n",
    "    dataset = build_gaussian_dataset(config)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config['TRAIN']['BATCH_SIZE_PER_GPU'],\n",
    "        shuffle=True,\n",
    "        num_workers=config['WORKERS'],\n",
    "        pin_memory=True,\n",
    "        sampler=None,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return data_loader\n",
    "    \n",
    "gaussian_loader = build_gaussian_dataloader(config) # will only be used for train_data \n",
    "print(f\"\\nDataLoader Info:\")\n",
    "print(f\"Gaussian Loader batches: {len(gaussian_loader)}\")\n",
    "print(f\"Gaussian dataset size: {len(gaussian_loader.dataset)}\")\n",
    "print(f\"Gaussian Dataset of classes: {len(gaussian_loader.dataset.classes)}\")\n",
    "\n",
    "# Test loading a batch\n",
    "images, labels = next(iter(gaussian_loader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"Images: {images.shape}\")\n",
    "print(f\"Labels: {labels.shape}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d3a0c7",
   "metadata": {},
   "source": [
    "## Defining Cvt-13 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd8966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVT code from: https://github.com/microsoft/CvT/blob/main/lib/models/cls_cvt.py\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "import collections.abc as container_abcs\n",
    "from collections import OrderedDict\n",
    "from einops.layers.torch import Rearrange\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "\n",
    "# From PyTorch internals\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, container_abcs.Iterable):\n",
    "            return x\n",
    "        return tuple(repeat(x, n))\n",
    "\n",
    "    return parse\n",
    "\n",
    "\n",
    "to_1tuple = _ntuple(1)\n",
    "to_2tuple = _ntuple(2)\n",
    "to_3tuple = _ntuple(3)\n",
    "to_4tuple = _ntuple(4)\n",
    "to_ntuple = _ntuple\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features=None,\n",
    "                 out_features=None,\n",
    "                 act_layer=QuickGELU,\n",
    "                 drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim_in,\n",
    "                 dim_out,\n",
    "                 num_heads,\n",
    "                 qkv_bias=False,\n",
    "                 attn_drop=0.,\n",
    "                 proj_drop=0.,\n",
    "                 method='dw_bn',\n",
    "                 kernel_size=3,\n",
    "                 stride_kv=1,\n",
    "                 stride_q=1,\n",
    "                 padding_kv=1,\n",
    "                 padding_q=1,\n",
    "                 with_cls_token=True,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.stride_kv = stride_kv\n",
    "        self.stride_q = stride_q\n",
    "        self.dim = dim_out\n",
    "        self.num_heads = num_heads\n",
    "        # head_dim = self.qkv_dim // num_heads\n",
    "        self.scale = dim_out ** -0.5\n",
    "        self.with_cls_token = with_cls_token\n",
    "\n",
    "        self.conv_proj_q = self._build_projection(\n",
    "            dim_in, dim_out, kernel_size, padding_q,\n",
    "            stride_q, 'linear' if method == 'avg' else method\n",
    "        )\n",
    "        self.conv_proj_k = self._build_projection(\n",
    "            dim_in, dim_out, kernel_size, padding_kv,\n",
    "            stride_kv, method\n",
    "        )\n",
    "        self.conv_proj_v = self._build_projection(\n",
    "            dim_in, dim_out, kernel_size, padding_kv,\n",
    "            stride_kv, method\n",
    "        )\n",
    "\n",
    "        self.proj_q = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "        self.proj_k = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "        self.proj_v = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim_out, dim_out)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def _build_projection(self,\n",
    "                          dim_in,\n",
    "                          dim_out,\n",
    "                          kernel_size,\n",
    "                          padding,\n",
    "                          stride,\n",
    "                          method):\n",
    "        if method == 'dw_bn':\n",
    "            proj = nn.Sequential(OrderedDict([\n",
    "                ('conv', nn.Conv2d(\n",
    "                    dim_in,\n",
    "                    dim_in,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                    groups=dim_in\n",
    "                )),\n",
    "                ('bn', nn.BatchNorm2d(dim_in)),\n",
    "                ('rearrage', Rearrange('b c h w -> b (h w) c')),\n",
    "            ]))\n",
    "        elif method == 'avg':\n",
    "            proj = nn.Sequential(OrderedDict([\n",
    "                ('avg', nn.AvgPool2d(\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    stride=stride,\n",
    "                    ceil_mode=True\n",
    "                )),\n",
    "                ('rearrage', Rearrange('b c h w -> b (h w) c')),\n",
    "            ]))\n",
    "        elif method == 'linear':\n",
    "            proj = None\n",
    "        else:\n",
    "            raise ValueError('Unknown method ({})'.format(method))\n",
    "\n",
    "        return proj\n",
    "\n",
    "    def forward_conv(self, x, h, w):\n",
    "        if self.with_cls_token:\n",
    "            cls_token, x = torch.split(x, [1, h*w], 1)\n",
    "\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "        if self.conv_proj_q is not None:\n",
    "            q = self.conv_proj_q(x)\n",
    "        else:\n",
    "            q = rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "        if self.conv_proj_k is not None:\n",
    "            k = self.conv_proj_k(x)\n",
    "        else:\n",
    "            k = rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "        if self.conv_proj_v is not None:\n",
    "            v = self.conv_proj_v(x)\n",
    "        else:\n",
    "            v = rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "        if self.with_cls_token:\n",
    "            q = torch.cat((cls_token, q), dim=1)\n",
    "            k = torch.cat((cls_token, k), dim=1)\n",
    "            v = torch.cat((cls_token, v), dim=1)\n",
    "\n",
    "        return q, k, v\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        if (\n",
    "            self.conv_proj_q is not None\n",
    "            or self.conv_proj_k is not None\n",
    "            or self.conv_proj_v is not None\n",
    "        ):\n",
    "            q, k, v = self.forward_conv(x, h, w)\n",
    "\n",
    "        q = rearrange(self.proj_q(q), 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        k = rearrange(self.proj_k(k), 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        v = rearrange(self.proj_v(v), 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "\n",
    "        attn_score = torch.einsum('bhlk,bhtk->bhlt', [q, k]) * self.scale\n",
    "        attn = F.softmax(attn_score, dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = torch.einsum('bhlt,bhtv->bhlv', [attn, v])\n",
    "        x = rearrange(x, 'b h t d -> b t (h d)')\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_macs(module, input, output):\n",
    "        # T: num_token\n",
    "        # S: num_token\n",
    "        input = input[0]\n",
    "        flops = 0\n",
    "\n",
    "        _, T, C = input.shape\n",
    "        H = W = int(np.sqrt(T-1)) if module.with_cls_token else int(np.sqrt(T))\n",
    "\n",
    "        H_Q = H / module.stride_q\n",
    "        W_Q = H / module.stride_q\n",
    "        T_Q = H_Q * W_Q + 1 if module.with_cls_token else H_Q * W_Q\n",
    "\n",
    "        H_KV = H / module.stride_kv\n",
    "        W_KV = W / module.stride_kv\n",
    "        T_KV = H_KV * W_KV + 1 if module.with_cls_token else H_KV * W_KV\n",
    "\n",
    "        # C = module.dim\n",
    "        # S = T\n",
    "        # Scaled-dot-product macs\n",
    "        # [B x T x C] x [B x C x T] --> [B x T x S]\n",
    "        # multiplication-addition is counted as 1 because operations can be fused\n",
    "        flops += T_Q * T_KV * module.dim\n",
    "        # [B x T x S] x [B x S x C] --> [B x T x C]\n",
    "        flops += T_Q * module.dim * T_KV\n",
    "\n",
    "        if (\n",
    "            hasattr(module, 'conv_proj_q')\n",
    "            and hasattr(module.conv_proj_q, 'conv')\n",
    "        ):\n",
    "            params = sum(\n",
    "                [\n",
    "                    p.numel()\n",
    "                    for p in module.conv_proj_q.conv.parameters()\n",
    "                ]\n",
    "            )\n",
    "            flops += params * H_Q * W_Q\n",
    "\n",
    "        if (\n",
    "            hasattr(module, 'conv_proj_k')\n",
    "            and hasattr(module.conv_proj_k, 'conv')\n",
    "        ):\n",
    "            params = sum(\n",
    "                [\n",
    "                    p.numel()\n",
    "                    for p in module.conv_proj_k.conv.parameters()\n",
    "                ]\n",
    "            )\n",
    "            flops += params * H_KV * W_KV\n",
    "\n",
    "        if (\n",
    "            hasattr(module, 'conv_proj_v')\n",
    "            and hasattr(module.conv_proj_v, 'conv')\n",
    "        ):\n",
    "            params = sum(\n",
    "                [\n",
    "                    p.numel()\n",
    "                    for p in module.conv_proj_v.conv.parameters()\n",
    "                ]\n",
    "            )\n",
    "            flops += params * H_KV * W_KV\n",
    "\n",
    "        params = sum([p.numel() for p in module.proj_q.parameters()])\n",
    "        flops += params * T_Q\n",
    "        params = sum([p.numel() for p in module.proj_k.parameters()])\n",
    "        flops += params * T_KV\n",
    "        params = sum([p.numel() for p in module.proj_v.parameters()])\n",
    "        flops += params * T_KV\n",
    "        params = sum([p.numel() for p in module.proj.parameters()])\n",
    "        flops += params * T\n",
    "\n",
    "        module.__flops__ += flops\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim_in,\n",
    "                 dim_out,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.with_cls_token = kwargs['with_cls_token']\n",
    "\n",
    "        self.norm1 = norm_layer(dim_in)\n",
    "        self.attn = Attention(\n",
    "            dim_in, dim_out, num_heads, qkv_bias, attn_drop, drop,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) \\\n",
    "            if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim_out)\n",
    "\n",
    "        dim_mlp_hidden = int(dim_out * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim_out,\n",
    "            hidden_features=dim_mlp_hidden,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        res = x\n",
    "\n",
    "        x = self.norm1(x)\n",
    "        attn = self.attn(x, h, w)\n",
    "        x = res + self.drop_path(attn)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvEmbed(nn.Module):\n",
    "    \"\"\" Image to Conv Embedding\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 patch_size=7,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=64,\n",
    "                 stride=4,\n",
    "                 padding=2,\n",
    "                 norm_layer=None):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=stride,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 patch_size=16,\n",
    "                 patch_stride=16,\n",
    "                 patch_padding=0,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=768,\n",
    "                 depth=12,\n",
    "                 num_heads=12,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 init='trunc_norm',\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "\n",
    "        self.rearrage = None\n",
    "\n",
    "        self.patch_embed = ConvEmbed(\n",
    "            # img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            stride=patch_stride,\n",
    "            padding=patch_padding,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer\n",
    "        )\n",
    "\n",
    "        with_cls_token = kwargs['with_cls_token']\n",
    "        if with_cls_token:\n",
    "            self.cls_token = nn.Parameter(\n",
    "                torch.zeros(1, 1, embed_dim)\n",
    "            )\n",
    "        else:\n",
    "            self.cls_token = None\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "\n",
    "        blocks = []\n",
    "        for j in range(depth):\n",
    "            blocks.append(\n",
    "                Block(\n",
    "                    dim_in=embed_dim,\n",
    "                    dim_out=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    drop=drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                    drop_path=dpr[j],\n",
    "                    act_layer=act_layer,\n",
    "                    norm_layer=norm_layer,\n",
    "                    **kwargs\n",
    "                )\n",
    "            )\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "        if self.cls_token is not None:\n",
    "            trunc_normal_(self.cls_token, std=.02)\n",
    "\n",
    "        if init == 'xavier':\n",
    "            self.apply(self._init_weights_xavier)\n",
    "        else:\n",
    "            self.apply(self._init_weights_trunc_normal)\n",
    "\n",
    "    def _init_weights_trunc_normal(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            logging.info('=> init weight of Linear from trunc norm')\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                logging.info('=> init bias of Linear to zeros')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def _init_weights_xavier(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            logging.info('=> init weight of Linear from xavier uniform')\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                logging.info('=> init bias of Linear to zeros')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "        cls_tokens = None\n",
    "        if self.cls_token is not None:\n",
    "            # stole cls_tokens impl from Phil Wang, thanks\n",
    "            cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x, H, W)\n",
    "\n",
    "        if self.cls_token is not None:\n",
    "            cls_tokens, x = torch.split(x, [1, H*W], 1)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)\n",
    "\n",
    "        return x, cls_tokens\n",
    "\n",
    "\n",
    "class ConvolutionalVisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_chans=3,\n",
    "                 num_classes=1000,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 init='trunc_norm',\n",
    "                 spec=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.num_stages = spec['NUM_STAGES']\n",
    "        for i in range(self.num_stages):\n",
    "            kwargs = {\n",
    "                'patch_size': spec['PATCH_SIZE'][i],\n",
    "                'patch_stride': spec['PATCH_STRIDE'][i],\n",
    "                'patch_padding': spec['PATCH_PADDING'][i],\n",
    "                'embed_dim': spec['DIM_EMBED'][i],\n",
    "                'depth': spec['DEPTH'][i],\n",
    "                'num_heads': spec['NUM_HEADS'][i],\n",
    "                'mlp_ratio': spec['MLP_RATIO'][i],\n",
    "                'qkv_bias': spec['QKV_BIAS'][i],\n",
    "                'drop_rate': spec['DROP_RATE'][i],\n",
    "                'attn_drop_rate': spec['ATTN_DROP_RATE'][i],\n",
    "                'drop_path_rate': spec['DROP_PATH_RATE'][i],\n",
    "                'with_cls_token': spec['CLS_TOKEN'][i],\n",
    "                'method': spec['QKV_PROJ_METHOD'][i],\n",
    "                'kernel_size': spec['KERNEL_QKV'][i],\n",
    "                'padding_q': spec['PADDING_Q'][i],\n",
    "                'padding_kv': spec['PADDING_KV'][i],\n",
    "                'stride_kv': spec['STRIDE_KV'][i],\n",
    "                'stride_q': spec['STRIDE_Q'][i],\n",
    "            }\n",
    "\n",
    "            stage = VisionTransformer(\n",
    "                in_chans=in_chans,\n",
    "                init=init,\n",
    "                act_layer=act_layer,\n",
    "                norm_layer=norm_layer,\n",
    "                **kwargs\n",
    "            )\n",
    "            setattr(self, f'stage{i}', stage)\n",
    "\n",
    "            in_chans = spec['DIM_EMBED'][i]\n",
    "\n",
    "        dim_embed = spec['DIM_EMBED'][-1]\n",
    "        self.norm = norm_layer(dim_embed)\n",
    "        self.cls_token = spec['CLS_TOKEN'][-1]\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(dim_embed, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        trunc_normal_(self.head.weight, std=0.02)\n",
    "\n",
    "    def init_weights(self, pretrained='', pretrained_layers=[], verbose=True):\n",
    "        if os.path.isfile(pretrained):\n",
    "            pretrained_dict = torch.load(pretrained, map_location='cpu')\n",
    "            logging.info(f'=> loading pretrained model {pretrained}')\n",
    "            model_dict = self.state_dict()\n",
    "            pretrained_dict = {\n",
    "                k: v for k, v in pretrained_dict.items()\n",
    "                if k in model_dict.keys()\n",
    "            }\n",
    "            need_init_state_dict = {}\n",
    "            for k, v in pretrained_dict.items():\n",
    "                need_init = (\n",
    "                        k.split('.')[0] in pretrained_layers\n",
    "                        or pretrained_layers[0] is '*'\n",
    "                )\n",
    "                if need_init:\n",
    "                    if verbose:\n",
    "                        logging.info(f'=> init {k} from {pretrained}')\n",
    "                    if 'pos_embed' in k and v.size() != model_dict[k].size():\n",
    "                        size_pretrained = v.size()\n",
    "                        size_new = model_dict[k].size()\n",
    "                        logging.info(\n",
    "                            '=> load_pretrained: resized variant: {} to {}'\n",
    "                            .format(size_pretrained, size_new)\n",
    "                        )\n",
    "\n",
    "                        ntok_new = size_new[1]\n",
    "                        ntok_new -= 1\n",
    "\n",
    "                        posemb_tok, posemb_grid = v[:, :1], v[0, 1:]\n",
    "\n",
    "                        gs_old = int(np.sqrt(len(posemb_grid)))\n",
    "                        gs_new = int(np.sqrt(ntok_new))\n",
    "\n",
    "                        logging.info(\n",
    "                            '=> load_pretrained: grid-size from {} to {}'\n",
    "                            .format(gs_old, gs_new)\n",
    "                        )\n",
    "\n",
    "                        posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
    "                        zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
    "                        posemb_grid = scipy.ndimage.zoom(\n",
    "                            posemb_grid, zoom, order=1\n",
    "                        )\n",
    "                        posemb_grid = posemb_grid.reshape(1, gs_new ** 2, -1)\n",
    "                        v = torch.tensor(\n",
    "                            np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
    "                        )\n",
    "\n",
    "                    need_init_state_dict[k] = v\n",
    "            self.load_state_dict(need_init_state_dict, strict=False)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        layers = set()\n",
    "        for i in range(self.num_stages):\n",
    "            layers.add(f'stage{i}.pos_embed')\n",
    "            layers.add(f'stage{i}.cls_token')\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for i in range(self.num_stages):\n",
    "            x, cls_tokens = getattr(self, f'stage{i}')(x)\n",
    "\n",
    "        if self.cls_token:\n",
    "            x = self.norm(cls_tokens)\n",
    "            x = torch.squeeze(x)\n",
    "        else:\n",
    "            x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "            x = self.norm(x)\n",
    "            x = torch.mean(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ccc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cvt_13(config, num_classes=100):\n",
    "    spec = config['MODEL']['SPEC']\n",
    "    model = ConvolutionalVisionTransformer(\n",
    "        in_chans=3,\n",
    "        num_classes=num_classes,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=partial(LayerNorm, eps=1e-5),\n",
    "        init=config['MODEL']['SPEC']['INIT'],\n",
    "        spec=spec\n",
    "    )\n",
    "    return model\n",
    "model = create_cvt_13(config, num_classes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b18871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count model parameters\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Model Parameter Count\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return total_params, trainable_params\n",
    "\n",
    "# Count parameters\n",
    "total_params, trainable_params = count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d41901",
   "metadata": {},
   "source": [
    "## Setup Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e23eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_depthwise(m):\n",
    "    return (\n",
    "        isinstance(m, nn.Conv2d)\n",
    "        and m.groups == m.in_channels\n",
    "        and m.groups == m.out_channels\n",
    "    )\n",
    "\n",
    "def set_wd(cfg, model):\n",
    "    without_decay_list = cfg[\"TRAIN\"][\"WITHOUT_WD_LIST\"]\n",
    "    without_decay_depthwise = []\n",
    "    without_decay_norm = []\n",
    "    for m in model.modules():\n",
    "        if _is_depthwise(m) and 'dw' in without_decay_list:\n",
    "            without_decay_depthwise.append(m.weight)\n",
    "        elif isinstance(m, nn.BatchNorm2d) and 'bn' in without_decay_list:\n",
    "            without_decay_norm.append(m.weight)\n",
    "            without_decay_norm.append(m.bias)\n",
    "        elif isinstance(m, nn.GroupNorm) and 'gn' in without_decay_list:\n",
    "            without_decay_norm.append(m.weight)\n",
    "            without_decay_norm.append(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm) and 'ln' in without_decay_list:\n",
    "            without_decay_norm.append(m.weight)\n",
    "            without_decay_norm.append(m.bias)\n",
    "\n",
    "    with_decay = []\n",
    "    without_decay = []\n",
    "\n",
    "    skip = {}\n",
    "    if hasattr(model, 'no_weight_decay'):\n",
    "        skip = model.no_weight_decay()\n",
    "\n",
    "    skip_keys = {}\n",
    "    if hasattr(model, 'no_weight_decay_keywords'):\n",
    "        skip_keys = model.no_weight_decay_keywords()\n",
    "\n",
    "    for n, p in model.named_parameters():\n",
    "        ever_set = False\n",
    "\n",
    "        if p.requires_grad is False:\n",
    "            continue\n",
    "\n",
    "        skip_flag = False\n",
    "        if n in skip:\n",
    "            print('=> set {} wd to 0'.format(n))\n",
    "            without_decay.append(p)\n",
    "            skip_flag = True\n",
    "        else:\n",
    "            for i in skip:\n",
    "                if i in n:\n",
    "                    print('=> set {} wd to 0'.format(n))\n",
    "                    without_decay.append(p)\n",
    "                    skip_flag = True\n",
    "\n",
    "        if skip_flag:\n",
    "            continue\n",
    "\n",
    "        for i in skip_keys:\n",
    "            if i in n:\n",
    "                print('=> set {} wd to 0'.format(n))\n",
    "\n",
    "        if skip_flag:\n",
    "            continue\n",
    "\n",
    "        for pp in without_decay_depthwise:\n",
    "            if p is pp:\n",
    "                if cfg['DEBUG']['DEBUG']:\n",
    "                    print('=> set depthwise({}) wd to 0'.format(n))\n",
    "                without_decay.append(p)\n",
    "                ever_set = True\n",
    "                break\n",
    "\n",
    "        for pp in without_decay_norm:\n",
    "            if p is pp:\n",
    "                if cfg['DEBUG']['DEBUG']:\n",
    "                    print('=> set norm({}) wd to 0'.format(n))\n",
    "                without_decay.append(p)\n",
    "                ever_set = True\n",
    "                break\n",
    "\n",
    "        if (\n",
    "            (not ever_set)\n",
    "            and 'bias' in without_decay_list\n",
    "            and n.endswith('.bias')\n",
    "        ):\n",
    "            if cfg['DEBUG']['DEBUG']:\n",
    "                print('=> set bias({}) wd to 0'.format(n))\n",
    "            without_decay.append(p)\n",
    "        elif not ever_set:\n",
    "            with_decay.append(p)\n",
    "\n",
    "    # assert (len(with_decay) + len(without_decay) == len(list(model.parameters())))\n",
    "    params = [\n",
    "        {'params': with_decay},\n",
    "        {'params': without_decay, 'weight_decay': 0.}\n",
    "    ]\n",
    "    return params\n",
    "def build_optimizer(config, model):\n",
    "    optimizer = None\n",
    "    params = set_wd(config, model)\n",
    "    optimizer = torch.optim.AdamW(params, lr=config['TRAIN']['LR'], weight_decay=config['TRAIN']['WD'])\n",
    "    print(f\"Optimizer: AdamW\")\n",
    "    print(f\"  LR: {config['TRAIN']['LR']}\")\n",
    "    print(f\"  Weight Decay: {config['TRAIN']['WD']}\")\n",
    "    return optimizer\n",
    "optimizer = build_optimizer(config, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1084832f",
   "metadata": {},
   "source": [
    "## Setup Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2018a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "\n",
    "def get_scheduler(config, optimizer, n_iter_per_epoch):\n",
    "\n",
    "    lr_scheduler_args = config['TRAIN']['LR_SCHEDULER']['ARGS']\n",
    "    num_epochs = config['TRAIN']['END_EPOCH']\n",
    "    \n",
    "    warmup_epochs = lr_scheduler_args.get('warmup_epochs', 5)\n",
    "    warmup_lr = lr_scheduler_args.get('warmup_lr', 1e-6)\n",
    "    min_lr = lr_scheduler_args.get('min_lr', 1e-5)\n",
    "    \n",
    "    scheduler = CosineLRScheduler(\n",
    "        optimizer,\n",
    "        t_initial=num_epochs,\n",
    "        lr_min=min_lr,\n",
    "        warmup_t=warmup_epochs,\n",
    "        warmup_lr_init=warmup_lr,\n",
    "        warmup_prefix=True,\n",
    "        cycle_limit=1,\n",
    "        t_in_epochs=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"Scheduler: Cosine with warmup\")\n",
    "    print(f\"  Total epochs: {num_epochs}\")\n",
    "    print(f\"  Warmup epochs: {warmup_epochs}\")\n",
    "    print(f\"  Warmup LR: {warmup_lr}\")\n",
    "    print(f\"  Min LR: {min_lr}\")\n",
    "    \n",
    "    return scheduler\n",
    "\n",
    "scheduler = get_scheduler(config, optimizer, len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b86f6",
   "metadata": {},
   "source": [
    "## Set Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a10b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data.mixup import Mixup\n",
    "class SoftTargetCrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoftTargetCrossEntropy, self).__init__()\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        loss = torch.sum(-target * F.log_softmax(x, dim=-1), dim=-1)\n",
    "        return loss.mean()\n",
    "\n",
    "def build_criterion(is_train=True):\n",
    "    if is_train:\n",
    "        return SoftTargetCrossEntropy()\n",
    "    else:\n",
    "        return nn.CrossEntropyLoss()\n",
    "aug = config['AUG']\n",
    "mixup_fn = Mixup(\n",
    "        mixup_alpha=aug['MIXUP'], cutmix_alpha=aug['MIXCUT'],\n",
    "        cutmix_minmax=None,\n",
    "        prob=aug['MIXUP_PROB'],\n",
    "        label_smoothing=0.0,\n",
    "        num_classes=100\n",
    "    )\n",
    "criterion = build_criterion()\n",
    "criterion.cuda()\n",
    "criterion_eval = build_criterion(is_train=False)\n",
    "criterion_eval.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1443a04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1193ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    maxk = min(max(topk), output.size()[1])\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "    return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c3963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, model, train_loader, gaussian_loader, criterion, optimizer, scheduler, \n",
    "                    config, scaler, mixup_fn):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    Following: https://github.com/microsoft/CvT/blob/main/lib/core/function.py\n",
    "    \"\"\"\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    acc_m = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "    accumulation_steps = config['TRAIN'].get('GRADIENT_ACCUMULATION_STEPS', 1)\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n",
    "    loader = gaussian_loader if epoch < config['TRAIN']['BLUR']['EPOCHS'] else train_loader\n",
    "    for idx, (images, targets) in enumerate(loader):\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        targets = targets.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        images, targets = mixup_fn(images, targets)\n",
    "\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            outputs = model(images)\n",
    "   \n",
    "            loss = criterion(outputs, targets)\n",
    "       \n",
    "            loss = loss / accumulation_steps\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        if (idx + 1) % accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            if idx % 100 == 0 and epoch >= 6:\n",
    "                total_norm = 0\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        total_norm += p.grad.norm().item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "                print(f\"Epoch {epoch}, Batch {idx}, Grad norm: {total_norm:.4f}\")            \n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad.data.clamp_(-0.5, 0.5)                    \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()        \n",
    "\n",
    "        targets_for_acc = torch.argmax(targets, dim=1)\n",
    "\n",
    "        acc = accuracy(outputs, targets_for_acc)\n",
    "\n",
    "        losses.update(loss.item()*accumulation_steps, images.size(0))\n",
    "        acc_m.update(acc[0].item(), images.size(0))\n",
    "        batch_bar.set_postfix(\n",
    "            acc=\"{:.02f}% ({:.02f}%)\".format(acc[0].item(), acc_m.avg),\n",
    "            loss=\"{:.04f} ({:.04f})\".format(loss.item()*accumulation_steps, losses.avg),\n",
    "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "        batch_bar.update() # Update tqdm bar        \n",
    "    \n",
    "    if (idx + 1) % accumulation_steps != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-0.5, 0.5)                    \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)                    \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    batch_bar.close()\n",
    "\n",
    "    scheduler.step(epoch + 1)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return losses.avg, acc_m.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111e7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, val_loader, criterion, config):\n",
    "    losses = AverageMeter()\n",
    "    acc_m = AverageMeter()\n",
    "    \n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val.', ncols=5)\n",
    "\n",
    "    for idx, (images, targets) in enumerate(val_loader):\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        targets = targets.to(DEVICE, non_blocking=True)\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        acc = accuracy(outputs, targets)\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        acc_m.update(acc[0].item(), images.size(0))\n",
    "        batch_bar.set_postfix(\n",
    "            acc=\"{:.02f}% ({:.02f}%)\".format(acc[0].item(), acc_m.avg),\n",
    "            loss=\"{:.04f} ({:.04f})\".format(loss.item(), losses.avg))\n",
    "\n",
    "        batch_bar.update()\n",
    "\n",
    "    batch_bar.close()\n",
    "    print(f' * Acc {acc_m.avg:.3f}')\n",
    "    \n",
    "    return losses.avg, acc_m.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae0f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, epoch, path):\n",
    "    torch.save(\n",
    "        {'model_state_dict'         : model.state_dict(),\n",
    "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
    "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
    "         'epoch'                    : epoch},\n",
    "         path)\n",
    "\n",
    "\n",
    "def load_model(model, optimizer=None, scheduler=None, path='./checkpoint.pth'):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    else:\n",
    "        optimizer = None\n",
    "    if scheduler is not None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    else:\n",
    "        scheduler = None\n",
    "    epoch = checkpoint['epoch']\n",
    "    return model, optimizer, scheduler, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da521cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config['OUTPUT_DIR'], exist_ok=True)\n",
    "model = model.to(DEVICE)\n",
    "scaler = scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea26dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=os.environ.get('WANDB_API_KEY')) # API Key is in your wandb account, under settings (wandb.ai/settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1871908",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    name = \"idl-project-cvt-13-imagenet-blur-1\", ## Wandb creates random run names if you skip this field\n",
    "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "#     id = \"t0xoatlu\", #Insert specific run id here if you want to resume a previous run\n",
    "#     resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"idl-project\", ### Project should be created in your wandb account\n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46e25ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect() # These commands help you when you face CUDA OOM error\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4707afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "best_loss = -1\n",
    "start_epoch = config['TRAIN']['BEGIN_EPOCH']\n",
    "end_epoch = config['TRAIN']['END_EPOCH']\n",
    "\n",
    "print(\"Starting Training\")\n",
    "print(f\"Epochs: {start_epoch} -> {end_epoch}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "for epoch in range(start_epoch, end_epoch):\n",
    "    # epoch\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, end_epoch))\n",
    "    train_loss, train_acc = train_one_epoch(epoch, model, train_loader, gaussian_loader, criterion, optimizer, scheduler, \n",
    "                    config, scaler, mixup_fn)\n",
    "    \n",
    "    val_loss, val_acc = validate(model, val_loader, criterion_eval, config)\n",
    "    is_best = (best_loss) == -1 or val_loss < best_loss\n",
    "    best_loss = min(val_loss, best_loss)\n",
    "    if epoch % 10 == 0:\n",
    "        save_model(model, optimizer, scheduler, epoch,os.path.join(config['OUTPUT_DIR'], f'{epoch}.pth'))\n",
    "    if is_best:\n",
    "        save_model(model, optimizer, scheduler, epoch, os.path.join(config['OUTPUT_DIR'], 'best.pth') )\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Train Acc: {train_acc:.3f}, Val Acc: {val_acc:.3f}\")\n",
    "    metrics = {'train_loss': train_loss, 'val_loss': val_loss, 'train_acc': train_acc, 'val_acc': val_acc, 'epoch': epoch}\n",
    "    if run is not None:\n",
    "        run.log(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c86f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler, epoch = load_model(model, optimizer, scheduler, path=\"./OUTPUT/imagenet-blur-1/best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, test_loader, config):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for images, targets in tqdm(test_loader, desc='Testing'):\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        targets = targets.to(DEVICE, non_blocking=True)\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        \n",
    "        _, preds = outputs.topk(1, 1, True, True)\n",
    "        \n",
    "        all_predictions.extend(preds.cpu().numpy().flatten())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        all_probabilities.extend(probs.cpu().numpy())\n",
    "        \n",
    "        acc1 = accuracy(outputs, targets)\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0].item(), images.size(0))\n",
    "    \n",
    "    return {\n",
    "        'predictions': np.array(all_predictions),\n",
    "        'targets': np.array(all_targets),\n",
    "        'probabilities': np.array(all_probabilities),\n",
    "        'loss': losses.avg,\n",
    "        'top1_acc': top1.avg\n",
    "    }\n",
    "\n",
    "test_results = test(model, val_loader, config)\n",
    "\n",
    "print(\"Final Test Results\")\n",
    "print(f\"Test Loss: {test_results['loss']:.4f}\")\n",
    "print(f\"Test Acc@1: {test_results['top1_acc']:.3f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac60410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
