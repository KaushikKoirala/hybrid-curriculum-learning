{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d79ca51",
      "metadata": {
        "id": "4d79ca51",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install torchsummary torchvision tqdm wandb typing_extensions pytorch_metric_learning timm==0.9.16 einops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f3c0d88",
      "metadata": {
        "id": "7f3c0d88"
      },
      "outputs": [],
      "source": [
        "!pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e465c97b",
      "metadata": {
        "id": "e465c97b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import datasets, transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import gc\n",
        "# from tqdm import tqdm\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics as mt\n",
        "from scipy.optimize import brentq\n",
        "from scipy.interpolate import interp1d\n",
        "import glob\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_metric_learning import samplers\n",
        "import csv\n",
        "import logging\n",
        "from timm.data import create_loader, create_transform\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c88c2bb9",
      "metadata": {
        "id": "c88c2bb9"
      },
      "outputs": [],
      "source": [
        "# Configuration for ResNet-18 with LeRaC on CIFAR-10\n",
        "config = {\n",
        "  \"OUTPUT_DIR\": \"OUTPUT/CIFAR-10-ResNet18-LeRaC\",\n",
        "  \"WORKERS\": 8,\n",
        "  \"PRINT_FREQ\": 500,\n",
        "  \"AMP\": {\n",
        "    \"ENABLED\": True\n",
        "  },\n",
        "  \"MODEL\": {\n",
        "    \"NAME\": \"resnet18\",\n",
        "    \"SPEC\": {\n",
        "      \"NUM_CLASSES\": 10\n",
        "    }\n",
        "  },\n",
        "  \"AUG\": {\n",
        "    \"MIXUP_PROB\": 1.0,\n",
        "    \"MIXUP\": 0.8,\n",
        "    \"MIXCUT\": 1.0,\n",
        "    \"TIMM_AUG\": {\n",
        "      \"USE_LOADER\": False,\n",
        "      \"RE_COUNT\": 1,\n",
        "      \"RE_MODE\": \"pixel\",\n",
        "      \"RE_SPLIT\": False,\n",
        "      \"RE_PROB\": 0.25,\n",
        "      \"AUTO_AUGMENT\": \"rand-m9-mstd0.5-inc1\",\n",
        "      \"HFLIP\": 0.5,\n",
        "      \"VFLIP\": 0.0,\n",
        "      \"COLOR_JITTER\": 0.4,\n",
        "      \"INTERPOLATION\": \"bicubic\"\n",
        "    }\n",
        "  },\n",
        "  \"LOSS\": {\n",
        "    \"LABEL_SMOOTHING\": 0.1\n",
        "  },\n",
        "  \"CUDNN\": {\n",
        "    \"BENCHMARK\": True,\n",
        "    \"DETERMINISTIC\": False,\n",
        "    \"ENABLED\": True\n",
        "  },\n",
        "  \"DATASET\": {\n",
        "    \"DATASET\": \"cifar-10\",\n",
        "    \"DATA_FORMAT\": \"jpg\",\n",
        "    \"ROOT\": \"./cifar-10\",\n",
        "    \"TEST_SET\": \"val\",\n",
        "    \"TRAIN_SET\": \"train\"\n",
        "  },\n",
        "  \"TEST\": {\n",
        "    \"BATCH_SIZE_PER_GPU\": 256,\n",
        "    \"IMAGE_SIZE\": [32, 32],\n",
        "    \"MODEL_FILE\": \"\",\n",
        "    \"INTERPOLATION\": \"bicubic\"\n",
        "  },\n",
        "  \"TRAIN\": {\n",
        "    \"BATCH_SIZE_PER_GPU\": 512,\n",
        "    \"GRADIENT_ACCUMULATION_STEPS\": 4,\n",
        "    \"LR\": 0.05,\n",
        "    \"IMAGE_SIZE\": [32, 32],\n",
        "    \"BEGIN_EPOCH\": 0,\n",
        "    \"END_EPOCH\": 100,\n",
        "    \"LR_CURRICULUM\": {\n",
        "        \"MIN_LR\": 5e-5,\n",
        "        \"WARMUP_EPOCHS\": 5,  #  ADDED: LeRaC warmup duration\n",
        "        \"C\": 10.0            #  ADDED: Exponential growth factor for LeRaC\n",
        "    },\n",
        "    \"CLF_LR_MULTIPLIER\": 0.01,\n",
        "    \"LR_SCHEDULER\": {\n",
        "      \"METHOD\": \"lerac\",\n",
        "      \"ARGS\": {\n",
        "        \"sched\": \"cosine\",\n",
        "        \"warmup_epochs\": 5,  # Used by LeRaC scheduler\n",
        "        \"warmup_lr\": 1e-5,\n",
        "        \"min_lr\": 1e-6,      # Minimum LR for cosine phase\n",
        "        \"cooldown_epochs\": 0,\n",
        "        \"decay_rate\": 0.1\n",
        "      }\n",
        "    },\n",
        "    \"OPTIMIZER\": \"adamW\",\n",
        "    \"WD\": 0.05,\n",
        "    \"WITHOUT_WD_LIST\": [\"bn\", \"bias\", \"ln\"],\n",
        "    \"SHUFFLE\": True\n",
        "  },\n",
        "  \"DEBUG\": {\n",
        "    \"DEBUG\": False\n",
        "  }\n",
        "}\n",
        "\n",
        "# Create output directory\n",
        "import os\n",
        "os.makedirs(config['OUTPUT_DIR'], exist_ok=True)\n",
        "print(f\"✓ Config loaded for CIFAR-10 ResNet-18 with LeRaC\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e29029b6",
      "metadata": {
        "id": "e29029b6"
      },
      "source": [
        "### Defining transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "facc7444",
      "metadata": {
        "id": "facc7444"
      },
      "outputs": [],
      "source": [
        "def build_transforms(config, is_train):\n",
        "    if is_train:\n",
        "        img_size = config['TRAIN']['IMAGE_SIZE'][0]\n",
        "        timm_cfg = config['AUG']['TIMM_AUG']\n",
        "        transforms = create_transform(\n",
        "            input_size = img_size,\n",
        "            is_training = True,\n",
        "            use_prefetcher=False,\n",
        "            no_aug=False,\n",
        "            re_prob=timm_cfg['RE_PROB'],\n",
        "            re_mode=timm_cfg['RE_MODE'],\n",
        "            re_count=timm_cfg['RE_COUNT'],\n",
        "            scale=(0.8, 1.0),\n",
        "            ratio=(3.0/4.0, 4.0/3.0),\n",
        "            hflip=timm_cfg['HFLIP'],\n",
        "            vflip=timm_cfg['VFLIP'],\n",
        "            color_jitter=timm_cfg['COLOR_JITTER'],\n",
        "            auto_augment=timm_cfg['AUTO_AUGMENT'],\n",
        "            interpolation=timm_cfg['INTERPOLATION'],\n",
        "            mean=(0.491, 0.482, 0.446),\n",
        "            std=(0.247, 0.243, 0.261),\n",
        "\n",
        "        )\n",
        "    else:\n",
        "        normalize = T.Normalize(mean=[0.491, 0.482, 0.446], std=[0.247, 0.243, 0.261])\n",
        "        img_size = config['TEST']['IMAGE_SIZE'][0]\n",
        "        transforms = T.Compose([\n",
        "            T.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "    return transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c46aa4e",
      "metadata": {
        "id": "6c46aa4e"
      },
      "source": [
        "### Building Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "234f1b25",
      "metadata": {
        "id": "234f1b25"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def build_dataset(config, is_train):\n",
        "    '''\n",
        "    In the CIFAR file it will call the appropriate method\n",
        "    '''\n",
        "    dataset = None\n",
        "    transforms = build_transforms(config, is_train)\n",
        "    dataset = datasets.CIFAR10(root=config['DATASET']['ROOT'], train=is_train, download=True, transform=transforms)\n",
        "    logging.info(f'load samples: {len(dataset)}, is_train: {is_train}')\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33df8988",
      "metadata": {
        "id": "33df8988"
      },
      "source": [
        "### Building Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ab5c15",
      "metadata": {
        "id": "c2ab5c15"
      },
      "outputs": [],
      "source": [
        "def build_dataloader(config, is_train):\n",
        "    if is_train:\n",
        "        batch_size_per_gpu = config['TRAIN']['BATCH_SIZE_PER_GPU']\n",
        "        shuffle = True\n",
        "    else:\n",
        "        batch_size_per_gpu = config['TEST']['BATCH_SIZE_PER_GPU']\n",
        "        shuffle = False\n",
        "    dataset = build_dataset(config, is_train)\n",
        "    sampler = None\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size_per_gpu,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=config['WORKERS'],\n",
        "        pin_memory=True,\n",
        "        sampler=sampler,\n",
        "        drop_last=is_train,\n",
        "    )\n",
        "    return data_loader\n",
        "train_loader = build_dataloader(config, is_train=True)\n",
        "val_loader = build_dataloader(config, is_train=False)\n",
        "\n",
        "print(f\"\\nDataLoader Info:\")\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n",
        "print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
        "print(f\"Val dataset size: {len(val_loader.dataset)}\")\n",
        "print(f\"Number of classes: {len(train_loader.dataset.classes)}\")\n",
        "\n",
        "# Test loading a batch\n",
        "images, labels = next(iter(train_loader))\n",
        "print(f\"\\nBatch shapes:\")\n",
        "print(f\"Images: {images.shape}\")\n",
        "print(f\"Labels: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2afd8966",
      "metadata": {
        "id": "2afd8966"
      },
      "outputs": [],
      "source": [
        "# ResNet-18 Model Definition\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.planes = planes\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = F.relu(self.bn1(out))\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes, first_kernel_size=3):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=first_kernel_size, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = F.relu(self.bn1(out))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        spatial_size = out.size(2)\n",
        "        out = F.avg_pool2d(out, spatial_size, 1)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(num_classes, first_kernel_size=3):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes, first_kernel_size=first_kernel_size)\n",
        "\n",
        "\n",
        "print(\"ResNet-18 model defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "915ccc8a",
      "metadata": {
        "id": "915ccc8a"
      },
      "outputs": [],
      "source": [
        "# Create ResNet-18 model for CIFAR-10\n",
        "model = ResNet18(num_classes=10, first_kernel_size=3)\n",
        "print(\"ResNet-18 model created for CIFAR-10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56b18871",
      "metadata": {
        "id": "56b18871"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"Count model parameters\"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Model Parameter Count\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return total_params, trainable_params\n",
        "\n",
        "# Count parameters\n",
        "total_params, trainable_params = count_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4d41901",
      "metadata": {
        "id": "f4d41901"
      },
      "source": [
        "## Setup Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e23eef8",
      "metadata": {
        "id": "5e23eef8"
      },
      "outputs": [],
      "source": [
        "def _is_depthwise(m):\n",
        "    return (\n",
        "        isinstance(m, nn.Conv2d)\n",
        "        and m.groups == m.in_channels\n",
        "        and m.groups == m.out_channels\n",
        "    )\n",
        "\n",
        "def set_wd(cfg, model):\n",
        "    \"\"\"Separate parameters by weight decay\"\"\"\n",
        "    without_decay_list = cfg[\"TRAIN\"][\"WITHOUT_WD_LIST\"]\n",
        "    without_decay_depthwise = []\n",
        "    without_decay_norm = []\n",
        "\n",
        "    for m in model.modules():\n",
        "        if _is_depthwise(m) and 'dw' in without_decay_list:\n",
        "            without_decay_depthwise.append(m.weight)\n",
        "        elif isinstance(m, nn.BatchNorm2d) and 'bn' in without_decay_list:\n",
        "            without_decay_norm.append(m.weight)\n",
        "            without_decay_norm.append(m.bias)\n",
        "        elif isinstance(m, nn.GroupNorm) and 'gn' in without_decay_list:\n",
        "            without_decay_norm.append(m.weight)\n",
        "            without_decay_norm.append(m.bias)\n",
        "        elif isinstance(m, nn.LayerNorm) and 'ln' in without_decay_list:\n",
        "            without_decay_norm.append(m.weight)\n",
        "            without_decay_norm.append(m.bias)\n",
        "\n",
        "    with_decay = []\n",
        "    without_decay = []\n",
        "\n",
        "    skip = {}\n",
        "    if hasattr(model, 'no_weight_decay'):\n",
        "        skip = model.no_weight_decay()\n",
        "\n",
        "    skip_keys = {}\n",
        "    if hasattr(model, 'no_weight_decay_keywords'):\n",
        "        skip_keys = model.no_weight_decay_keywords()\n",
        "\n",
        "    for n, p in model.named_parameters():\n",
        "        ever_set = False\n",
        "\n",
        "        if p.requires_grad is False:\n",
        "            continue\n",
        "\n",
        "        skip_flag = False\n",
        "        if n in skip:\n",
        "            print('=> set {} wd to 0'.format(n))\n",
        "            without_decay.append(p)\n",
        "            skip_flag = True\n",
        "        else:\n",
        "            for i in skip:\n",
        "                if i in n:\n",
        "                    print('=> set {} wd to 0'.format(n))\n",
        "                    without_decay.append(p)\n",
        "                    skip_flag = True\n",
        "\n",
        "        if skip_flag:\n",
        "            continue\n",
        "\n",
        "        for i in skip_keys:\n",
        "            if i in n:\n",
        "                print('=> set {} wd to 0'.format(n))\n",
        "\n",
        "        if skip_flag:\n",
        "            continue\n",
        "\n",
        "        for pp in without_decay_depthwise:\n",
        "            if p is pp:\n",
        "                if cfg['DEBUG']['DEBUG']:\n",
        "                    print('=> set depthwise({}) wd to 0'.format(n))\n",
        "                without_decay.append(p)\n",
        "                ever_set = True\n",
        "                break\n",
        "\n",
        "        for pp in without_decay_norm:\n",
        "            if p is pp:\n",
        "                if cfg['DEBUG']['DEBUG']:\n",
        "                    print('=> set norm({}) wd to 0'.format(n))\n",
        "                without_decay.append(p)\n",
        "                ever_set = True\n",
        "                break\n",
        "\n",
        "        if (\n",
        "            (not ever_set)\n",
        "            and 'bias' in without_decay_list\n",
        "            and n.endswith('.bias')\n",
        "        ):\n",
        "            if cfg['DEBUG']['DEBUG']:\n",
        "                print('=> set bias({}) wd to 0'.format(n))\n",
        "            without_decay.append(p)\n",
        "        elif not ever_set:\n",
        "            with_decay.append(p)\n",
        "\n",
        "    params = [\n",
        "        {'params': with_decay},\n",
        "        {'params': without_decay, 'weight_decay': 0.}\n",
        "    ]\n",
        "    return params\n",
        "\n",
        "\n",
        "def build_optimizer_resnet18_lerac(model, config):\n",
        "    \"\"\"\n",
        "    Build optimizer with LeRaC (Learning Rate Curriculum) for ResNet-18\n",
        "    \"\"\"\n",
        "    base_lr = config['TRAIN']['LR']\n",
        "    min_lr = config['TRAIN']['LR_CURRICULUM']['MIN_LR']\n",
        "    weight_decay = config['TRAIN']['WD']\n",
        "\n",
        "\n",
        "    layers = ['layer1', 'layer2', 'layer3', 'layer4']\n",
        "    blocks = [str(i) for i in range(2)]  # ResNet-18 has 2 blocks per layer\n",
        "\n",
        "    # Build learning rate dictionary with GENTLER decay\n",
        "    dictionary_lr = {}\n",
        "    start_lr = base_lr * 0.5  # Start at 50% of base (instead of 10%)\n",
        "\n",
        "    for layer in layers:\n",
        "        for block in blocks:\n",
        "            key = f\"{layer}.{block}\"\n",
        "            dictionary_lr[key] = max(start_lr, min_lr)\n",
        "            if start_lr >= min_lr:\n",
        "                start_lr *= 0.5  # Decay by 50% each step (instead of 10%)\n",
        "\n",
        "    # Get parameter groups with weight decay handling\n",
        "    params_with_wd_info = set_wd(config, model)\n",
        "\n",
        "    # Build parameter groups with layer-wise learning rates\n",
        "    param_to_name = {id(p): n for n, p in model.named_parameters()}\n",
        "    new_param_groups = []\n",
        "\n",
        "    # Process both with_decay and without_decay groups\n",
        "    for group in params_with_wd_info:\n",
        "        group_params = group['params'] if isinstance(group['params'], list) else [group['params']]\n",
        "        wd = group.get('weight_decay', weight_decay)\n",
        "\n",
        "        for param in group_params:\n",
        "            name = param_to_name.get(id(param), \"unknown\")\n",
        "            assigned_lr = base_lr\n",
        "\n",
        "            # Check if parameter belongs to a specific layer block\n",
        "            if name.startswith('conv1') or name.startswith('bn1') or name.startswith('maxpool'):\n",
        "                assigned_lr = base_lr\n",
        "            elif 'linear' in name:\n",
        "                assigned_lr = base_lr * config['TRAIN'].get('CLF_LR_MULTIPLIER', 0.01)\n",
        "            else:\n",
        "                # Check layer blocks\n",
        "                for key in dictionary_lr:\n",
        "                    if name.startswith(key):\n",
        "                        assigned_lr = dictionary_lr[key]\n",
        "                        break\n",
        "\n",
        "            new_param_groups.append({\n",
        "                'params': param,\n",
        "                'lr': assigned_lr,\n",
        "                'weight_decay': wd\n",
        "            })\n",
        "\n",
        "    optimizer = torch.optim.AdamW(new_param_groups)\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Optimizer: AdamW with LeRaC for ResNet-18\")\n",
        "    print(f\"Total parameter groups: {len(new_param_groups)}\")\n",
        "    print(f\"Base LR: {base_lr:.10f}\")\n",
        "    print(f\"Min LR: {min_lr:.10f}\")\n",
        "    print(f\"Weight Decay: {weight_decay}\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Layer-wise Learning Rates (NEW):\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Print the LR schedule to verify\n",
        "    print(f\"conv1/bn1:       {base_lr:.6f}\")\n",
        "    for layer in layers:\n",
        "        for block in blocks:\n",
        "            key = f\"{layer}.{block}\"\n",
        "            if key in dictionary_lr:\n",
        "                print(f\"{key:15s} {dictionary_lr[key]:.6f}\")\n",
        "    print(f\"classifier:      {base_lr * config['TRAIN'].get('CLF_LR_MULTIPLIER', 0.01):.6f}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Show sample from actual params\n",
        "    import random\n",
        "    sample_size = min(10, len(new_param_groups))\n",
        "    sampled_indices = random.sample(range(len(new_param_groups)), sample_size)\n",
        "    sampled_info = []\n",
        "\n",
        "    for idx in sampled_indices:\n",
        "        param = new_param_groups[idx]['params']\n",
        "        name = param_to_name.get(id(param), \"unknown\")\n",
        "        lr = new_param_groups[idx]['lr']\n",
        "        wd = new_param_groups[idx]['weight_decay']\n",
        "        sampled_info.append((name, lr, wd))\n",
        "\n",
        "    print(\"\\nSample Parameters:\")\n",
        "    for name, lr, wd in sorted(sampled_info, key=lambda x: x[1], reverse=True):\n",
        "        print(f\"{name:55s} LR: {lr:.6f}  WD: {wd:.4f}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "# Build the optimizer\n",
        "optimizer = build_optimizer_resnet18_lerac(model, config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1084832f",
      "metadata": {
        "id": "1084832f"
      },
      "source": [
        "## Setup Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2018a522",
      "metadata": {
        "id": "2018a522"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import _LRScheduler, SequentialLR, CosineAnnealingLR\n",
        "\n",
        "class LeRaCScheduler(_LRScheduler):\n",
        "    \"\"\"\n",
        "    LeRaC Warmup Scheduler: Exponential growth during warmup\n",
        "    Based on Eq.(9): lr_t = init_lr * (c^t) for t in {1..warmup_epochs}\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, base_lr, warmup_epochs, c=10.0, last_epoch=-1):\n",
        "        self.base_lr = float(base_lr)\n",
        "        self.warmup_epochs = max(1, int(warmup_epochs))\n",
        "        self.c = float(c)\n",
        "        super().__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        # After warmup: all groups use base_lr\n",
        "        if self.last_epoch >= self.warmup_epochs:\n",
        "            return [self.base_lr for _ in self.optimizer.param_groups]\n",
        "\n",
        "        # Warmup epoch counter t ∈ {1..warmup_epochs}\n",
        "        t = self.last_epoch + 1\n",
        "\n",
        "        lrs = []\n",
        "        for g in self.optimizer.param_groups:\n",
        "            init = float(g.get('_init_lr', g['lr']))\n",
        "            # Exponential growth per Eq.(9), clamped to base_lr\n",
        "            lr_t = init * (self.c ** t)\n",
        "            lrs.append(min(lr_t, self.base_lr))  # Don't exceed base_lr\n",
        "        return lrs\n",
        "\n",
        "\n",
        "def get_scheduler(config, optimizer):\n",
        "    \"\"\"\n",
        "    Build LeRaC scheduler: Exponential warmup → Cosine annealing\n",
        "    \"\"\"\n",
        "    num_epochs = int(config['TRAIN']['END_EPOCH'])\n",
        "    warmup_epochs = int(config['TRAIN']['LR_CURRICULUM'].get('WARMUP_EPOCHS', 5))\n",
        "    base_lr = float(config['TRAIN']['LR'])\n",
        "    min_lr = float(config['TRAIN']['LR_SCHEDULER']['ARGS'].get('min_lr', 1e-6))\n",
        "    c = float(config['TRAIN']['LR_CURRICULUM'].get('C', 10.0))  # Growth factor\n",
        "\n",
        "    # Store initial LRs for LeRaC warmup\n",
        "    for g in optimizer.param_groups:\n",
        "        if '_init_lr' not in g:\n",
        "            g['_init_lr'] = g['lr']\n",
        "\n",
        "    # Phase 1: LeRaC exponential warmup\n",
        "    lerac_scheduler = LeRaCScheduler(\n",
        "        optimizer,\n",
        "        base_lr=base_lr,\n",
        "        warmup_epochs=warmup_epochs,\n",
        "        c=c,\n",
        "        last_epoch=-1\n",
        "    )\n",
        "\n",
        "    # Phase 2: Cosine annealing after warmup\n",
        "    cosine_epochs = max(1, num_epochs - warmup_epochs)\n",
        "    cosine_scheduler = CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=cosine_epochs,\n",
        "        eta_min=min_lr\n",
        "    )\n",
        "\n",
        "    # Combine: LeRaC warmup → Cosine decay\n",
        "    scheduler = SequentialLR(\n",
        "        optimizer,\n",
        "        schedulers=[lerac_scheduler, cosine_scheduler],\n",
        "        milestones=[warmup_epochs]\n",
        "    )\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"LeRaC Scheduler Configuration\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Phase 1 - LeRaC Warmup:\")\n",
        "    print(f\"  Duration: {warmup_epochs} epochs\")\n",
        "    print(f\"  Growth factor (c): {c}\")\n",
        "    print(f\"  Target LR: {base_lr}\")\n",
        "    print(f\"\\nPhase 2 - Cosine Annealing:\")\n",
        "    print(f\"  Duration: {cosine_epochs} epochs\")\n",
        "    print(f\"  Start LR: {base_lr}\")\n",
        "    print(f\"  Min LR: {min_lr}\")\n",
        "    print(f\"\\nTotal epochs: {num_epochs}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return scheduler\n",
        "\n",
        "\n",
        "# Build scheduler for CIFAR-10\n",
        "scheduler = get_scheduler(config, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e06b86f6",
      "metadata": {
        "id": "e06b86f6"
      },
      "source": [
        "## Set Criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83a10b7d",
      "metadata": {
        "id": "83a10b7d"
      },
      "outputs": [],
      "source": [
        "from timm.data.mixup import Mixup\n",
        "\n",
        "class SoftTargetCrossEntropy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SoftTargetCrossEntropy, self).__init__()\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        loss = torch.sum(-target * F.log_softmax(x, dim=-1), dim=-1)\n",
        "        return loss.mean()\n",
        "\n",
        "def build_criterion(is_train=True):\n",
        "    if is_train:\n",
        "        return SoftTargetCrossEntropy()\n",
        "    else:\n",
        "        return nn.CrossEntropyLoss()\n",
        "\n",
        "# Dynamically get number of classes from config\n",
        "num_classes = config['MODEL']['SPEC']['NUM_CLASSES']\n",
        "\n",
        "aug = config['AUG']\n",
        "mixup_fn = Mixup(\n",
        "    mixup_alpha=aug['MIXUP'],\n",
        "    cutmix_alpha=aug['MIXCUT'],\n",
        "    cutmix_minmax=None,\n",
        "    prob=aug['MIXUP_PROB'],\n",
        "    label_smoothing=0.0,\n",
        "    num_classes=num_classes  # Dynamic based on dataset\n",
        ")\n",
        "\n",
        "criterion = build_criterion()\n",
        "criterion.cuda()\n",
        "criterion_eval = build_criterion(is_train=False)\n",
        "criterion_eval.cuda()\n",
        "\n",
        "print(f\"✓ Criterion setup complete for {num_classes} classes\")\n",
        "print(f\"  Mixup alpha: {aug['MIXUP']}\")\n",
        "print(f\"  Cutmix alpha: {aug['MIXCUT']}\")\n",
        "print(f\"  Mixup probability: {aug['MIXUP_PROB']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1443a04c",
      "metadata": {
        "id": "1443a04c"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1193ec5c",
      "metadata": {
        "id": "1193ec5c"
      },
      "outputs": [],
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    maxk = min(max(topk), output.size()[1])\n",
        "    batch_size = target.size(0)\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
        "    return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d7c3963",
      "metadata": {
        "id": "4d7c3963"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch, model, train_loader, criterion, optimizer, scheduler,\n",
        "                    config, scaler, mixup_fn):\n",
        "    \"\"\"\n",
        "    Train for one epoch\n",
        "    Note: scheduler is passed but NOT stepped here - stepping happens in main loop\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    losses = AverageMeter()\n",
        "    acc_m = AverageMeter()\n",
        "\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True,\n",
        "                     leave=False, position=0, desc=f'Train Epoch {epoch+1}')\n",
        "\n",
        "    num_updates = epoch * len(train_loader)\n",
        "\n",
        "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "        images = images.to(DEVICE, non_blocking=True)\n",
        "        targets = targets.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        # Apply mixup/cutmix\n",
        "        if mixup_fn is not None:\n",
        "            images, targets = mixup_fn(images, targets)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=config['AMP']['ENABLED']):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Gradient accumulation\n",
        "        if (batch_idx + 1) % config['TRAIN']['GRADIENT_ACCUMULATION_STEPS'] == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Metrics\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "\n",
        "        # Calculate accuracy (handle both hard labels and soft targets from mixup)\n",
        "        if targets.dim() == 1:  # Hard labels\n",
        "            acc = accuracy(outputs, targets)[0]\n",
        "        else:  # Soft targets from mixup\n",
        "            _, hard_targets = targets.max(dim=1)\n",
        "            acc = accuracy(outputs, hard_targets)[0]\n",
        "\n",
        "        acc_m.update(acc.item(), images.size(0))\n",
        "\n",
        "        # Update progress bar\n",
        "        batch_bar.set_postfix(\n",
        "            loss=f\"{losses.avg:.4f}\",\n",
        "            acc=f\"{acc_m.avg:.2f}%\",\n",
        "            lr=f\"{optimizer.param_groups[0]['lr']:.6f}\"\n",
        "        )\n",
        "        batch_bar.update()\n",
        "\n",
        "        num_updates += 1\n",
        "\n",
        "    # Final cleanup\n",
        "    if (batch_idx + 1) % config['TRAIN']['GRADIENT_ACCUMULATION_STEPS'] != 0:\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return losses.avg, acc_m.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d111e7d1",
      "metadata": {
        "id": "d111e7d1"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate(model, val_loader, criterion, config):\n",
        "    losses = AverageMeter()\n",
        "    acc_m = AverageMeter()\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val.', ncols=5)\n",
        "\n",
        "    for idx, (images, targets) in enumerate(val_loader):\n",
        "        images = images.to(DEVICE, non_blocking=True)\n",
        "        targets = targets.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        acc = accuracy(outputs, targets)\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        acc_m.update(acc[0].item(), images.size(0))\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.02f}% ({:.02f}%)\".format(acc[0].item(), acc_m.avg),\n",
        "            loss=\"{:.04f} ({:.04f})\".format(loss.item(), losses.avg))\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "    print(f' * Acc {acc_m.avg:.3f}')\n",
        "\n",
        "    return losses.avg, acc_m.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ae0f47e",
      "metadata": {
        "id": "3ae0f47e"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         'epoch'                    : epoch},\n",
        "         path)\n",
        "\n",
        "\n",
        "def load_model(model, optimizer=None, scheduler=None, path='./checkpoint.pth'):\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    else:\n",
        "        optimizer = None\n",
        "    if scheduler is not None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    else:\n",
        "        scheduler = None\n",
        "    epoch = checkpoint['epoch']\n",
        "    return model, optimizer, scheduler, epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da521cae",
      "metadata": {
        "id": "da521cae"
      },
      "outputs": [],
      "source": [
        "os.makedirs(config['OUTPUT_DIR'], exist_ok=True)\n",
        "model = model.to(DEVICE)\n",
        "scaler = scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea26dc10",
      "metadata": {
        "id": "ea26dc10"
      },
      "outputs": [],
      "source": [
        "wandb.login(key=os.environ.get('WANDB_API_KEY')) # API Key is in your wandb account, under settings (wandb.ai/settings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1871908",
      "metadata": {
        "id": "c1871908"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    name = \"idl-project-cvt-13-baseline-no-curriculum-learning-cifar-10-lerac-2\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    project = \"idl-project\", ### Project should be created in your wandb account\n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38aa432a",
      "metadata": {
        "id": "38aa432a"
      },
      "outputs": [],
      "source": [
        "gc.collect() # These commands help you when you face CUDA OOM error\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79070f24",
      "metadata": {
        "id": "79070f24"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "train_accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "best_loss = -1\n",
        "start_epoch = config['TRAIN']['BEGIN_EPOCH']\n",
        "end_epoch = config['TRAIN']['END_EPOCH']\n",
        "\n",
        "print(\"Starting Training\")\n",
        "print(f\"Epochs: {start_epoch} -> {end_epoch}\")\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "\n",
        "for epoch in range(start_epoch, end_epoch):\n",
        "    # epoch\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, end_epoch))\n",
        "    train_loss, train_acc = train_one_epoch(epoch, model, train_loader, criterion, optimizer, scheduler,\n",
        "                    config, scaler, mixup_fn)\n",
        "\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion_eval, config)\n",
        "    scheduler.step()\n",
        "    is_best = (best_loss) == -1 or val_loss < best_loss\n",
        "    best_loss = min(val_loss, best_loss)\n",
        "    save_model(model, optimizer, scheduler, epoch,os.path.join(config['OUTPUT_DIR'], f'{epoch}.pth'))\n",
        "    if is_best:\n",
        "        save_model(model, optimizer, scheduler, epoch, os.path.join(config['OUTPUT_DIR'], 'best.pth') )\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"  Train Acc: {train_acc:.3f}, Val Acc: {val_acc:.3f}\")\n",
        "    metrics = {'train_loss': train_loss, 'val_loss': val_loss, 'train_acc': train_acc, 'val_acc': val_acc, 'epoch': epoch}\n",
        "    if run is not None:\n",
        "        run.log(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d8a06d1",
      "metadata": {
        "id": "5d8a06d1"
      },
      "outputs": [],
      "source": [
        "model, optimizer, scheduler, epoch = load_model(model, optimizer, scheduler, path=\"./OUTPUTOUTPUT/CIFAR-10-LeRaC/best.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb2814d5",
      "metadata": {
        "id": "fb2814d5"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "train_accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "best_loss = -1\n",
        "start_epoch = epoch\n",
        "end_epoch = config['TRAIN']['END_EPOCH']\n",
        "\n",
        "print(\"Starting Training\")\n",
        "print(f\"Epochs: {start_epoch} -> {end_epoch}\")\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "\n",
        "for epoch in range(start_epoch, end_epoch):\n",
        "    # epoch\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, end_epoch))\n",
        "    train_loss, train_acc = train_one_epoch(epoch, model, train_loader, criterion, optimizer, scheduler,\n",
        "                    config, scaler, mixup_fn)\n",
        "\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion_eval, config)\n",
        "    is_best = (best_loss) == -1 or val_loss < best_loss\n",
        "    best_loss = min(val_loss, best_loss)\n",
        "    save_model(model, optimizer, scheduler, epoch,os.path.join(config['OUTPUT_DIR'], f'{epoch}.pth'))\n",
        "    if is_best:\n",
        "        save_model(model, optimizer, scheduler, epoch, os.path.join(config['OUTPUT_DIR'], 'best.pth') )\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"  Train Acc: {train_acc:.3f}, Val Acc: {val_acc:.3f}\")\n",
        "    metrics = {'train_loss': train_loss, 'val_loss': val_loss, 'train_acc': train_acc, 'val_acc': val_acc, 'epoch': epoch}\n",
        "    if run is not None:\n",
        "        run.log(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "867bef61",
      "metadata": {
        "id": "867bef61",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test(model, test_loader, config):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for images, targets in tqdm(test_loader, desc='Testing'):\n",
        "        images = images.to(DEVICE, non_blocking=True)\n",
        "        targets = targets.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=True):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "        _, preds = outputs.topk(1, 1, True, True)\n",
        "\n",
        "        all_predictions.extend(preds.cpu().numpy().flatten())\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "        all_probabilities.extend(probs.cpu().numpy())\n",
        "\n",
        "        acc1 = accuracy(outputs, targets)\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        top1.update(acc1[0].item(), images.size(0))\n",
        "\n",
        "    return {\n",
        "        'predictions': np.array(all_predictions),\n",
        "        'targets': np.array(all_targets),\n",
        "        'probabilities': np.array(all_probabilities),\n",
        "        'loss': losses.avg,\n",
        "        'top1_acc': top1.avg\n",
        "    }\n",
        "\n",
        "test_results = test(model, val_loader, config)\n",
        "\n",
        "print(\"Final Test Results\")\n",
        "print(f\"Test Loss: {test_results['loss']:.4f}\")\n",
        "print(f\"Test Acc@1: {test_results['top1_acc']:.3f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}