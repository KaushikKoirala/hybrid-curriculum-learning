{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ResNet-18 with Gaussian Blur Curriculum Learning on CIFAR-10\n",
        "\n",
        "This notebook implements ResNet-18 with Gaussian blur curriculum learning on CIFAR-10, based on the approach from the CVT_13_blur notebook.\n",
        "\n",
        "## Curriculum Learning Strategy\n",
        "- **Early epochs**: Train with Gaussian blur applied to images\n",
        "- **Later epochs**: Train with original sharp images\n",
        "- **Blur parameters**: Kernel size 7, sigma 1.0, applied for first 20 epochs\n",
        "\n",
        "## Model: ResNet-18\n",
        "- Standard ResNet-18 architecture\n",
        "- ~11.7M parameters\n",
        "- Curriculum learning with Gaussian blur\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install torchsummary torchvision tqdm wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import os\n",
        "import gc\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "import logging\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration with blur curriculum\n",
        "config = {\n",
        "    'batch_size': 128,\n",
        "    'num_epochs': 100,\n",
        "    'learning_rate': 0.01,\n",
        "    'weight_decay': 5e-4,\n",
        "    'momentum': 0.9,\n",
        "    'num_classes': 10,\n",
        "    'image_size': 32,\n",
        "    'num_workers': 4,\n",
        "    'save_dir': './checkpoints_cifar10_blur',\n",
        "    'use_wandb': True,\n",
        "    'project_name': 'resnet18-cifar10-blur-curriculum',\n",
        "    'run_name': 'blur-curriculum-training',\n",
        "    'DATASET': {\n",
        "        'ROOT': './data'\n",
        "    },\n",
        "    'BLUR': {\n",
        "        'KERNEL_SIZE': 7,\n",
        "        'SIGMA': 1.0,\n",
        "        'EPOCHS': 20  # Number of epochs to use blur\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in config.items():\n",
        "    if isinstance(value, dict):\n",
        "        print(f\"  {key}:\")\n",
        "        for sub_key, sub_value in value.items():\n",
        "            print(f\"    {sub_key}: {sub_value}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Loading and Preprocessing with Blur Curriculum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_transforms(config, is_train):\n",
        "    \"\"\"Build transforms for CIFAR-10\"\"\"\n",
        "    if is_train:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "        ])\n",
        "    else:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "        ])\n",
        "    return transform\n",
        "\n",
        "def build_gaussian_transforms(config):\n",
        "    \"\"\"Build transforms with Gaussian blur for curriculum learning\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.GaussianBlur(\n",
        "            kernel_size=config['BLUR']['KERNEL_SIZE'], \n",
        "            sigma=config['BLUR']['SIGMA']\n",
        "        ),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "    return transform\n",
        "\n",
        "def build_dataset(config, is_train):\n",
        "    '''\n",
        "    Build CIFAR-10 dataset\n",
        "    '''\n",
        "    transforms = build_transforms(config, is_train)\n",
        "    dataset = datasets.CIFAR10(\n",
        "        root=config['DATASET']['ROOT'], \n",
        "        train=is_train, \n",
        "        download=True, \n",
        "        transform=transforms\n",
        "    )\n",
        "    logging.info(f'load samples: {len(dataset)}, is_train: {is_train}')\n",
        "    return dataset\n",
        "\n",
        "def build_gaussian_dataset(config):\n",
        "    \"\"\"Build dataset with Gaussian blur for curriculum learning\"\"\"\n",
        "    transforms = build_gaussian_transforms(config)\n",
        "    dataset = datasets.CIFAR10(\n",
        "        root=config['DATASET']['ROOT'], \n",
        "        train=True, \n",
        "        download=True, \n",
        "        transform=transforms\n",
        "    )\n",
        "    logging.info(f'load samples: {len(dataset)}, is_train: True (blur)')\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the datasets\n",
        "print(\"Loading CIFAR-10 dataset...\")\n",
        "train_dataset = build_dataset(config, is_train=True)\n",
        "val_dataset = build_dataset(config, is_train=False)\n",
        "gaussian_dataset = build_gaussian_dataset(config)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=config['batch_size'], \n",
        "    shuffle=True,\n",
        "    num_workers=config['num_workers'], \n",
        "    pin_memory=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, \n",
        "    batch_size=config['batch_size'], \n",
        "    shuffle=False,\n",
        "    num_workers=config['num_workers'], \n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "gaussian_loader = DataLoader(\n",
        "    gaussian_dataset, \n",
        "    batch_size=config['batch_size'], \n",
        "    shuffle=True,\n",
        "    num_workers=config['num_workers'], \n",
        "    pin_memory=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Gaussian blur samples: {len(gaussian_dataset)}\")\n",
        "print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
        "print(f\"Classes: {train_dataset.classes}\")\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "print(f\"Gaussian blur batches: {len(gaussian_loader)}\")\n",
        "\n",
        "# Test loading a batch from each loader\n",
        "print(\"\\nTesting data loaders:\")\n",
        "train_images, train_labels = next(iter(train_loader))\n",
        "gaussian_images, gaussian_labels = next(iter(gaussian_loader))\n",
        "print(f\"Train batch shape: {train_images.shape}\")\n",
        "print(f\"Gaussian batch shape: {gaussian_images.shape}\")\n",
        "print(f\"Blur curriculum epochs: {config['BLUR']['EPOCHS']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ResNet-18 Model Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # First conv layer for CIFAR-10 (32x32 input)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        # ResNet layers\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        \n",
        "        # Global average pooling and classifier\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(num_classes=10):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "\n",
        "# Test model creation\n",
        "model = ResNet18(num_classes=config['num_classes'])\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"ResNet-18 created with {total_params:,} parameters\")\n",
        "print(f\"Model size: {total_params/1e6:.2f}M parameters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training and Evaluation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def train_epoch_with_curriculum(model, train_loader, gaussian_loader, criterion, optimizer, device, epoch, config):\n",
        "    \"\"\"Train for one epoch with blur curriculum\"\"\"\n",
        "    model.train()\n",
        "    \n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    \n",
        "    # Choose loader based on curriculum\n",
        "    if epoch < config['BLUR']['EPOCHS']:\n",
        "        loader = gaussian_loader\n",
        "        curriculum_type = \"Blur\"\n",
        "    else:\n",
        "        loader = train_loader\n",
        "        curriculum_type = \"Sharp\"\n",
        "    \n",
        "    pbar = tqdm(loader, desc=f'Epoch {epoch+1} ({curriculum_type})')\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Update metrics\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), data.size(0))\n",
        "        top1.update(acc1[0], data.size(0))\n",
        "        top5.update(acc5[0], data.size(0))\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'Loss': f'{losses.avg:.4f}',\n",
        "            'Acc@1': f'{top1.avg:.2f}%',\n",
        "            'Acc@5': f'{top5.avg:.2f}%',\n",
        "            'Type': curriculum_type\n",
        "        })\n",
        "    \n",
        "    return losses.avg, top1.avg, top5.avg\n",
        "\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(val_loader, desc='Validation')\n",
        "        for data, target in pbar:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            # Update metrics\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), data.size(0))\n",
        "            top1.update(acc1[0], data.size(0))\n",
        "            top5.update(acc5[0], data.size(0))\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'Loss': f'{losses.avg:.4f}',\n",
        "                'Acc@1': f'{top1.avg:.2f}%',\n",
        "                'Acc@5': f'{top5.avg:.2f}%'\n",
        "            })\n",
        "    \n",
        "    return losses.avg, top1.avg, top5.avg\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, initial_lr, lr_decay_epochs=[60, 120, 160]):\n",
        "    \"\"\"Decay learning rate by 10 at specified epochs\"\"\"\n",
        "    lr = initial_lr\n",
        "    for decay_epoch in lr_decay_epochs:\n",
        "        if epoch >= decay_epoch:\n",
        "            lr *= 0.1\n",
        "    \n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    \n",
        "    return lr\n",
        "\n",
        "print(\"Training and evaluation functions defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Initialize Model and Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create checkpoint directory\n",
        "os.makedirs(config['save_dir'], exist_ok=True)\n",
        "print(f\"Checkpoint directory created: {config['save_dir']}\")\n",
        "\n",
        "# Initialize model, optimizer, and loss\n",
        "model = ResNet18(num_classes=config['num_classes']).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'], weight_decay=config['weight_decay'])\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.1)\n",
        "\n",
        "print(f\"Model initialized on {DEVICE}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Initial learning rate: {config['learning_rate']}\")\n",
        "print(f\"Optimizer: SGD with momentum {config['momentum']}\")\n",
        "print(f\"Weight decay: {config['weight_decay']}\")\n",
        "print(f\"Blur curriculum: {config['BLUR']['EPOCHS']} epochs with blur, then sharp images\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Initialize Weights & Biases (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.login(key=os.environ.get('WANDB_API_KEY')) # API Key is in your wandb account, under settings (wandb.ai/settings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    name = \"ResNet18 CIFAR10 Blur Curriculum\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"cifar10-blur-curriculum\", ### Project should be created in your wandb account\n",
        "    config = config ### Wandb Config for your run\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Loop with Blur Curriculum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for existing checkpoint and resume if found\n",
        "last_path = os.path.join(config['save_dir'], 'last.pth')\n",
        "best_path = os.path.join(config['save_dir'], 'best_model.pth')\n",
        "\n",
        "start_epoch = 0\n",
        "best_acc1 = 0.0\n",
        "best_epoch = 0\n",
        "\n",
        "if os.path.exists(last_path):\n",
        "    print(f\"Found checkpoint at {last_path}. Resuming...\")\n",
        "    try:\n",
        "        ckpt = torch.load(last_path, map_location=DEVICE)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
        "        start_epoch = int(ckpt.get('epoch', 0)) + 1\n",
        "        best_acc1 = float(ckpt.get('best_acc1', 0.0))\n",
        "        print(f\"Resumed from epoch {start_epoch} with best_acc1={best_acc1:.2f}%\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading checkpoint: {e}\")\n",
        "        print(\"Starting fresh training...\")\n",
        "        start_epoch = 0\n",
        "        best_acc1 = 0.0\n",
        "else:\n",
        "    print(\"No existing checkpoint. Starting fresh.\")\n",
        "\n",
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc1': [],\n",
        "    'train_acc5': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc1': [],\n",
        "    'val_acc5': [],\n",
        "    'learning_rate': [],\n",
        "    'curriculum_type': []  # Track whether blur or sharp was used\n",
        "}\n",
        "\n",
        "print(f\"Starting training for {config['num_epochs']} epochs...\")\n",
        "print(f\"Blur curriculum: First {config['BLUR']['EPOCHS']} epochs with blur, then sharp images\")\n",
        "print(f\"Training on {len(train_dataset)} samples\")\n",
        "print(f\"Validation on {len(val_dataset)} samples\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(start_epoch, config['num_epochs']):\n",
        "    print(f\"\\nEpoch {epoch+1}/{config['num_epochs']}\")\n",
        "    \n",
        "    # Determine curriculum type\n",
        "    curriculum_type = \"Blur\" if epoch < config['BLUR']['EPOCHS'] else \"Sharp\"\n",
        "    print(f\"Curriculum: {curriculum_type} images\")\n",
        "    \n",
        "    # Adjust learning rate\n",
        "    current_lr = adjust_learning_rate(optimizer, epoch, config['learning_rate'])\n",
        "    \n",
        "    # Train for one epoch with curriculum\n",
        "    train_loss, train_acc1, train_acc5 = train_epoch_with_curriculum(\n",
        "        model, train_loader, gaussian_loader, criterion, optimizer, DEVICE, epoch, config\n",
        "    )\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc1, val_acc5 = validate(model, val_loader, criterion, DEVICE)\n",
        "    \n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "    \n",
        "    # Save 'last' checkpoint every epoch\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'best_acc1': best_acc1,\n",
        "        'val_acc1': val_acc1,\n",
        "        'val_acc5': val_acc5,\n",
        "    }, last_path)\n",
        "    \n",
        "    # Track best and save best model\n",
        "    if val_acc1 > best_acc1:\n",
        "        best_acc1 = val_acc1\n",
        "        best_epoch = epoch + 1\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'best_acc1': best_acc1,\n",
        "            'val_acc1': val_acc1,\n",
        "            'val_acc5': val_acc5,\n",
        "        }, best_path)\n",
        "        print(f\"New best model saved! Acc@1: {best_acc1:.2f}%\")\n",
        "    \n",
        "    # Save metrics (convert tensors to floats for JSON serialization)\n",
        "    history['train_loss'].append(float(train_loss))\n",
        "    history['train_acc1'].append(float(train_acc1))\n",
        "    history['train_acc5'].append(float(train_acc5))\n",
        "    history['val_loss'].append(float(val_loss))\n",
        "    history['val_acc1'].append(float(val_acc1))\n",
        "    history['val_acc5'].append(float(val_acc5))\n",
        "    history['learning_rate'].append(float(current_lr))\n",
        "    history['curriculum_type'].append(curriculum_type)\n",
        "    \n",
        "    # Log to wandb\n",
        "    if run is not None:\n",
        "        run.log({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'train_acc1': train_acc1,\n",
        "            'train_acc5': train_acc5,\n",
        "            'val_loss': val_loss,\n",
        "            'val_acc1': val_acc1,\n",
        "            'val_acc5': val_acc5,\n",
        "            'learning_rate': current_lr,\n",
        "            'curriculum_type': curriculum_type\n",
        "        })\n",
        "    \n",
        "    # Print epoch results\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc@1: {train_acc1:.2f}% | Train Acc@5: {train_acc5:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Acc@1: {val_acc1:.2f}% | Val Acc@5: {val_acc5:.2f}%\")\n",
        "    print(f\"Learning Rate: {current_lr:.6f}\")\n",
        "    print(f\"Curriculum: {curriculum_type}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Best validation accuracy: {best_acc1:.2f}% (Epoch {best_epoch})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Final Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model and history\n",
        "final_checkpoint = {\n",
        "    'epoch': config['num_epochs'],\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'best_acc1': best_acc1,\n",
        "    'best_epoch': best_epoch,\n",
        "    'history': history,\n",
        "    'config': config\n",
        "}\n",
        "\n",
        "torch.save(final_checkpoint, os.path.join(config['save_dir'], 'final_model.pth'))\n",
        "\n",
        "# Save training history as JSON (convert any remaining tensors to floats)\n",
        "def convert_to_serializable(obj):\n",
        "    \"\"\"Convert tensors and other non-serializable objects to Python types\"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_to_serializable(item) for item in obj]\n",
        "    elif hasattr(obj, 'item'):  # PyTorch tensor\n",
        "        return obj.item()\n",
        "    elif hasattr(obj, 'tolist'):  # NumPy array\n",
        "        return obj.tolist()\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# Convert history to JSON-serializable format\n",
        "serializable_history = convert_to_serializable(history)\n",
        "\n",
        "with open(os.path.join(config['save_dir'], 'training_history.json'), 'w') as f:\n",
        "    json.dump(serializable_history, f, indent=2)\n",
        "\n",
        "print(f\"Final results saved to {config['save_dir']}\")\n",
        "print(f\"Best model: {os.path.join(config['save_dir'], 'best_model.pth')}\")\n",
        "print(f\"Final model: {os.path.join(config['save_dir'], 'final_model.pth')}\")\n",
        "print(f\"Training history: {os.path.join(config['save_dir'], 'training_history.json')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_history(history, save_path=None):\n",
        "    \"\"\"Plot training and validation metrics with curriculum information\"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    \n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    \n",
        "    # Plot loss\n",
        "    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "    axes[0, 0].axvline(x=config['BLUR']['EPOCHS'], color='g', linestyle='--', alpha=0.7, label='Blur→Sharp Transition')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].set_title('Training and Validation Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot accuracy (Top-1)\n",
        "    axes[0, 1].plot(epochs, history['train_acc1'], 'b-', label='Train Acc@1', linewidth=2)\n",
        "    axes[0, 1].plot(epochs, history['val_acc1'], 'r-', label='Val Acc@1', linewidth=2)\n",
        "    axes[0, 1].axvline(x=config['BLUR']['EPOCHS'], color='g', linestyle='--', alpha=0.7, label='Blur→Sharp Transition')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
        "    axes[0, 1].set_title('Training and Validation Accuracy (Top-1)')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot accuracy (Top-5)\n",
        "    axes[0, 2].plot(epochs, history['train_acc5'], 'b-', label='Train Acc@5', linewidth=2)\n",
        "    axes[0, 2].plot(epochs, history['val_acc5'], 'r-', label='Val Acc@5', linewidth=2)\n",
        "    axes[0, 2].axvline(x=config['BLUR']['EPOCHS'], color='g', linestyle='--', alpha=0.7, label='Blur→Sharp Transition')\n",
        "    axes[0, 2].set_xlabel('Epoch')\n",
        "    axes[0, 2].set_ylabel('Accuracy (%)')\n",
        "    axes[0, 2].set_title('Training and Validation Accuracy (Top-5)')\n",
        "    axes[0, 2].legend()\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot learning rate\n",
        "    axes[1, 0].plot(epochs, history['learning_rate'], 'g-', linewidth=2)\n",
        "    axes[1, 0].axvline(x=config['BLUR']['EPOCHS'], color='g', linestyle='--', alpha=0.7, label='Blur→Sharp Transition')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Learning Rate')\n",
        "    axes[1, 0].set_title('Learning Rate Schedule')\n",
        "    axes[1, 0].set_yscale('log')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot curriculum type\n",
        "    curriculum_colors = ['red' if ct == 'Blur' else 'blue' for ct in history['curriculum_type']]\n",
        "    axes[1, 1].scatter(epochs, [1 if ct == 'Blur' else 0 for ct in history['curriculum_type']], \n",
        "                      c=curriculum_colors, alpha=0.7, s=20)\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Curriculum Type')\n",
        "    axes[1, 1].set_title('Curriculum Learning Schedule')\n",
        "    axes[1, 1].set_yticks([0, 1])\n",
        "    axes[1, 1].set_yticklabels(['Sharp', 'Blur'])\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot validation accuracy comparison\n",
        "    blur_epochs = [i+1 for i, ct in enumerate(history['curriculum_type']) if ct == 'Blur']\n",
        "    sharp_epochs = [i+1 for i, ct in enumerate(history['curriculum_type']) if ct == 'Sharp']\n",
        "    blur_acc = [history['val_acc1'][i-1] for i in blur_epochs]\n",
        "    sharp_acc = [history['val_acc1'][i-1] for i in sharp_epochs]\n",
        "    \n",
        "    if blur_epochs:\n",
        "        axes[1, 2].plot(blur_epochs, blur_acc, 'ro-', label='Blur Curriculum', linewidth=2, markersize=4)\n",
        "    if sharp_epochs:\n",
        "        axes[1, 2].plot(sharp_epochs, sharp_acc, 'bo-', label='Sharp Images', linewidth=2, markersize=4)\n",
        "    axes[1, 2].set_xlabel('Epoch')\n",
        "    axes[1, 2].set_ylabel('Validation Accuracy (%)')\n",
        "    axes[1, 2].set_title('Validation Accuracy by Curriculum Phase')\n",
        "    axes[1, 2].legend()\n",
        "    axes[1, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history(history, os.path.join(config['save_dir'], 'training_plots.png'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Final Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"FINAL TRAINING RESULTS - BLUR CURRICULUM\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Dataset: CIFAR-10\")\n",
        "print(f\"Model: ResNet-18\")\n",
        "print(f\"Training Strategy: Gaussian Blur Curriculum Learning\")\n",
        "print(f\"Blur Curriculum: First {config['BLUR']['EPOCHS']} epochs with blur, then sharp images\")\n",
        "print(f\"Blur Parameters: Kernel size {config['BLUR']['KERNEL_SIZE']}, Sigma {config['BLUR']['SIGMA']}\")\n",
        "print(f\"Total Epochs: {config['num_epochs']}\")\n",
        "print(f\"Batch Size: {config['batch_size']}\")\n",
        "print(f\"Initial Learning Rate: {config['learning_rate']}\")\n",
        "print(f\"Weight Decay: {config['weight_decay']}\")\n",
        "print(f\"Momentum: {config['momentum']}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"Best Validation Accuracy: {best_acc1:.2f}% (Epoch {best_epoch})\")\n",
        "print(f\"Final Training Accuracy: {history['train_acc1'][-1]:.2f}%\")\n",
        "print(f\"Final Validation Accuracy: {history['val_acc1'][-1]:.2f}%\")\n",
        "print(f\"Final Training Top-5 Accuracy: {history['train_acc5'][-1]:.2f}%\")\n",
        "print(f\"Final Validation Top-5 Accuracy: {history['val_acc5'][-1]:.2f}%\")\n",
        "print(\"-\"*70)\n",
        "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Model Size: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Close wandb run if active\n",
        "if run is not None:\n",
        "    run.finish()\n",
        "    print(\"Wandb run finished.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Load and Test Best Model (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model for testing\n",
        "best_model_path = os.path.join(config['save_dir'], 'best_model.pth')\n",
        "if os.path.exists(best_model_path):\n",
        "    print(\"Loading best model for final evaluation...\")\n",
        "    \n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(best_model_path)\n",
        "    \n",
        "    # Create model and load weights\n",
        "    best_model = ResNet18(num_classes=config['num_classes']).to(DEVICE)\n",
        "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    \n",
        "    # Evaluate on validation set\n",
        "    print(\"Evaluating best model on validation set...\")\n",
        "    val_loss, val_acc1, val_acc5 = validate(best_model, val_loader, criterion, DEVICE)\n",
        "    \n",
        "    print(f\"Best Model Results:\")\n",
        "    print(f\"  Validation Loss: {val_loss:.4f}\")\n",
        "    print(f\"  Validation Acc@1: {val_acc1:.2f}%\")\n",
        "    print(f\"  Validation Acc@5: {val_acc5:.2f}%\")\n",
        "    print(f\"  Best epoch: {checkpoint['epoch']}\")\n",
        "else:\n",
        "    print(\"Best model checkpoint not found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Cleanup and Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up GPU memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Training completed successfully!\")\n",
        "print(f\"\\nCheckpoint directory: {config['save_dir']}\")\n",
        "print(\"Files created:\")\n",
        "for file in os.listdir(config['save_dir']):\n",
        "    file_path = os.path.join(config['save_dir'], file)\n",
        "    if os.path.isfile(file_path):\n",
        "        size = os.path.getsize(file_path) / (1024*1024)  # Size in MB\n",
        "        print(f\"  - {file} ({size:.1f} MB)\")\n",
        "\n",
        "print(\"\\nThis blur curriculum implementation can be compared with the baseline ResNet-18.\")\n",
        "print(\"Key differences:\")\n",
        "print(f\"- First {config['BLUR']['EPOCHS']} epochs use Gaussian blur (kernel={config['BLUR']['KERNEL_SIZE']}, sigma={config['BLUR']['SIGMA']})\")\n",
        "print(\"- Remaining epochs use sharp images\")\n",
        "print(\"- Same model architecture and hyperparameters as baseline\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
