{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ResNet-18 Baseline Training on ImageNet-100\n",
        "\n",
        "This notebook implements a baseline ResNet-18 model trained on ImageNet-100 using conventional training methods (no curriculum learning).\n",
        "\n",
        "## Dataset: ImageNet-100\n",
        "- 100 classes subset of ImageNet\n",
        "- 224x224 input images\n",
        "- Standard data augmentation\n",
        "\n",
        "## Model: ResNet-18\n",
        "- Standard ResNet-18 architecture\n",
        "- ~11.7M parameters\n",
        "- Conventional training (no curriculum learning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install torchsummary torchvision tqdm wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import os\n",
        "import gc\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "config = {\n",
        "    'dataset_path': 'IDL_data/ImageNet100_224',\n",
        "    'batch_size': 128,\n",
        "    'num_epochs': 100,\n",
        "    'learning_rate': 0.01,\n",
        "    'weight_decay': 1e-4,\n",
        "    'momentum': 0.9,\n",
        "    'num_classes': 100,\n",
        "    'image_size': 224,\n",
        "    'num_workers': 8,\n",
        "    'save_dir': './checkpoints_ImageNet',\n",
        "    'use_wandb': True,\n",
        "    'project_name': 'resnet18-imagenet100-baseline',\n",
        "    'run_name': 'conventional-training'\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ResNet-18 Model Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # First conv layer for ImageNet (224x224 input)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        # ResNet layers\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        \n",
        "        # Global average pooling and classifier\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.maxpool(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(num_classes=100):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "\n",
        "# Test model creation\n",
        "model = ResNet18(num_classes=config['num_classes'])\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"ResNet-18 created with {total_params:,} parameters\")\n",
        "print(f\"Model size: {total_params/1e6:.2f}M parameters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update model to return dictionary format for experiment compatibility\n",
        "class ResNetExperiment(ResNet):\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.maxpool(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return {\"out\": out}\n",
        "\n",
        "# Create model with experiment format\n",
        "model = ResNetExperiment(BasicBlock, [2, 2, 2, 2], num_classes=config['num_classes'])\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"ResNet-18 (Experiment format) created with {total_params:,} parameters\")\n",
        "print(f\"Model size: {total_params/1e6:.2f}M parameters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_imagenet100_loaders(data_path, batch_size=128, num_workers=5):\n",
        "    \"\"\"Load ImageNet-100 dataset with standard augmentation\"\"\"\n",
        "    \n",
        "    # ImageNet normalization\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                   std=[0.229, 0.224, 0.225])\n",
        "    \n",
        "    # Training transforms with augmentation\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "    \n",
        "    # Validation transforms (no augmentation)\n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "    \n",
        "    # Load datasets\n",
        "    train_dir = Path(data_path) / 'train'\n",
        "    val_dir = Path(data_path) / 'val'\n",
        "    \n",
        "    train_dataset = datasets.ImageFolder(train_dir, transform=transform_train)\n",
        "    val_dataset = datasets.ImageFolder(val_dir, transform=transform_val)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=True,\n",
        "        num_workers=num_workers, \n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=False,\n",
        "        num_workers=num_workers, \n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    return train_loader, val_loader, train_dataset, val_dataset\n",
        "\n",
        "# Load the dataset\n",
        "print(\"Loading ImageNet-100 dataset...\")\n",
        "train_loader, val_loader, train_dataset, val_dataset = get_imagenet100_loaders(\n",
        "    data_path=config['dataset_path'],\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers']\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
        "print(f\"Classes: {train_dataset.classes[:10]}...\" if len(train_dataset.classes) > 10 else f\"Classes: {train_dataset.classes}\")\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    \n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Update metrics\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), data.size(0))\n",
        "        top1.update(acc1[0], data.size(0))\n",
        "        top5.update(acc5[0], data.size(0))\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'Loss': f'{losses.avg:.4f}',\n",
        "            'Acc@1': f'{top1.avg:.2f}%',\n",
        "            'Acc@5': f'{top5.avg:.2f}%'\n",
        "        })\n",
        "    \n",
        "    return losses.avg, top1.avg, top5.avg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training and Evaluation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model saving/loading functions (simplified for baseline)\n",
        "print(\"Model saving/loading functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "# Old train_epoch function removed - using experiment-style function from cell 11\n",
        "\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(val_loader, desc='Validation')\n",
        "        for data, target in pbar:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            # Update metrics\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), data.size(0))\n",
        "            top1.update(acc1[0], data.size(0))\n",
        "            top5.update(acc5[0], data.size(0))\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'Loss': f'{losses.avg:.4f}',\n",
        "                'Acc@1': f'{top1.avg:.2f}%',\n",
        "                'Acc@5': f'{top5.avg:.2f}%'\n",
        "            })\n",
        "    \n",
        "    return losses.avg, top1.avg, top5.avg\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, initial_lr, lr_decay_epochs=[30, 60, 90]):\n",
        "    \"\"\"Decay learning rate by 10 at specified epochs\"\"\"\n",
        "    lr = initial_lr\n",
        "    for decay_epoch in lr_decay_epochs:\n",
        "        if epoch >= decay_epoch:\n",
        "            lr *= 0.1\n",
        "    \n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    \n",
        "    return lr\n",
        "\n",
        "print(\"Training and evaluation functions defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Initialize Model, Optimizer, and Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = ResNet18(num_classes=config['num_classes']).to(DEVICE)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (SGD with momentum)\n",
        "optimizer = optim.SGD(\n",
        "    model.parameters(), \n",
        "    lr=config['learning_rate'], \n",
        "    momentum=config['momentum'], \n",
        "    weight_decay=config['weight_decay']\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "print(f\"Model initialized on {DEVICE}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "print(f\"Initial learning rate: {config['learning_rate']}\")\n",
        "print(f\"Optimizer: SGD with momentum {config['momentum']}\")\n",
        "print(f\"Weight decay: {config['weight_decay']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Initialize Weights & Biases (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.login(key=os.environ.get('WANDB_API_KEY')) # API Key is in your wandb account, under settings (wandb.ai/settings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    name = \"ResNet18 ImageNet100 Baseline\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"plain training\", ### Project should be created in your wandb account\n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create Checkpoint Directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create checkpoint directory\n",
        "os.makedirs(config['save_dir'], exist_ok=True)\n",
        "print(f\"Checkpoint directory created: {config['save_dir']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Main Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc1': [],\n",
        "    'train_acc5': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc1': [],\n",
        "    'val_acc5': [],\n",
        "    'learning_rate': []\n",
        "}\n",
        "\n",
        "best_acc1 = 0.0\n",
        "best_epoch = 0\n",
        "\n",
        "print(f\"Starting training for {config['num_epochs']} epochs...\")\n",
        "print(f\"Training on {len(train_dataset)} samples\")\n",
        "print(f\"Validation on {len(val_dataset)} samples\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(config['num_epochs']):\n",
        "    print(f\"\\nEpoch {epoch+1}/{config['num_epochs']}\")\n",
        "    \n",
        "    # Adjust learning rate\n",
        "    current_lr = adjust_learning_rate(optimizer, epoch, config['learning_rate'])\n",
        "    \n",
        "    # Train for one epoch\n",
        "    train_loss, train_acc1, train_acc5 = train_epoch(\n",
        "        model, train_loader, criterion, optimizer, DEVICE, epoch+1\n",
        "    )\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc1, val_acc5 = validate(model, val_loader, criterion, DEVICE)\n",
        "    \n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "    \n",
        "    # Save metrics\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc1'].append(train_acc1)\n",
        "    history['train_acc5'].append(train_acc5)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc1'].append(val_acc1)\n",
        "    history['val_acc5'].append(val_acc5)\n",
        "    history['learning_rate'].append(current_lr)\n",
        "    \n",
        "    # Log to wandb\n",
        "    if run is not None:\n",
        "        run.log({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'train_acc1': train_acc1,\n",
        "            'train_acc5': train_acc5,\n",
        "            'val_loss': val_loss,\n",
        "            'val_acc1': val_acc1,\n",
        "            'val_acc5': val_acc5,\n",
        "            'learning_rate': current_lr\n",
        "        })\n",
        "    \n",
        "    # Print epoch results\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc@1: {train_acc1:.2f}% | Train Acc@5: {train_acc5:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Acc@1: {val_acc1:.2f}% | Val Acc@5: {val_acc5:.2f}%\")\n",
        "    print(f\"Learning Rate: {current_lr:.6f}\")\n",
        "    \n",
        "    # Save best model\n",
        "    if val_acc1 > best_acc1:\n",
        "        best_acc1 = val_acc1\n",
        "        best_epoch = epoch + 1\n",
        "        \n",
        "        # Save checkpoint\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'best_acc1': best_acc1,\n",
        "            'val_acc1': val_acc1,\n",
        "            'val_acc5': val_acc5,\n",
        "            'config': config\n",
        "        }\n",
        "        \n",
        "        torch.save(checkpoint, os.path.join(config['save_dir'], 'best_model.pth'))\n",
        "        print(f\"New best model saved! Val Acc@1: {val_acc1:.2f}%\")\n",
        "    \n",
        "    # Save regular checkpoint every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'val_acc1': val_acc1,\n",
        "            'val_acc5': val_acc5,\n",
        "            'config': config\n",
        "        }\n",
        "        torch.save(checkpoint, os.path.join(config['save_dir'], f'checkpoint_epoch_{epoch+1}.pth'))\n",
        "    \n",
        "    print(\"-\" * 60)\n",
        "\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Best validation accuracy: {best_acc1:.2f}% at epoch {best_epoch}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Training Loop (Resumable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for existing checkpoint and resume if found\n",
        "last_path = os.path.join(config['save_dir'], 'last.pth')\n",
        "best_path = os.path.join(config['save_dir'], 'best_model.pth')\n",
        "\n",
        "start_epoch = 0\n",
        "best_acc1 = 0.0\n",
        "best_epoch = 0\n",
        "\n",
        "if os.path.exists(last_path):\n",
        "    print(f\"Found checkpoint at {last_path}. Resuming...\")\n",
        "    try:\n",
        "        ckpt = torch.load(last_path, map_location=DEVICE)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
        "        start_epoch = int(ckpt.get('epoch', 0)) + 1\n",
        "        best_acc1 = float(ckpt.get('best_acc1', 0.0))\n",
        "        print(f\"Resumed from epoch {start_epoch} with best_acc1={best_acc1:.2f}%\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading checkpoint: {e}\")\n",
        "        print(\"Starting fresh training...\")\n",
        "        start_epoch = 0\n",
        "        best_acc1 = 0.0\n",
        "else:\n",
        "    print(\"No existing checkpoint. Starting fresh.\")\n",
        "\n",
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc1': [],\n",
        "    'train_acc5': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc1': [],\n",
        "    'val_acc5': [],\n",
        "    'learning_rate': []\n",
        "}\n",
        "\n",
        "print(f\"Starting training for {config['num_epochs']} epochs...\")\n",
        "print(f\"Training on {len(train_dataset)} samples\")\n",
        "print(f\"Validation on {len(val_dataset)} samples\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(start_epoch, config['num_epochs']):\n",
        "    print(f\"\\nEpoch {epoch+1}/{config['num_epochs']}\")\n",
        "    \n",
        "    # Adjust learning rate\n",
        "    current_lr = adjust_learning_rate(optimizer, epoch, config['learning_rate'])\n",
        "    \n",
        "    # Train for one epoch\n",
        "    train_loss, train_acc1, train_acc5 = train_epoch(\n",
        "        model, train_loader, criterion, optimizer, DEVICE, epoch+1\n",
        "    )\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc1, val_acc5 = validate(model, val_loader, criterion, DEVICE)\n",
        "    \n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "    \n",
        "    # Save 'last' checkpoint every epoch\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'best_acc1': best_acc1,\n",
        "        'val_acc1': val_acc1,\n",
        "        'val_acc5': val_acc5,\n",
        "    }, last_path)\n",
        "    \n",
        "    # Track best and save best model\n",
        "    if val_acc1 > best_acc1:\n",
        "        best_acc1 = val_acc1\n",
        "        best_epoch = epoch + 1\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'best_acc1': best_acc1,\n",
        "            'val_acc1': val_acc1,\n",
        "            'val_acc5': val_acc5,\n",
        "        }, best_path)\n",
        "        print(f\"New best model saved! Acc@1: {best_acc1:.2f}%\")\n",
        "    \n",
        "    # Save metrics (convert tensors to floats for JSON serialization)\n",
        "    history['train_loss'].append(float(train_loss))\n",
        "    history['train_acc1'].append(float(train_acc1))\n",
        "    history['train_acc5'].append(float(train_acc5))\n",
        "    history['val_loss'].append(float(val_loss))\n",
        "    history['val_acc1'].append(float(val_acc1))\n",
        "    history['val_acc5'].append(float(val_acc5))\n",
        "    history['learning_rate'].append(float(current_lr))\n",
        "    \n",
        "    # Log to wandb\n",
        "    if run is not None:\n",
        "        run.log({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'train_acc1': train_acc1,\n",
        "            'train_acc5': train_acc5,\n",
        "            'val_loss': val_loss,\n",
        "            'val_acc1': val_acc1,\n",
        "            'val_acc5': val_acc5,\n",
        "            'learning_rate': current_lr\n",
        "        })\n",
        "    \n",
        "    # Print epoch results\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc@1: {train_acc1:.2f}% | Train Acc@5: {train_acc5:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Acc@1: {val_acc1:.2f}% | Val Acc@5: {val_acc5:.2f}%\")\n",
        "    print(f\"Learning Rate: {current_lr:.6f}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Best validation accuracy: {best_acc1:.2f}% (Epoch {best_epoch})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Final Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model and history\n",
        "final_checkpoint = {\n",
        "    'epoch': config['num_epochs'],\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'best_acc1': best_acc1,\n",
        "    'best_epoch': best_epoch,\n",
        "    'history': history,\n",
        "    'config': config\n",
        "}\n",
        "\n",
        "torch.save(final_checkpoint, os.path.join(config['save_dir'], 'final_model.pth'))\n",
        "\n",
        "# Save training history as JSON (convert any remaining tensors to floats)\n",
        "def convert_to_serializable(obj):\n",
        "    \"\"\"Convert tensors and other non-serializable objects to Python types\"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_to_serializable(item) for item in obj]\n",
        "    elif hasattr(obj, 'item'):  # PyTorch tensor\n",
        "        return obj.item()\n",
        "    elif hasattr(obj, 'tolist'):  # NumPy array\n",
        "        return obj.tolist()\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# Convert history to JSON-serializable format\n",
        "serializable_history = convert_to_serializable(history)\n",
        "\n",
        "with open(os.path.join(config['save_dir'], 'training_history.json'), 'w') as f:\n",
        "    json.dump(serializable_history, f, indent=2)\n",
        "\n",
        "print(f\"Final results saved to {config['save_dir']}\")\n",
        "print(f\"Best model: {os.path.join(config['save_dir'], 'best_model.pth')}\")\n",
        "print(f\"Final model: {os.path.join(config['save_dir'], 'final_model.pth')}\")\n",
        "print(f\"Training history: {os.path.join(config['save_dir'], 'training_history.json')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Final Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"FINAL TRAINING RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Dataset: ImageNet-100\")\n",
        "print(f\"Model: ResNet-18\")\n",
        "print(f\"Training Strategy: Conventional (No Curriculum Learning)\")\n",
        "print(f\"Total Epochs: {config['num_epochs']}\")\n",
        "print(f\"Batch Size: {config['batch_size']}\")\n",
        "print(f\"Initial Learning Rate: {config['learning_rate']}\")\n",
        "print(f\"Weight Decay: {config['weight_decay']}\")\n",
        "print(f\"Momentum: {config['momentum']}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"Best Validation Accuracy: {best_acc1:.2f}% (Epoch {best_epoch})\")\n",
        "print(f\"Final Training Accuracy: {history['train_acc1'][-1]:.2f}%\")\n",
        "print(f\"Final Validation Accuracy: {history['val_acc1'][-1]:.2f}%\")\n",
        "print(f\"Final Training Top-5 Accuracy: {history['train_acc5'][-1]:.2f}%\")\n",
        "print(f\"Final Validation Top-5 Accuracy: {history['val_acc5'][-1]:.2f}%\")\n",
        "print(\"-\"*70)\n",
        "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Model Size: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Close wandb run if active\n",
        "if run is not None:\n",
        "    run.finish()\n",
        "    print(\"Wandb run finished.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Load and Test Best Model (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model for testing\n",
        "best_model_path = os.path.join(config['save_dir'], 'best_model.pth')\n",
        "if os.path.exists(best_model_path):\n",
        "    print(\"Loading best model for final evaluation...\")\n",
        "    \n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(best_model_path)\n",
        "    \n",
        "    # Create model and load weights\n",
        "    best_model = ResNet18(num_classes=config['num_classes']).to(DEVICE)\n",
        "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    \n",
        "    # Evaluate on validation set\n",
        "    print(\"Evaluating best model on validation set...\")\n",
        "    val_loss, val_acc1, val_acc5 = validate(best_model, val_loader, criterion, DEVICE)\n",
        "    \n",
        "    print(f\"Best Model Results:\")\n",
        "    print(f\"  Validation Loss: {val_loss:.4f}\")\n",
        "    print(f\"  Validation Acc@1: {val_acc1:.2f}%\")\n",
        "    print(f\"  Validation Acc@5: {val_acc5:.2f}%\")\n",
        "    print(f\"  Best epoch: {checkpoint['epoch']}\")\n",
        "else:\n",
        "    print(\"Best model checkpoint not found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Cleanup and Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up GPU memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Training completed successfully!\")\n",
        "print(f\"\\nCheckpoint directory: {config['save_dir']}\")\n",
        "print(\"Files created:\")\n",
        "for file in os.listdir(config['save_dir']):\n",
        "    file_path = os.path.join(config['save_dir'], file)\n",
        "    if os.path.isfile(file_path):\n",
        "        size = os.path.getsize(file_path) / (1024*1024)  # Size in MB\n",
        "        print(f\"  - {file} ({size:.1f} MB)\")\n",
        "\n",
        "print(\"\\nThis baseline can be used for comparison with curriculum learning approaches.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
