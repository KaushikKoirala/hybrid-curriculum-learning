{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ResNet-18 with Gaussian Blur Curriculum Learning on ImageNet-100\n",
        "\n",
        "This notebook implements ResNet-18 with Gaussian blur curriculum learning on ImageNet-100, based on the approach from the CVT_13_blur notebook.\n",
        "\n",
        "## Curriculum Learning Strategy\n",
        "- **Early epochs**: Train with Gaussian blur applied to images\n",
        "- **Later epochs**: Train with original sharp images\n",
        "- **Blur parameters**: Kernel size 7, sigma 1.0, applied for first 20 epochs\n",
        "\n",
        "## Dataset: ImageNet-100\n",
        "- 100 classes subset of ImageNet\n",
        "- 224x224 input images\n",
        "- Standard data augmentation with blur curriculum\n",
        "\n",
        "## Model: ResNet-18\n",
        "- Standard ResNet-18 architecture\n",
        "- ~11.7M parameters\n",
        "- Curriculum learning with Gaussian blur\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install torchsummary torchvision tqdm wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip freeze > requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import os\n",
        "import gc\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration with blur curriculum\n",
        "config = {\n",
        "    'dataset_path': 'IDL_data/ImageNet100_224',\n",
        "    'batch_size': 128,\n",
        "    'num_epochs': 100,\n",
        "    'learning_rate': 0.01,\n",
        "    'weight_decay': 1e-4,\n",
        "    'momentum': 0.9,\n",
        "    'num_classes': 100,\n",
        "    'image_size': 224,\n",
        "    'num_workers': 8,\n",
        "    'save_dir': './checkpoints_blur_curriculum',\n",
        "    'use_wandb': True,\n",
        "    'project_name': 'resnet18-imagenet100-blur-curriculum',\n",
        "    'run_name': 'blur-curriculum-training',\n",
        "    'BLUR': {\n",
        "        'KERNEL_SIZE': 7,\n",
        "        'SIGMA': 1.0,\n",
        "        'EPOCHS': 20  # Number of epochs to use blur\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in config.items():\n",
        "    if isinstance(value, dict):\n",
        "        print(f\"  {key}:\")\n",
        "        for sub_key, sub_value in value.items():\n",
        "            print(f\"    {sub_key}: {sub_value}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ResNet-18 Model Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # First conv layer for ImageNet (224x224 input)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        # ResNet layers\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        \n",
        "        # Global average pooling and classifier\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.maxpool(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(num_classes=100):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "\n",
        "# Test model creation\n",
        "model = ResNet18(num_classes=config['num_classes'])\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"ResNet-18 created with {total_params:,} parameters\")\n",
        "print(f\"Model size: {total_params/1e6:.2f}M parameters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update model to return dictionary format for experiment compatibility\n",
        "class ResNetExperiment(ResNet):\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.maxpool(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return {\"out\": out}\n",
        "\n",
        "# Create model with experiment format\n",
        "model = ResNetExperiment(BasicBlock, [2, 2, 2, 2], num_classes=config['num_classes'])\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"ResNet-18 (Experiment format) created with {total_params:,} parameters\")\n",
        "print(f\"Model size: {total_params/1e6:.2f}M parameters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Loading and Preprocessing with Blur Curriculum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_imagenet100_loaders_with_blur(data_path, batch_size=128, num_workers=5, blur_config=None):\n",
        "    \"\"\"Load ImageNet-100 dataset with blur curriculum\"\"\"\n",
        "    \n",
        "    # ImageNet normalization\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                   std=[0.229, 0.224, 0.225])\n",
        "    \n",
        "    # Training transforms with augmentation (sharp images)\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "    \n",
        "    # Training transforms with Gaussian blur for curriculum\n",
        "    transform_train_blur = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.GaussianBlur(\n",
        "            kernel_size=blur_config['KERNEL_SIZE'] if blur_config else 7,\n",
        "            sigma=blur_config['SIGMA'] if blur_config else 1.0\n",
        "        ),\n",
        "        normalize,\n",
        "    ])\n",
        "    \n",
        "    # Validation transforms (no augmentation)\n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "    \n",
        "    # Load datasets\n",
        "    train_dir = Path(data_path) / 'train'\n",
        "    val_dir = Path(data_path) / 'val'\n",
        "    \n",
        "    train_dataset = datasets.ImageFolder(train_dir, transform=transform_train)\n",
        "    train_dataset_blur = datasets.ImageFolder(train_dir, transform=transform_train_blur)\n",
        "    val_dataset = datasets.ImageFolder(val_dir, transform=transform_val)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=True,\n",
        "        num_workers=num_workers, \n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    \n",
        "    train_loader_blur = DataLoader(\n",
        "        train_dataset_blur, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=True,\n",
        "        num_workers=num_workers, \n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=False,\n",
        "        num_workers=num_workers, \n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    return train_loader, train_loader_blur, val_loader, train_dataset, val_dataset\n",
        "\n",
        "# Load the dataset\n",
        "print(\"Loading ImageNet-100 dataset with blur curriculum...\")\n",
        "train_loader, train_loader_blur, val_loader, train_dataset, val_dataset = get_imagenet100_loaders_with_blur(\n",
        "    data_path=config['dataset_path'],\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=config['num_workers'],\n",
        "    blur_config=config['BLUR']\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
        "print(f\"Classes: {train_dataset.classes[:10]}...\")\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Training blur batches: {len(train_loader_blur)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "print(f\"Blur curriculum: First {config['BLUR']['EPOCHS']} epochs with blur, then sharp images\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch_with_curriculum(model, train_loader, train_loader_blur, criterion, optimizer, device, epoch, config):\n",
        "    \"\"\"Train for one epoch with blur curriculum\"\"\"\n",
        "    model.train()\n",
        "    \n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    \n",
        "    # Choose loader based on curriculum\n",
        "    if epoch < config['BLUR']['EPOCHS']:\n",
        "        loader = train_loader_blur\n",
        "        curriculum_type = \"Blur\"\n",
        "    else:\n",
        "        loader = train_loader\n",
        "        curriculum_type = \"Sharp\"\n",
        "    \n",
        "    pbar = tqdm(loader, desc=f'Epoch {epoch+1} ({curriculum_type})')\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Update metrics\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), data.size(0))\n",
        "        top1.update(acc1[0], data.size(0))\n",
        "        top5.update(acc5[0], data.size(0))\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'Loss': f'{losses.avg:.4f}',\n",
        "            'Acc@1': f'{top1.avg:.2f}%',\n",
        "            'Acc@5': f'{top5.avg:.2f}%',\n",
        "            'Type': curriculum_type\n",
        "        })\n",
        "    \n",
        "    return losses.avg, top1.avg, top5.avg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training and Evaluation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(val_loader, desc='Validation')\n",
        "        for data, target in pbar:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            # Update metrics\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), data.size(0))\n",
        "            top1.update(acc1[0], data.size(0))\n",
        "            top5.update(acc5[0], data.size(0))\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'Loss': f'{losses.avg:.4f}',\n",
        "                'Acc@1': f'{top1.avg:.2f}%',\n",
        "                'Acc@5': f'{top5.avg:.2f}%'\n",
        "            })\n",
        "    \n",
        "    return losses.avg, top1.avg, top5.avg\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, initial_lr, lr_decay_epochs=[30, 60, 90]):\n",
        "    \"\"\"Decay learning rate by 10 at specified epochs\"\"\"\n",
        "    lr = initial_lr\n",
        "    for decay_epoch in lr_decay_epochs:\n",
        "        if epoch >= decay_epoch:\n",
        "            lr *= 0.1\n",
        "    \n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    \n",
        "    return lr\n",
        "\n",
        "print(\"Training and evaluation functions defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Initialize Model, Optimizer, and Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = ResNet18(num_classes=config['num_classes']).to(DEVICE)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (SGD with momentum)\n",
        "optimizer = optim.SGD(\n",
        "    model.parameters(), \n",
        "    lr=config['learning_rate'], \n",
        "    momentum=config['momentum'], \n",
        "    weight_decay=config['weight_decay']\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "print(f\"Model initialized on {DEVICE}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "print(f\"Initial learning rate: {config['learning_rate']}\")\n",
        "print(f\"Optimizer: SGD with momentum {config['momentum']}\")\n",
        "print(f\"Weight decay: {config['weight_decay']}\")\n",
        "print(f\"Blur curriculum: {config['BLUR']['EPOCHS']} epochs with blur, then sharp images\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Initialize Weights & Biases (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.login(key=os.environ.get('WANDB_API_KEY')) # API Key is in your wandb account, under settings (wandb.ai/settings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    name = \"ResNet18 ImageNet100 Blur Curriculum\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"imagenet100-blur-curriculum\", ### Project should be created in your wandb account\n",
        "    config = config ### Wandb Config for your run\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create Checkpoint Directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create checkpoint directory\n",
        "os.makedirs(config['save_dir'], exist_ok=True)\n",
        "print(f\"Checkpoint directory created: {config['save_dir']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Main Training Loop with Blur Curriculum\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Training Loop (Resumable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for existing checkpoint and resume if found\n",
        "last_path = os.path.join(config['save_dir'], 'last.pth')\n",
        "best_path = os.path.join(config['save_dir'], 'best_model.pth')\n",
        "\n",
        "start_epoch = 0\n",
        "best_acc1 = 0.0\n",
        "best_epoch = 0\n",
        "\n",
        "if os.path.exists(last_path):\n",
        "    print(f\"Found checkpoint at {last_path}. Resuming...\")\n",
        "    try:\n",
        "        ckpt = torch.load(last_path, map_location=DEVICE)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
        "        start_epoch = int(ckpt.get('epoch', 0)) + 1\n",
        "        best_acc1 = float(ckpt.get('best_acc1', 0.0))\n",
        "        print(f\"Resumed from epoch {start_epoch} with best_acc1={best_acc1:.2f}%\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading checkpoint: {e}\")\n",
        "        print(\"Starting fresh training...\")\n",
        "        start_epoch = 0\n",
        "        best_acc1 = 0.0\n",
        "else:\n",
        "    print(\"No existing checkpoint. Starting fresh.\")\n",
        "\n",
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc1': [],\n",
        "    'train_acc5': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc1': [],\n",
        "    'val_acc5': [],\n",
        "    'learning_rate': [],\n",
        "    'curriculum_type': []  # Track whether blur or sharp was used\n",
        "}\n",
        "\n",
        "print(f\"Starting training for {config['num_epochs']} epochs...\")\n",
        "print(f\"Blur curriculum: First {config['BLUR']['EPOCHS']} epochs with blur, then sharp images\")\n",
        "print(f\"Training on {len(train_dataset)} samples\")\n",
        "print(f\"Validation on {len(val_dataset)} samples\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(start_epoch, config['num_epochs']):\n",
        "    print(f\"\\nEpoch {epoch+1}/{config['num_epochs']}\")\n",
        "    \n",
        "    # Determine curriculum type\n",
        "    curriculum_type = \"Blur\" if epoch < config['BLUR']['EPOCHS'] else \"Sharp\"\n",
        "    print(f\"Curriculum: {curriculum_type} images\")\n",
        "    \n",
        "    # Adjust learning rate\n",
        "    current_lr = adjust_learning_rate(optimizer, epoch, config['learning_rate'])\n",
        "    \n",
        "    # Train for one epoch with curriculum\n",
        "    train_loss, train_acc1, train_acc5 = train_epoch_with_curriculum(\n",
        "        model, train_loader, train_loader_blur, criterion, optimizer, DEVICE, epoch, config\n",
        "    )\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc1, val_acc5 = validate(model, val_loader, criterion, DEVICE)\n",
        "    \n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "    \n",
        "    # Save 'last' checkpoint every epoch\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'best_acc1': best_acc1,\n",
        "        'val_acc1': val_acc1,\n",
        "        'val_acc5': val_acc5,\n",
        "    }, last_path)\n",
        "    \n",
        "    # Track best and save best model\n",
        "    if val_acc1 > best_acc1:\n",
        "        best_acc1 = val_acc1\n",
        "        best_epoch = epoch + 1\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'best_acc1': best_acc1,\n",
        "            'val_acc1': val_acc1,\n",
        "            'val_acc5': val_acc5,\n",
        "        }, best_path)\n",
        "        print(f\"New best model saved! Acc@1: {best_acc1:.2f}%\")\n",
        "    \n",
        "    # Save metrics (convert tensors to floats for JSON serialization)\n",
        "    history['train_loss'].append(float(train_loss))\n",
        "    history['train_acc1'].append(float(train_acc1))\n",
        "    history['train_acc5'].append(float(train_acc5))\n",
        "    history['val_loss'].append(float(val_loss))\n",
        "    history['val_acc1'].append(float(val_acc1))\n",
        "    history['val_acc5'].append(float(val_acc5))\n",
        "    history['learning_rate'].append(float(current_lr))\n",
        "    history['curriculum_type'].append(curriculum_type)\n",
        "    \n",
        "    # Log to wandb\n",
        "    if run is not None:\n",
        "        run.log({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'train_acc1': train_acc1,\n",
        "            'train_acc5': train_acc5,\n",
        "            'val_loss': val_loss,\n",
        "            'val_acc1': val_acc1,\n",
        "            'val_acc5': val_acc5,\n",
        "            'learning_rate': current_lr,\n",
        "            'curriculum_type': curriculum_type\n",
        "        })\n",
        "    \n",
        "    # Print epoch results\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc@1: {train_acc1:.2f}% | Train Acc@5: {train_acc5:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Acc@1: {val_acc1:.2f}% | Val Acc@5: {val_acc5:.2f}%\")\n",
        "    print(f\"Learning Rate: {current_lr:.6f}\")\n",
        "    print(f\"Curriculum: {curriculum_type}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Best validation accuracy: {best_acc1:.2f}% (Epoch {best_epoch})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Final Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model and history\n",
        "final_checkpoint = {\n",
        "    'epoch': config['num_epochs'],\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'best_acc1': best_acc1,\n",
        "    'best_epoch': best_epoch,\n",
        "    'history': history,\n",
        "    'config': config\n",
        "}\n",
        "\n",
        "torch.save(final_checkpoint, os.path.join(config['save_dir'], 'final_model.pth'))\n",
        "\n",
        "# Save training history as JSON (convert any remaining tensors to floats)\n",
        "def convert_to_serializable(obj):\n",
        "    \"\"\"Convert tensors and other non-serializable objects to Python types\"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_to_serializable(item) for item in obj]\n",
        "    elif hasattr(obj, 'item'):  # PyTorch tensor\n",
        "        return obj.item()\n",
        "    elif hasattr(obj, 'tolist'):  # NumPy array\n",
        "        return obj.tolist()\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# Convert history to JSON-serializable format\n",
        "serializable_history = convert_to_serializable(history)\n",
        "\n",
        "with open(os.path.join(config['save_dir'], 'training_history.json'), 'w') as f:\n",
        "    json.dump(serializable_history, f, indent=2)\n",
        "\n",
        "print(f\"Final results saved to {config['save_dir']}\")\n",
        "print(f\"Best model: {os.path.join(config['save_dir'], 'best_model.pth')}\")\n",
        "print(f\"Final model: {os.path.join(config['save_dir'], 'final_model.pth')}\")\n",
        "print(f\"Training history: {os.path.join(config['save_dir'], 'training_history.json')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Final Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"FINAL TRAINING RESULTS - BLUR CURRICULUM\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Dataset: ImageNet-100\")\n",
        "print(f\"Model: ResNet-18\")\n",
        "print(f\"Training Strategy: Gaussian Blur Curriculum Learning\")\n",
        "print(f\"Blur Curriculum: First {config['BLUR']['EPOCHS']} epochs with blur, then sharp images\")\n",
        "print(f\"Blur Parameters: Kernel size {config['BLUR']['KERNEL_SIZE']}, Sigma {config['BLUR']['SIGMA']}\")\n",
        "print(f\"Total Epochs: {config['num_epochs']}\")\n",
        "print(f\"Batch Size: {config['batch_size']}\")\n",
        "print(f\"Initial Learning Rate: {config['learning_rate']}\")\n",
        "print(f\"Weight Decay: {config['weight_decay']}\")\n",
        "print(f\"Momentum: {config['momentum']}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"Best Validation Accuracy: {best_acc1:.2f}% (Epoch {best_epoch})\")\n",
        "print(f\"Final Training Accuracy: {history['train_acc1'][-1]:.2f}%\")\n",
        "print(f\"Final Validation Accuracy: {history['val_acc1'][-1]:.2f}%\")\n",
        "print(f\"Final Training Top-5 Accuracy: {history['train_acc5'][-1]:.2f}%\")\n",
        "print(f\"Final Validation Top-5 Accuracy: {history['val_acc5'][-1]:.2f}%\")\n",
        "print(\"-\"*70)\n",
        "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Model Size: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Close wandb run if active\n",
        "if run is not None:\n",
        "    run.finish()\n",
        "    print(\"Wandb run finished.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Load and Test Best Model (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model for testing\n",
        "best_model_path = os.path.join(config['save_dir'], 'best_model.pth')\n",
        "if os.path.exists(best_model_path):\n",
        "    print(\"Loading best model for final evaluation...\")\n",
        "    \n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(best_model_path)\n",
        "    \n",
        "    # Create model and load weights\n",
        "    best_model = ResNet18(num_classes=config['num_classes']).to(DEVICE)\n",
        "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    \n",
        "    # Evaluate on validation set\n",
        "    print(\"Evaluating best model on validation set...\")\n",
        "    val_loss, val_acc1, val_acc5 = validate(best_model, val_loader, criterion, DEVICE)\n",
        "    \n",
        "    print(f\"Best Model Results:\")\n",
        "    print(f\"  Validation Loss: {val_loss:.4f}\")\n",
        "    print(f\"  Validation Acc@1: {val_acc1:.2f}%\")\n",
        "    print(f\"  Validation Acc@5: {val_acc5:.2f}%\")\n",
        "    print(f\"  Best epoch: {checkpoint['epoch']}\")\n",
        "else:\n",
        "    print(\"Best model checkpoint not found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Cleanup and Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up GPU memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Training completed successfully!\")\n",
        "print(f\"\\nCheckpoint directory: {config['save_dir']}\")\n",
        "print(\"Files created:\")\n",
        "for file in os.listdir(config['save_dir']):\n",
        "    file_path = os.path.join(config['save_dir'], file)\n",
        "    if os.path.isfile(file_path):\n",
        "        size = os.path.getsize(file_path) / (1024*1024)  # Size in MB\n",
        "        print(f\"  - {file} ({size:.1f} MB)\")\n",
        "\n",
        "print(\"\\nThis blur curriculum implementation can be compared with the baseline ResNet-18.\")\n",
        "print(\"Key differences from baseline:\")\n",
        "print(f\"- First {config['BLUR']['EPOCHS']} epochs use Gaussian blur (kernel={config['BLUR']['KERNEL_SIZE']}, sigma={config['BLUR']['SIGMA']})\")\n",
        "print(\"- Remaining epochs use sharp images\")\n",
        "print(\"- Same model architecture and hyperparameters as baseline\")\n",
        "print(\"- Curriculum learning strategy for improved training\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
